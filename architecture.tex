\section{System Overview}\label{sec-arch}
Frequent incremental maintenance of materialized views can be challenging given system resource constraints.
Existing materialized view maintenance techniques lie on a spectrum of accuracy and performance.
Immediate maintenance guarantees that query results on the view will always be accurate, while batch maintenance allows for a higher throughput of updates to the base table.
These two techniques sit at the extremes of the spectrum since they still require that all updates are processed and propagated to the view.

Modeling this problem as a data cleaning problem gives us a new perspective for addressing staleness.
When updates arrive, rows in the materialized view may be either out-of-date or missing altogether; making the view ``dirty".
The challenge is to clean the materialized view by updating the out-of-date rows and inserting the missing rows, however this can be very expensive if there are a large number of updates.
Suppose, we cleaned one dirty row at a time processing only as much of the updates as necessary to clean the row.
Then, for queries on the view, we get progressively less stale results for a lesser cost.
With this intuition in mind, to ensure that our result is unbiased, we clean a random sample of dirty rows.
For aggregate queries, such as \sumfunc, \countfunc, and \avgfunc, we can then estimate from this sample how the updates change the query results.
From this estimate, we can derive a correction to stale query results.

%Thus, our results are never stale; \reminder{This claim is a little strong. Since we do batch job, is it also possible for us to give stale results?} they have some inaccuracy introduced by the sampling ratio but in expectation they are correct.
%Furthermore, we also prove that this correction term is the optimal linear unbiased estimate for SUM, COUNT, and AVG.

In implementation, our proposed system will work in conjunction with existing maintenance or re-calculation approaches.
We envision the scenario where materialized views are being refreshed periodically eg. nightly.
While maintaining the entire view throughout the day may be infeasible, sampling allows the database to scale maintenance with the performance and resource constraints during the day.
Then, between maintenance periods, we can provide approximately up-to-date query results for aggregation queries.

An additional challenge is that sampling has the potential mask outliers in the updates.
We address this problem by using an ``outlier index" which has been applied in the SAQP setting \cite{chaudhuri2001overcoming}. 

\subsection{System Architecture}
The architecture of our proposed solution is shown in Figure \ref{sys-arch}.
The top of the diagram resembles a typical materialized view maintenance architecture.
However, there are a couple of key additions: (1) as updates arrive we sample the ``update pattern". 
(2) we maintain an outlier index, and (3) we combine the sample and the outlier index to process queries on the stale view.
In this section, we will introduce these three components, which are also explained in much more detail in the following sections.

\begin{figure}[h]
\label{sys-arch}
\centering
 \includegraphics[scale=0.35]{figs/sys-arch.pdf}
 \caption{We clean a stale query result by deriving a correction from a sample of up-to-date data. To make this process robust to skewed datasets, we couple sampling with an outlier indexing approach. For aggregation queries, we can guarantee that our results are unbiased and bounded.\reminder{What's the difference between dotted lines and solid lines? Add stale result and update-to-data result into the figure?}}
\end{figure}

\subsubsection{Supported Materialized Views}\label{subsubsec:supported-view}
We will first introduce the taxonomy of materialized views that can benefit from our approach. 
In this paper, we analyze and experiment with three classes of views: Select-Project Views, Foreign-key Join Views, and Aggregation Views.
Our technique can be applied to a broader class of views, please refer to [?] for a full description.
\vspace{1em}

\noindent\textbf{Select-Project Views (\spview): } One type of view that we consider are views generated from Select-Project
expressions of the following form:

\begin{lstlisting}
SELECT [col1,col2,...] 
FROM table 
WHERE condition([col1,col2,...]) 
\end{lstlisting}

\vspace{1em}

\noindent\textbf{Foreign-Key Join Views (\fjview): } As an extension to the Select-Project Views, we can support views derived from a Foreign-Key join:

\begin{lstlisting}
SELECT table1.[col1,col2,...], 
table2.[col1,col2,...]
FROM table1, table2 
WHERE table1.fk = table2.fk 
AND condition([col1,col2,...]) 
\end{lstlisting}

\vspace{1em}

\noindent\textbf{Aggregation Views (\aggview): } We also consider views defined by group-by aggregation queries of the following form:

\begin{lstlisting}
SELECT [f1(col1),f2(col2),...] 
FROM table 
WHERE condition([col1,col2,...]) 
GROUP BY [col1,col2,...]
\end{lstlisting}

\subsubsection{Sampling the Update Pattern}
Given these materialized views, the first challenge is sampling.
We have to sample the updates in a particular way so the sample accurately represents how updates
affect queries on the view. 
The three classes of views require different sampling techniques.
For example, insertions to the base database only result in insertions to Select-Project views.
But, for Aggregation Views, insertions to the base table can also result to updates to existing stale rows.
In Section \ref{sampling}, we describe the sampling algorithm and a cost analysis of how much sampling can reduce maintenance costs.

\subsubsection{Correcting a Query}
Once we have an appropriate sample, we can use information from the sample to correct stale query results.
Suppose, we issue an aggregate query to the stale view.
Then, we scan our sample, and calculate an approximate correction.
The corrected result is in expectation up-to-date and is probabilistically bounded.
Like the sampling, the algorithm to calculate the correction varies between the types of views.
We detail query correction in Section \ref{correction}.

\subsubsection{Outlier Indexing}
We are often interested in records that outliers, 
which we define in this work as records with abnormally large attribute values.
Outliers and power-law distributions are a common property in web-scale datasets.
Often the queries of interest involve the outlier records, however sampling does 
have the potential to mask outliers in the updates.
If we have a small sampling ratio, more likely than not, outliers will be missed.

Therefore, we propose coupling sampling with outlier indexing. 
That is, we guarantee that records (or rows in the view derived from those records) 
with abnormally large attribute values are included in the sample.
What is particularly interesting is that these records give information about the distribution 
and can be used to reduce variance in our estimates.
See Section \label{outlier} for details on this component.

\subsubsection{Example Application: Log Analysis}
To illustrate our approach, we use the following running example which is a 
simplified schema of one of our experimental datasets (Figure ?).
Imagine, we are querying logs from a video streaming company. 
These logs record visits from users as they happen and grow over time.
We have two tables: Log and Video, with the following schema:

\begin{lstlisting}[mathescape]
Log(sessionId$\textrm{,}$ videoId$\textrm{,}$ responseTime$\textrm{,}$ userAgent)
Video(videoId$\textrm{,}$ title$\textrm{,}$ duration)
\end{lstlisting}
These tables are related with a foreign-key relationship between
Log and Video, and there is an integrity constraint that every log
record must link to one video in the video table.

\begin{figure}[t] 
\centering
 \includegraphics[scale=0.40]{figs/sample-clean-example.png}\label{example-1}
 \caption{A simplified log analysis example dataset. In this dataset, there are two tables: a fact table representing video views and a dimension table representing the videos.}
\end{figure}

Consider the following example materialized view \aggview, which stores a result for each video and the maximum time it took for the server to load that video:
\begin{lstlisting} 
SELECT videoId, 
max(responseTime) AS maxResponseTime 
FROM Log 
GROUP BY videoId;
\end{lstlisting}

Suppose, the user creates and materializes this view.
The user wants to know how many videos had a max response time of greater than 100ms.
\begin{lstlisting} 
SELECT COUNT(1)
FROM AggView
WHERE maxResponseTime > 100
\end{lstlisting}
Let us suppose the query result is $15$.
\reminder{The example made the point, but it may need to be polished.}
Since the user queried the table, there have been new logs inserted into to the Log table. 
So materialized aggregation view and the old result of $15$ are now stale.
For example, if our sampling ratio 5\%, our system maintains a sample of this view.
That means for 5\% of the videos (distinct videoID's) we refresh stale maxResponseTime if necessary.
From this sample, we calculate how many new videos changed from a maxResponseTime of less than 100ms to times greater than 100ms; let us suppose this answer is $2$.
Since our sampling ratio is 5\%, we extrapolate that $10$ new videos throughout the view should now be included in the count, resulting in the estimate of $25$.
In contrast, if we had applied SAQP, we would have counted how many videos in the sample had a max response time of greater than 100ms.