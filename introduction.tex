\vspace{-0.5em}
\section{Introduction}
Materialized views (MVs), storing pre-computed query results, are a well-studied approach to speed up queries on large datasets \cite{LarsonY85, gupta1995maintenance, chirkova2011materialized, halevy2001answering}.
During the last 30 years, the research community has thoroughly studied MVs, and all major database vendors have added support for them.
In a world of ever-increasing data sizes, MVs are becoming even more important, both for traditional query processing and for more advanced analytics based on linear algebra and machine learning \cite{nikolic2014linview, zhang2014mat}.

However, when the underlying data is changed MVs can become \emph{stale}; the pre-computed results do not reflect the recent changes to the data. 
One solution would be to recompute the MV every time a change occurs, however, in many cases, it is more efficient to incrementally update the MV instead of recomputing the query.
There has been substantial work in deriving incremental updates (incremental maintenance) for different classes of MVs and optimizing their execution \cite{chirkova2011materialized}.

For frequently changing tables even incremental maintenance can be expensive since every update to the underlying data requires updating all the dependent views.  
This problem is exacerbated in Big Data environments, where new records arrive at an increasingly fast rate and where data are often 
distributed across multiple machines.  
As a result, in production environments it is common to defer view maintenance to a later time \cite{chirkova2011materialized, zhou2007lazy, DBLP:conf/sigmod/ColbyGLMT96} so that updates can be batched together to amortize overheads and maintenance work can be scheduled for times of low system utilization.  

While deferring maintenance has compelling benefits, it unfortunately leads to the problem that the views are stale in between maintenance periods. 
As a result, as the time since the last maintenance increases, queries using those views return increasingly incorrect answers.
The problem of stale MVs parallels the problem of dirty data studied in data cleaning~\cite{rahm2000data}; as both staleness and erroneous ``dirty" records are a type of data error.
The observation that stale MVs are a type of dirty data leads us to the key insight behind our work; namely, that data cleaning techniques can be used to mitigate the negative impacts of deferred MV maintenance.

\begin{figure}[t] \vspace{-2em}
\centering
 \includegraphics[scale=0.24]{figs/sys-arch.pdf} \vspace{-.25em}
 \caption{If a view is being periodically maintained, in the interim, query results may be stale. Sample-View-Clean sits above an existing view maintenance architecture, and provides an interface for approximately up-to-date query results by maintaining a sample. The user can tune the sampling ratio to meet the resource constraints of the system.\label{sys-arch}}\vspace{-1.75em}
\end{figure}

Data cleaning has been studied extensively in the literature (e.g., see Rahm and Do for a survey\cite{rahm2000data}) but increasing data volumes and arrival rates have led to development of new, efficient sampling-based approaches for coping with dirty data.   
In our prior work, we developed the SampleClean framework to greatly improve query accuracy while cleaning only a small sample of dirty records \cite{wang1999sample}.  
We proposed an approach called \nsc that corrects dirty query results by using a sample of clean data to learn how the dirtiness affects that query and then calculates a correction.  
This perspective raises a new possibility for MVs: we can use a sample of clean (up-to-date) data to return more accurate query results without incurring the cost of full view maintenance.
Of course, the metaphor of stale MVs as dirty data only goes so far. 
View staleness is a different type of error than typical dirty data, which raises interesting new challenges in sampling, cleaning, and efficient query processing.

To address these new challenges, we propose Sample-View-Clean (SVC), a framework that applies sampling to approximately correct query results on stale views.
SVC takes a sample of up-to-date rows from the view, and extrapolates a correction factor for query answers on the stale view. Figure~\ref{sys-arch} shows how SVC can be used as complementary to existing deferred maintenance approaches. When the MVs become stale between maintenance cycles, we apply SVC for query result estimation for a far smaller cost than having to maintain the entire view.
The query results from SVC are up-to-date in the sense that they reflect the most recent data, however they are approximate. 
The approximation error due to sampling is more manageable than staleness: (1) the uniformity of sampling allows us to apply theory from statistics such as the Central Limit Theorem to give tight bounds on approximate results, and (2) the approximate error is parameterized by the sample size which the user can control trading off accuracy for computation.
Sampling is known to be sensitive to skewed datasets, and we incorporate an outlier indexing technique to improve the accuracy of our approximate query results.

In practice, both MVs and queries can be arbitrarily complex.
Since SVC is based on sampling, there are a subclass of views for which SVC can save significant computation and a subclass of queries on these views for which SVC can give accurate corrections.
In this work, we explore these classes from both a theoretical perspective (i.e. when is our query correction optimal w.r.t estimate variance) and an empirical perspective for queries that do not satisfy the optimality conditions when is SVC still beneficial.

To summarize, our contributions are as follows: (1) we model the incremental maintenance problem as a data cleaning problem and staleness as a type of data error; allowing us to apply query processing techniques designed for dirty data, (1) we modify these techniques to apply in the materialized view setting and show that they can give us a tradeoff between query result accuracy and computation by applying sampling, (3) we use an outlier index to increase the accuracy of the approach for power-law, long-tailed, and skewed distributions, and (4) we evaluate our approach on a single-node MySQL database with a 10GB skewed TPCD benchmark dataset and on a 20-node Apache Spark cluster with a 1TB log dataset from a video streaming company. 
%\end{itemize}

The paper is organized as follows: 
In Section~\ref{sec-background}, we give the necessary background for our work.
Next, in Section~\ref{sec-arch}, we formalize the problem.
In Section~\ref{sampling} and~\ref{correction}, we describe the sampling and query processing of our technique.
In Section~\ref{outlier}, we describe the outlier indexing framework.
%In Section~\ref{sec:ext}, we discuss extensions to our framework.
Then, in Section~\ref{exp}, we evaluate our approach.
Finally, we discuss Related Work in Section~\ref{related} and present our Conclusions and Future Work in Section~\ref{conclusion}.
