
\section{Extensions and Discussion}\label{sec:ext}
In this section, we present two additional features of the SVC framework, and discuss some additional points.

\subsection{Correction vs. Direct Estimate}
We argued that in many cases correcting query results approximately is more accurate than an
AQP-style direct estimate. 
We can provide some more rigorous mathematical intuition for this.
For \sumfunc, \countfunc, and \avgfunc, our correction algorithm gives a confidence interval (via CLT) that is
proportional to the variance of the \emph{change} and inversely proportional to the sample size:
\[\frac{\sigma_{c}^2}{k}\]
On the other hand, an AQP approach would give us an estimate that is proportional to the variance of the up-to-date data:
\[\frac{\sigma_{S'}^2}{k}\]
Since the change is the difference between the stale and up-to-date data, this can be rewritten as: 
\[\frac{\sigma_{S}^2 + \sigma_{S'}^2 - 2cov(S,S')}{k}\]
Therefore, a correction will be more precise when:
\[\sigma_{S}^2 - 2cov(S,S') \le 0 \]
If the difference is small, i.e. $S$ is nearly identical to $S'$,then $cov(S,S') \approx \sigma_{S}^2$ and it is clearly less than 0.
On the other hand, there is a break even point where changes are significant enough that direct estimates might be more accurate.

\subsection{Hash-Operator}
We defined a concept of tuple-lineage with primary keys.
However, a curious property of the deterministic hashing technique is that we can actually hash any key while retain key
statistical properties.
This is due to the linearity of expectation operator.
A consequence of this is that we can push down the hashing operator through arbitrary equality joins (not just many-to-one) by hashing the join key.

We defer further exploration of this property to future work as it introduces new tradeoffs.
For example, sampling on a non-unique key, while unbiased in expectation, hash higher variance in the size of the sample.
Happening to hash a large group may lead to decreased performance. 
Suppose my keys are duplicated $\mu_k$ times on average with variance $\sigma_k^2$, then the variance of the
sample size is for sampling fraction $m$:
\[m(1-m)\mu_k^2+(1-m)*\sigma_k^2\]
In this setting, our optimizer would have to consider this variance against the benefits of pushing the hash operator futher down the query tree. 

\subsection{Sampling Updates vs. Sampling Views}
In SVC, we sample from views and work backwards through the view definition using relational algebra.
An alternative approach is to sample the updates to the view.
However, this approach quickly leads to some bottlenecks.
For example, if our view is an aggregate view with a nested selection, we can easily construct a distinct count problem rendering any aggregate query inestimable \cite{DBLP:conf/pods/CharikarCMN00}.
On the other hand this approach, does allow for better support for very selective queries on the view.
This is a tradeoff that is an interesting avenue for future work.

\subsection{Multi-View Setting}
In a production environment, the database system might have many materialized views. 
With the sampling ratio, SVC gives the database administrator an additional degree of freedom to adjust throughput, storage, and accuracy.
The sampling ratio of each view can be adpatively adjusted to suit the workload.

We can pose minimizing the expected estimation error as a Geometric Convex Program.
In one time period, if view $i$ has a expected cardinality of $N_i$, an average query variance of $\alpha_i$, a sampling ratio of $m_i$, there is a total space budget of $B$, the cost for update is $C_i$ secs/Record and throughput demand of $D$ latency:
\[\arg \min_{m_i} \sum_i \frac{\alpha_i}{m_i \cdot N_i}\]
\[\text{subject to:} \sum_i m_i \cdot N_i \le B \]
\[\sum_i m_i\cdot N_i \cdot C_i \le D \]




