\section{Results}
\label{exp}
Our experiments are organized in the following way.
We first evaluate our approach on a synthetic benchmark dataset where we can control the data distribution and the update rate.
Our first experiment will discuss the practical overheads and tradeoffs to using our approach.
Then, for three specific materialized views, we evaluate accuracy, performance, and outlier indexing.
After evaluation on the benchmark, we present an end-to-end application of log analysis with a dataset from a video streaming company.

We define the following metrics for evaluation.
For accuracy, we measure the relative error between estimated query result and the true query result.
A relative error of 5\% means that the difference between the estimated query result and the true result is 5\% of the true result.
Whenever we discuss error, we discuss it in terms of average query results. 
That is for a set of queries on the view what is the mean relative error. 
For accuracy, we compare against two alternative approaches, no maintenance where the materialized view is left to be stale and SAQP where instead 
of calculating an approximate query correction we estimate directly from the up-to-date view.
We fix the sampling ratio of SAQP and our approach to be the same so they will have the same maintenance time.
For performance, we measure the maintenance time as the time required to calculate the delta view and then refresh the materialized view for a batch of updates.
As a baseline, we compare the performance to full incremental maintenance.

\subsection{Experimental Setting}
\subsubsection{TPCD-Skew}
The first dataset, TPCD-Skew dataset, is based on the Transaction Processing Council's benchmark
schema but is modified so that it generates a dataset with values drawn from a Zipfian powerlaw distribution instead of uniformly.
The Zipfian distribution \cite{mitzenmacher2004brief} is a long-tailed distribution with a single parameter $z=\{0,1,2,3,4\}$ which a larger
value means a more extreme tail.
This database has been applied to benchmark other sampling based approaches, approximate queries, and outlier performance \cite{chaudhuri2001overcoming, agrawal2005database}.
The dataset is provided in the form of a generation program which can generate both the base tables and a set of updates.

For this dataset, we applied our approach to three materialized views, each of a different type:
\vspace{0.5em}

\textbf{Select-Project View}
\begin{lstlisting}
SELECT *,  if(lcase(l_shipinstruct) LIKE '%deliver%' AND lcase(l_shipmode) LIKE '%air%', 'priority', 'slow')  
FROM lineitem_s
\end{lstlisting}

\vspace{0.5em}

\textbf{Aggregation View}
\begin{lstlisting}
SELECT l_orderkey, l_shipdate, sum(l_quantity) as quantity_sum,  sum(l_extendedprice) as extendedprice_sum,  max(l_receiptdate) as receiptdate_max, count(*) as group_count 
FROM LINEITEM GROUP BY l_orderkey, l_shipdate
\end{lstlisting}

\vspace{0.5em}

\textbf{Foreign-Key Join View}
\begin{lstlisting}
SELECT supplier.*, customer.* 
FROM   customer, orders, lineitem, supplier, partsupp 
WHERE  c_custkey = o_custkey  AND o_orderkey = l_orderkey  AND l_suppkey = ps_suppkey AND l_partkey = ps_partkey AND ps_suppkey = s_suppkey AND s_nationkey <> c_nationkey
\end{lstlisting}

\vspace{1em}

The base dataset is 10GB corresponding to 60,000,000 records, and the above three views are derived from this base dataset.
Then, we randomly generated aggregation queries and evaluated them for different sample sizes, different update rates, and different parameter settings for the Zipfian distribution.
All of our experiments for the TPCD-Skew dataset are run on a single r3.large Amazon EC2 node with a MySQL database.

\subsubsection{Conviva}
Conviva is a video streaming company and we evaluated our approach on user activity logs. 
We experimented with a 1TB dataset of these logs and a workload corresponding analyst queries on the log.
We used this dataset to evaluate the end-to-end accuracy and performance of the system on a real dataset and query workload.
We evaluated performance on Apache Spark with a 20 node r3.large Amazon EC2 cluster. 

\subsection{Update Rate and Accuracy}
In the first experiment, for each of the three views listed above, we evaluate the accuracy of our approach.
We measure accuracy by using the average relative query error for a set of 10000 randomly generated aggregation queries on each view.
We set the sample size to $5\%$ and then vary the number of inserted records by increments of 500000 records to a final count of 8000000 records (1.3GB).
In Figure \ref{exp2udpate}, we show the results of the experiment. 

\begin{figure*}[ht!]
\label{exp2update}
\centering
 \includegraphics[scale=0.27]{exp/exp2-full.eps}
 \caption{For each of the three views listed above, we plot the accuracy for 5\% samples compared to SAQP and a stale baseline. We also plot the maintenance time for each of the views in comparison to full incremental maintenance for a 1\%, 5\% and 10\% sample. }
\end{figure*}

The results highlight an important point comparing the accuracy of SAQP to our approach. 
The accuracy our approach is proportional to the amount of correction needed, while SAQP keeps a roughly constant accuracy.
As more records are inserted the approximation error in our approach increases.
However, we find that even for very large amounts of inserted records (>10\% of dataset size), our approach gives significantly more accurate results
than SAQP.
The gain is most pronounced in aggregation views where there are a mixture of updated and inserted rows into the view.
Correcting an update to an existing row is often much smaller than doing so for a new inserted record.

\subsection{Computational Efficiency}
In the next experiment (Figure \ref{exp2udpate}), we evaluate how sampling can reduce maintenance time.
For a batch of updates, we evaluate how long it takes to incrementally maintain a view compared to maintaining a sample.
For, the join view, we build an index on the foreign key of the view.
As before, we inserted records in increments of 500000.
We find that for the Aggregation and Join view, which are more expensive to maintain, sampling leads to more pronounced savings. 
Furthermore, there are diminishing returns with small sample sizes as overheads start becoming significant.

\subsection{Overhead of Query Correction}
In Section \ref{sampling}, we ran cost analyses to argue that sampling can greatly reduce delta view and refresh costs.
In practice, these gains will only be meaningful if the overheads for the query correction are small.
Since, we are correcting a stale query rather than the materialized view, we are shifting some of the computational cost from maintenance time to query execution time.
If our updates only contain insertions, then for Select-Project and Foreign-Key join views, since we derive a correction from a sample of the updates, we are guaranteed to have a reduced query time.
However, for aggregation views, query correction requires a scan of the old view and a sample of the maintained view; potentially increasing the time to 
answer the query.
In our first experiment, we set the sample size to 5\% and evaluate the aggregation view listed earlier for 5000000 inserted records. 
In (Figure \ref{exp10overheads}), we show that average query time is small compared to maintenance time, and furthermore, 
the time to calculate a correction is a just fraction of the query time.
\begin{figure}[h]
\label{exp10overheads}
\centering
 \includegraphics[width=\columnwidth]{exp/total_time_agg_view.png}
 \caption{The overhead introduced by correcting a query is small compared to the overall maintenance time.}
\end{figure}

\subsection{View Complexity}
In our maintenance time experiments, we showed that more expensive views tended to benefit from our approach.
We evaluated this tradeoff by taking the simplest possible view, a SELECT of the base table, and then progressively adding clauses to the predicate.
For example:
\begin{lstlisting}
WHERE (condition1)
WHERE (condition1 || condition2)
WHERE (condition1 || condition2 || condition 3)
\end{lstlisting}
We set the sample size to 5\% and measure the maintenance time.
For a such a view, the only cost for maintenance is a scan of the data and evaluating the predicate. 
Sampling saves on predicate evaluation but introduces the overhead of random number generation.
Figure \ref{exp11overheads} illustrates how as the view becomes for complex the performance improvement given
by our approach increases.
Initially, the random number generation adds about 5\% overhead, but as the cost of evaluating the benefits increase.
We repeated the same experiment for the aggregation view, but instead we added terms to the group by clause to increase the
cardinality of the view.
We found that for a highly selective group by clause (thus a large view) the savings were nearly perfect (20x).
However, when the view was small the cost savings were smaller.
\begin{figure}[h]
\label{exp11overheads}
\hspace{-2.5em}
 \includegraphics[scale=0.25]{exp/complexity_efficiency_tradeoff.eps}
 \caption{As the views definitions become more complex our approach gives increasing gains. For simplest view (selection no predicate), the overhead is small (5\%). }
\end{figure}
This experiment emphasizes that our approach rarely makes performance worse and at worst the improvements are not linear with the sampling ratio.
Furthermore, we emphasize that when views are large and complex, as often the case, we can have significant improvements.

\subsection{Outlier Indexing}
We evaluated the accuracy for each of the views as a function of the size of the outlier index. 
We use a 5\% sample size and 5000000 records inserted, and we evaluate the accuracy of our approach with and without outlier indexing.
We apply the index to the Select-Project view, where we index the attributes l\_extendedprice and l\_quantity.
The results for the other two views were similar, and in this paper, we only present results for the Select-Project views.

In the first experiment, we varied the total number of indexed records and plot accuracy as a function of index size.
As a baseline, we compare if the same number of records were randomly sampled.
In Figure \ref{exp7outlier}, we find that outlier indexing can improve accuracy by a factor of 2.5 for this view. 
Next, we evaluated how the approach works under different levels of skew in the dataset. 
We varied the Zipfian parameter from (0,1,2,3,4) and measured the average improvement in accuracy. 
We found that as the dataset become more skewed, outlier indexing is increasingly important.
Finally, we evaluated the overhead for the approach. 
We found that small indicies were sufficient for good accuracy and thus the additional overhead was marginal and orders of magnitude smaller than sample maintenance time.

\begin{figure}[ht!]
\label{exp7outlier}
\hspace{-4em}
\includegraphics[scale=0.21]{exp/exp6-outlier-full.eps}
 \caption{Outlier indexing greatly improves accuracy of our approach especially in skewed datasets for a small overhead of building the index and ensuring those rows are in the view.}
\end{figure}

\subsubsection{Distribution of Query Error}
In our earlier experiments, we presented the average error for the queries on the views.
In this experiments, we looked at the distribution of query errors compared to their staleness.
For each of the three views, we derive the views from a 10GB dataset.
Then, as before, we simulate 5000000 inserted records and a 5\% sample.
We evaluated the relative error for each query.

\begin{figure}[ht!]
\label{exp3dist}
\hspace{-2em}
 \includegraphics[scale=0.21]{exp/exp3-query-error-dist.eps}
 \caption{On the x-axis is the accuracy of our approach and on the y-axis is the error if the query was not maintained. We find that there are many outlier queries results--motivating the need for outlier indexing.}
\end{figure}

In Figure \ref{exp3dist}, we present staleness vs. estimation accuracy as a scatter plot.
For all three of the views, the median error is greater than 10 times less than the staleness. 

However, this distribution motivates our outlier indexing as there are clearly outlier queries.
Outliers can occur for a variety of reasons.
First, some of the generated queries are highly selective and a uniform sampling approach may not sample enough records to answer it accurately.
Next, outliers can be caused by large updates to the data that are missed by our sampling.
In Section ?, we show how outlier indexing can solve the latter problem and, in fact, after outlier indexing all of our queries on these three views are estimates more accurately than the pre-existing staleness error.

\subsection{Sample Size and Accuracy}
In this experiment, we illustrate the tradeoff between sample size and accuracy.
We set a fixed 5000000 inserted records, and then vary the sampling ratio for the three types of views and show how much sampling is needed to achieve a given query accuracy.
In Figure \ref{exp1sample}, we show the accuracy as a function of sampling ratio for each of the views.
For both SAQP and our approach, there is a break-even point at which the approximation error is less than the staleness error.
At sampling ratios beyond this point, our query approximated queries are on average more accurate than queries on the stale view.
For all three of the views, the average query error is significantly less than SAQP as the break-even point happens earlier in our approach.

\begin{figure}[ht!]
\label{exp1sample}
\hspace{-3em}
 \includegraphics[scale=0.20]{exp/exp1-samplesize-accuracy.eps}
 \caption{As we increase the sample size, the relative error goes down following a $\frac{1}{\sqrt{k}}$ rate. SAQP also follows the same rate so the lines will never cross by only changing the sample size.}
\end{figure}

\subsection{Application: Log Analysis For Conviva}
We used a dataset of queries given by Conviva to generate three materialized views.
All three of these views are aggregation views, and for confidentiality, we exclude the actual definitions of the views. 
View 1 has the most selective group by clause and was chosen to a be a large view where most of the maintenance is in the form of new rows to insert.
View 1 aggregates session times for each visitor and video pair.
View 2 is a medium size view where maintenance is a mix of both updates and insertions.
View 2 computes buffering time statistics for each visitor over all videos they watched.
View 3 is a small view where most of the maintenance is updates to existing rows.
View 3 computes statistics for log events that triggered error states.

We selected these views to be representative of the dataset and query workload. 
We first present an experiment illustrating where these materialized views lie in the space of all the generated views from the conviva query log.
We derived each view from a 7GB base table and inserted 1.5GB of records to the table. 
On a single node, we evaluated the average query accuracy for a 1\% sample and the runtime of our approach.
In Figure \ref{exp12conviva}, we plot the three views on the distribution of error and runtime and we see that they 
are from different parts of the distribution.
\begin{figure}[ht!]
\label{exp12conviva}
\hspace{-3.5em}
\includegraphics[scale=0.23]{exp/conviva_efficiency_accuracy.eps}
 \caption{We chose three views that represented the distribution of all views well in terms of both accuracy and maintenance time.}
\end{figure}

\subsubsection{Accuracy in Conviva}
We evaluated the average query accuracy for different sample sizes and numbers of records inserted.
As before, we derived each view from a 7GB base table and inserted 1.5GB of records to the table. 
We built an outlier index on all of the attributes that represent time intervals or rates.
Figure \ref{exp5conviva}, compares the accuracy to the staleness of the query.
We find that even a $0.1\%$ sample gives significantly more accurate results for View 1 and View 2.
Even in the situation where the view is small, sampling can still have benefits as seen in View 3.

\begin{figure}[ht!]
\label{exp5conviva}
\hspace{-3.5em}
\includegraphics[scale=0.22]{exp/exp5-coniva-accuracy-woutlier.eps}
 \caption{We compare average query staleness to our approach for a variety of sample sizes. For View 1 and View 2, we find that small sample sizes can give accurate results. Even in the extreme case of View 3, we find that sampling can still provide reasonably accurate results.}
\end{figure}

\subsubsection{Performance in Conviva}
We evaluated performance on Apache Spark, on a 20 node r3.large Amazon EC2 cluster. 
This cluster had enough memory to hold all of the materialized views in memory.
Spark supports materialized views through a distributed data structure called an RDD [?].
The RDD's are immutable, thus requiring significant overhead to maintain.
We compare maintenance to recalculation of the view when the data is in memory and when only the updates are in memory. 
We derived the views from a 700GB base table and inserted records in 15GB increments. 
We scaled these experiments up significantly from the accuracy experiments to illustrate the performance benefits of sampling at a large scale, however, as insertions as a percentage of the base data are the same.
As Spark does not have support for indices, we rely on partitioned joins for incremental maintenance of the aggregation views.
Each view is partitioned by the group by key, and thus only the sampled delta view has to be shuffled. 

\begin{figure*}[ht!]
\label{exp6conviva}
\hspace{-1em}
\includegraphics[scale=0.18]{exp/exp5-efficiency-conviva-woutlier.eps}
 \caption{As before, we compare maintenance times for a variety of sample sizes and full incremental maintenance. We further include a comparision with view recalculation as our experimental platform, Spark, does not support indices or selective writes.}
\end{figure*}

In Figure \ref{exp6conviva}, we illustrate the maintenance time as a function of the number of inserted records.
We find that we achieve good scalability on view 1 and view 2, and still have some performance gains on view 3.
In view 1 and view 2, our gains are more pronounced due to the savings on communication in a distributed environment.
Aggregation views with a larger cardinality create larger delta views.
These delta views necessitate a shuffle operation that communications them during the refresh step.
As view 3 is smaller, the communication gains are less and the only savings are in computation.



