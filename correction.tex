\section{Efficient Maintenance With Sampling}
In this section, we discuss how to efficiently derive $\hat{S'}$ and the conditions under which
maintaining $\hat{S'}$ is much cheaper than maintaining the entire view $S'$.

\subsection{Desired Properties and Challenges}
We formalized the update procedure to transform $S$ into $S'$ as a maintenance strategy $\mathcal{M}$.
This maintenance strategy is itself a relational expression.
To derive a sample $\hat{S'}$, we could just apply the maintenance strategy and then sample.
However, this does not make the maintenance of the sample any more efficent.

Thus, ideally, we want to integrate the sampling into the maintenance expression $\mathcal{M}$ so that expensive operators
need not operate on the full data.
We can model this process as a ``push-down"\cite{??} on an operator tree of the relational expression.
An important challenge is that we want every relation above the sampling to still be a uniform sample.

We further desire a stronger property, which we call Sample Completeness:
\begin{definition} (SAMPLE COMPLETENESS) At every node in the operator tree of sampled maintenance strategy, we say the relation $\hat{R}$ at the node is \emph{sample complete} if at every corresponding node $R$ in the unsampled maintenance strategy:
\[ \forall r_s \in R_s : r_s \in R \]
\end{definition}
To illustrate this property imagine a group by aggregation.
A sample of this expression that satisfies the property is a uniform sample of the fully aggregated groups.
A sample that does not satisfy this property is a sample where some groups are partially aggregated.

However, it is known that uniform sampling commutes very poorly many relational operations such as joins and aggregates \cite{??}.
Furthermore, a ``coin-flip" algorithm to sample the relations would not achieve Sample Completeness in general.
The problem is that our sampling technique needs to include all of the rows in relations lower in the tree that could possibly contribute to a row.
Our approach to this problem will use a specific type of traceable unique identification for rows and then applying a deterministic hashing operator to ensure all of the relevant rows are processed.

\subsection{Identification With Row Lineage}
Lineage has been an important tool in the analysis of materialized views \cite{DBLP:journals/vldb/CuiW03} and in approximate query processing \cite{DBLP:conf/sigmod/ZengGMZ14}.
If we want to be able to push our sampling operation through the operator tree, we need to ensure that every subexpression has consistent identifying information.
We recursively define these identifying attributes:
\begin{definition}
(LINEAGE CONDITION). All views $S$ and maintenance plans $\mathcal{M}$ must satisfy the lineage condition which is defined inductively on a query tree:
\begin{itemize}\vspace{-.45em}
\item Base Case: All relations (leaves) must have an attribute $p$ which is designated as a primary key.\vspace{-.45em}
\item $\sigma_{\phi}(R)$: Always preserves lineage of R \vspace{-.45em}
\item $\Pi_{(a_1,...,a_k)}(R)$: Preserves lineage of R if the projection includes $p$ \vspace{-.45em}
\item $\bowtie_{\phi (r1,r2)}(R_1,R_2)$: Resets lineage. The primary key of the result is the union of the linages of the relations $p= p_1 \cup p_2$. 
\item $\gamma_{e}(R)$: Resets lineage. The primary key of the result is the group by expression $e(R)$.\vspace{-.45em}
\item $R_1 \dot{\cup} R_2$: Always preserves lineage attribute $p$
\item $R_1 \dot{\cap} R_2$: Always preserves lineage attribute $p$
\item $R_1 \dot{-} R_2$: Always preserves lineage attribute $p$
\end{itemize}
\end{definition}
The above definition is also constructive; that is, if a view does not satisfy the Lineage Condition we can modify the plan so that it does.
We can add a primary key to the base relations, and then we modify every projection operation to ensure that primary key of the relation is never projected out of the next level of the tree.
Given this definition of lineage, we see that we can trace through all the rows that contribute to an expression higher up in the tree.
That is if we have a deterministic way of mapping $p$ to a sample we can also ensure that all contributing expressions are also sampled. 
However, it is important to note that for some of the expressions, notably joins and aggregates, the lineage gets reset.
It turns out in general we cannot push down even a deterministic sample through those expressions.

\subsection{Hashing Operator}
Let us denote the hashing operator $\eta_{a_1, m}(R)$. 
For all tuples in R, this operator applies a hash function whose range is $[0,1]$ to attribute $a_1$ and selects those records with hash value less than or equal to $m$.
If the hash function is sufficiently uniform, then $h(a) \le m$ samples on average a fraction $m$ of the tuples.
This definition is without loss of generality for uniform hash function, as if we have a hash function whose range is the set of integers (as implemented in MySQL or Apache Hive) we can take the absolute value and divide by the maximum integer mapping this range back $[0,1]$. 

\subsection{Pushdown of the Hashing Operator}
To achieve the performance benefits of sampling, we can now push down the hashing operator through the query tree.
The further than we can push $\eta$ down the query tree, the more operators can benefit from the sampling.
We make two assumptions on this hash operator: (1) \emph{independence} there is no expression in $S_{def}$ that is dependent on the hash operator, and (2) \emph{uniformity} over the domain of possible attribute values the \emph{a priori} probability of including any tuple is the same.

\begin{proposition}
(HASH PUSHDOWN). Assuming the query tree satisfies the Lineage Condition, then the following rules can be applied to push $\eta(a)$ down the query tree. 
\begin{itemize}\vspace{-.45em}
\item $\sigma_{\phi}(R)$: Push $\eta$ through \vspace{-.45em}
\item $\Pi_{p,[a_2,...,a_k]}(R)$: Push $\eta $ through \vspace{-.45em}
\item $\bowtie_{\phi (r1,r2)}(R_1,R_2)$: Blocks $\eta $ in general. See (Section ??) for conditions when push down is possible.
\item $\gamma_{e}(R)$: Blocks $\eta$ in general. If the group by expression can be decomposed into $e(R) = a, e'(R)$ then pushdown is posible.\vspace{-.45em}
\item $R_1 \dot{\cup} R_2$: Push $\eta $ through to both $R_1$ and $R_2$
\item $R_1 \dot{\cap} R_2$: Push $\eta $ through to both $R_1$ and $R_2$
\item $R_1 \dot{-} R_2$: Push $\eta $ through to both $R_1$ and $R_2$
\end{itemize}
\end{proposition}

\subsubsection{More About Joins} 
The rules preserve sample completeness for the general operators. 
However, in special cases, we can achieve even further pushdown of the hashing operator. 

\textbf{Many-to-one Join: } If we have an join with two relations $R_1$ and $R_2$ and we know that for every $r_1 \in R_1$ there is at most one $r_2$ in $R_2$ that satisfies the join condition, then we push $\eta$ down to $R_1$.

\textbf{One-to-one Join: } If we have the previous condition and also the converse is true for every $r_2 \in R_2$ there is at most one $r_1$ in $R_1$, then we push $\eta$ down to $R_1$ and $R_2$.

\textbf{(Semi/Anti)-Join: } We can always push $\eta$ down on Semi-joins. For anti-joins we can push $\eta$ down because we can rewrite the node as $R_1 \dot{-} (R_1 \ltimes R_2) $ and apply the pushdown rules for set difference and Semi-Joins.

\subsection{Maintenance Savings}
\reminder{TODO}

\subsection{Running Example: Log Analysis}
Recall the example view ``LogView" in the previous section:
\begin{lstlisting} 
CREATE VIEW LogView AS 
SELECT videoId, 
count(1) AS slowResponseTimes 
FROM Log, Video
WHERE Log.videoID = Video.videoID and
    responseTime > .1*Video.duration
GROUP BY videoId;
\end{lstlisting}

Assume that insertions have been added to the Log table, and we call the set of insertions LogUpdates.
To update this view, we need to join the delta-relation LogUpdates with the Video table and then aggregate the counts.
After that, there are two cases: the video already exists in the stale view (in which case we have to update the count), or
the video does not exist in which case we insert a new row.
The following intentionally pedantic SQL maintenance plan can produce the updated view:
\begin{lstlisting}
 SELECT videoId, 
       LogView.slowResponseTimes + 
          deltaView.slowResponseTimes
 FROM LogView,
  (SELECT videoId, 
        count(1) AS slowResponseTimes 
   FROM LogUpdates, Video
   WHERE Log.videoID = Video.videoID and
        responseTime > .1*Video.duration
   GROUP BY videoId ) as deltaView
 WHERE LogView.videoId = deltaView.videoID

UNION 

  SELECT videoId, 
        count(1) AS slowResponseTimes 
  FROM LogUpdates, Video
  WHERE Log.videoID = Video.videoID and
        responseTime > .1*Video.duration
        Log.videoID NOT IN 
            (SELECT videoID FROM LogView)
  GROUP BY videoId

\end{lstlisting}
\reminder{figure here for the query tree}
If we apply the rules listed above and sample this maintenance plan, we find that we can push down the hash operator to LogView (hash on videoId) and LogUpdates (hash on videoId). 
This reduces the complexity for join between LogUpdates and Video, the subquery aggregation, and the join for deltaView and LogView.
