\section{Efficient Maintenance With Sampling}
In the previous section, we formalized the computation of $\hat{S'}$  as a data cleaning procedure.
A naive solution to derive a sample $\hat{S'}$ is to just apply the maintenance strategy and then sample.
However, this does not make the maintenance of the sample any more efficent.
Ideally, we want to integrate the sampling into the maintenance strategy $\mathcal{M}$ so that expensive operators
need not operate on the full data.
In this section, we discuss how to efficiently derive $\hat{S'}$ and the conditions under which
maintaining $\hat{S'}$ is much cheaper than maintaining the entire view $S'$.

\subsection{Desired Properties}
In this work, we focus on result estimation on uniform samples of views.
We call a sample view $\hat{S'}$ a uniform sample of $S'$, under the following condition:
\begin{definition}[Uniform Sample] We say the relation $\hat{S'}$ is a \emph{uniform sample} if
\[ \forall s \in \hat{S'} : s \in S' \]
and
\[Pr(s_1 \in \hat{S'}) =  Pr(s_2 \in \hat{S'}) \]
\end{definition}
A traditional ``coin-flip" sampling algorithm is not suited for this property as it is known that such sampling commutes very poorly many relational operations such as joins and aggregates \cite{chaudhuri1999random}.
Recall, the view in our running example \textsf{browserCountView}. 
If we sampled the base relation the result would have a mix of missing rows from the view and rows with incorrect aggregates.
The rows with incorrect aggregates appear with probability 0 in the unsampled view.

To get a uniform sample of a view, the main problem is that for every row sampled in the view, our sampling technique needs to include all of the rows in relations lower in the tree that could possibly contribute to its materialization.
Achieving this requires a definition of lineage; traceable unique identification for rows.

\subsection{Identification With Row Lineage}
Lineage has been an important tool in the analysis of materialized views \cite{DBLP:journals/vldb/CuiW03} and in approximate query processing \cite{DBLP:conf/sigmod/ZengGMZ14}.
We recursively define these identifying attributes:
\begin{definition}
(Primary Key). For every relational expression $R$, we define the primary key of every subexpression to be:
\begin{itemize}\vspace{-.45em}
\item Base Case: All relations (leaves) must have an attribute $p$ which is designated as a primary key.\vspace{-.45em}
\item $\sigma_{\phi}(R)$: Primary key is the primary key of R \vspace{-.45em}
\item $\Pi_{(a_1,...,a_k)}(R)$: If the projection includes the primary key of R then the primary key of the projection is the same as the primary key of R. Otherwise it is \textsf{null}. \vspace{-.45em}
\item $\bowtie_{\phi (r1,r2)}(R_1,R_2)$: The primary key of the result is the union of the primary key of $R_1$ and $R_2$. 
\item $\gamma_{f,e}(R)$: The primary key of the result is the group by expression $e(R)$.\vspace{-.45em}
\item $R_1 \cup R_2$: Primary key is the primary key of R
\item $R_1 \cap R_2$: Primary key is the primary key of R
\item $R_1 - R_2$: Primary key is the primary key of R
\end{itemize}
\end{definition}
This definition of a primary key for a relational expression, allows us to trace the primary key through the expression tree.
In the subsequent sections, we assume that the Primary Key for all expressions is never null.
Our definition of the primary key is also constructive; that is, if an expression has a null primary key then we modify every projection operation to ensure that primary key of the subrelation is never projected out.

\subsection{Hashing Operator}
If we have a deterministic way of mapping a primary key defined in the previous subsection to a sample we can also ensure that all contributing expressions are also sampled. 
To achieve this we use a hashing procedure.
Let us denote the hashing operator $\eta_{a_1, m}(R)$. 
For all tuples in R, this operator applies a hash function whose range is $[0,1]$ to primary key $a_1$ (which may be a set) and selects those records with hash value less than or equal to $m$.
If the hash function is sufficiently uniform, then $h(a) \le m$ samples on average a fraction $m$ of the tuples.
%This definition is without loss of generality for uniform hash function, as if we have a hash function whose range is the set of integers (as implemented in MySQL or Apache Hive) we can take the absolute value and divide by the maximum integer mapping this range back $[0,1]$. 

To achieve the performance benefits of sampling, we push down the hashing operator through the query tree.
The further than we can push $\eta$ down the query tree, the more operators can benefit from the sampling.
However, it is important to note that for some of the expressions, notably joins and aggregates, tracability is more complex.
It turns out in general we cannot push down even a deterministic sample through those expressions.
We formalize the push down rules below:
\begin{proposition}
(HASH PUSHDOWN). Assuming the query tree satisfies the Lineage Condition, then the following rules can be applied to push $\eta(a)$ down the query tree. 
\begin{itemize}\vspace{-.45em}
\item $\sigma_{\phi}(R)$: Push $\eta$ through \vspace{-.45em}
\item $\Pi_{p,[a_2,...,a_k]}(R)$: Push $\eta $ through \vspace{-.45em}
\item $\bowtie_{\phi (r1,r2)}(R_1,R_2)$: Blocks $\eta $ in general. 
\item $\gamma_{f,e}(R)$: Blocks $\eta$ in general. If the group by expression can be decomposed into $e(R) = a, e'(R)$ then pushdown is possible.\vspace{-.45em}
\item $R_1 \cup R_2$: Push $\eta $ through to both $R_1$ and $R_2$
\item $R_1 \cap R_2$: Push $\eta $ through to both $R_1$ and $R_2$
\item $R_1 - R_2$: Push $\eta $ through to both $R_1$ and $R_2$
\end{itemize}
\end{proposition}
In special cases, we can push the hashing operator down through joins. 

\textbf{Many-to-one Join: } If we have an join with two relations $R_1$ and $R_2$ and we know that for every $r_1 \in R_1$ there is at most one $r_2$ in $R_2$ that satisfies the join condition, then we push $\eta$ down to $R_1$.

\textbf{One-to-one Join: } If we have the previous condition and also the converse is true for every $r_2 \in R_2$ there is at most one $r_1$ in $R_1$, then we push $\eta$ down to $R_1$ and $R_2$.

\textbf{(Semi/Anti)-Join: } We can always push $\eta$ down on Semi-joins. For anti-joins we can push $\eta$ down because we can rewrite the node as $R_1 \dot{-} (R_1 \ltimes R_2) $ and apply the pushdown rules for set difference and Semi-Joins.

\subsection{Running Example: Log Analysis}
Recall the running example \textsf{countView}.
We will describe this example in terms of relational algebra, where the stale view $S$ is \textsf{countView}.
First, we construct a delta view, applying the view definition to the updates.
\[
\delta_{S} = \gamma_{(\countfunc,\Pi_{(\text{videoId})})}(\text{LogInserts} \bowtie_{\text{fk}} \text{Video})
\]
Then, we join this delta view to the stale view.
\[
  S_{tmp} = S \fullouterjoin_{\text{(1 to 1)}} \delta_{S}
\]
Finally, we increment the counts and project the result to get the up-to-date view $S'$.
\[
S' = \Pi_{(\text{videoId},S.\text{viewCount} + \delta_{S}.\text{viewCount})}(S_{tmp})
\]

Based on the defition of the primary key above, (videoId) is the primary key of the view.
We apply the rules above to push down the hash operator:
\[
\eta_{(\text{videoId}), m}(S')
\]
This pushes down all the way to the delta view:
\[
\hat{\delta_{S}} = \gamma_{(\countfunc,\Pi_{(\text{videoId})})}(\text{LogInserts} \bowtie_{\text{fk}} \eta(\text{Video}))
\]
Therefore, both the join and the aggregation ``above" the sampling have to do less computation.
