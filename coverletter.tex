{\noindent \normalsize \bf Dear PVLDB Chair and Referees: }

We thank the reviewers for bringing up these concerns and have revised our paper accordingly. To summarize our changes:
\begin{enumerate}[noitemsep]
\item We have improved the presentation by addressing all of the editing issues identified by the reviewers.
\item We revised the motivation for the work by clarifying the relationship between staleness, query result error, and approximation. In particular, staleness does not affect queries uniformly. Even if the number of updated/inserted/deleted rows is small (e.g 1\% of the view size), the resulting ``error" in a given query can be far greater (e.g $\mid fresh - stale\mid > 1\%$). Currently, the user has no way of measuring this error without full maintenance. SVC not only reduces the error but also bounds the error in confidence intervals allowing the user to know how accurate the query result is.
\item We clarify our specific research contributions in this paper and how they contrast with our prior work.
\item We revised our experimental description to clarify that we indeed used a real dataset with real updates to measure SVC’s performance. 
\end{enumerate}

We address the concerns in detail below: 

\vspace{1.5em}

\textbf{Foremost, during the discussions, the reviewers raised concerns about how often in practice the problem addressed in the paper is really observed? Is there is a real scenario where the updates to the base relations since the last view maintenance can be ever that large to really need serious effort? Are there cases where there may be millions of views and few machines to handle them when this difference would really matter? It will be great if the paper describes such a real world scenario.}

Materialized view maintenance can be very expensive resulting in staleness. Many important use-cases for Materialized Views require creating a large number of views including: visualization, personalization, privacy, and real-time monitoring. The problem with eager maintenance is that every view created by an analyst places a bottleneck on incoming transactions.  Therefore, many major commercial database vendors suggests caution on using eager maintenance when updates are frequent. For example:

\vspace{1.0em}

\emph{Oracle: ``ON COMMIT...The time taken to complete the commit may be slightly longer than usual when this method is chosen. This is because the refresh operation is performed as part of the commit process. Therefore, this method may not be suitable if many users are concurrently changing the tables upon which the materialized view is based."}

\url{http://docs.oracle.com/cd/B19306_01/server.102/b14223/basicmv.htm}

\vspace{1.0em}

\emph{Microsoft SQL: ``Indexed views [Materialized Views] work best when the underlying data is infrequently updated. The maintenance of an indexed view can be greater than the cost of maintaining a table index. If the underlying data is updated frequently, the cost of maintaining the indexed view data may outweigh the performance benefits of using the indexed view. If the underlying data is updated periodically in batches but treated primarily as read-only between updates, consider dropping any indexed views before updating, and rebuilding them afterward. Doing this may improve performance of the updates."}

\url{https://technet.microsoft.com/en-us/library/ms187864%28v=sql.105%29.aspx}

\vspace{1.0em}

Any amount of staleness can significantly bias analysis. Suppose 1\% of the base table’s records have been updated, this does not necessarily mean the queries on the view have 1\% error.  Consider the case where all of those updates correspond to a small subpopulation of frequently queried entities in the database or the data is skewed. For example, in Figure 8, we show that the 75\% quartile error for an update size of 10\% is more than 30\% on a skewed dataset. 

To make matters worse, without SVC, the user has no way of knowing how significant these errors are for a given query, other than an educated guess based on past data. In our current revision, we emphasize that our method gives confidence intervals on the result. SVC uses a small amount of up-to-date data to compensate for staleness (also giving a more accurate answer) via a bounded approximation with confidence intervals. The intuitive reasoning is that SVC turns an unknown systematic bias into random approximation error.

\vspace{1.5em}

\textbf{The presentation needs to be improved further. All reviewers have detailed comments on the presentation improvements. One general comments is to make sure the paper is free of typos.}

We have carefully revised the paper to ensure that it is free of typos. We have also incorporated the reviewers' valuable suggestions on clarifying the presentation.

\vspace{1.5em}

\textbf{The contributions of this paper versus previous work should be made clearer.}

We have added the following clarification to Section 2.2 of the paper

\emph{However, the metaphor of stale MVs as a Sample-and-Clean problem only goes so far and there are several new contributions. In prior work, we modeled data cleaning as a row-by-row user-specified transformation.
This model does not work for missing and superfluous rows in stale MVs. In particular, our sampling method has to account for this issue and we propose a hashing based technique to efficiently materialize a uniform sample even in the presence of missing/superfluous rows. Next, we greatly expand the query processing scope of SampleClean beyond \sumfunc, \countfunc, and \avgfunc queries. Bounding estimates that are not \sumfunc, \countfunc, and \avgfunc, is significantly more complicated. This requires new analytic tools such as a statistical bootstrap estimation to calculate confidence intervals. Finally, we add an outlier indexing technqiue to improve estimates on skewed data.}

\vspace{1.5em}

\textbf{Reviewer 3 still has concerns regarding evaluation that needs to be explained better.}

We clarified that we evaluate our approach on a 1TB dataset from Conviva. As TPCD is a synthetic dataset, the purpose of this experiment was to illustrate SVC’s applicability to real data distributions. The Conviva dataset is a 1TB user activity log. We derived materialized views based on actual analyst queries. Many queries were nested queries so we factored out inner queries as materialized views. For example, V5 was based on a nested query that groups users from similar regions/service providers together then aggregates over error types. The updates that we use are real updates corresponding to new log entries streaming in. For example, we create a view based on the first 800GB of data and use the remaining 200GB as the set of updates. 

\vspace{2.0em}

\textbf{[Additional details]}

\textbf{Reviewer 2 suggested: “Please revise the definition of staleness to make it precise. Although the reader can probably tell from the definition of staleness what is meant, the notation is misleading, since $u$ is defined as a set of attributes (not even attribute *values*).”}

We revised the definition to be the following:

Formally, for a stale view $S$ with primary key $u$ and an up-to-date view $S'$:
\begin{itemize}[noitemsep] \sloppy
	\item \textbf{Incorrect: } Incorrect row errors are the set of rows (identified by the primary key) that are updated in $S'$. For $s \in S$, let $s(u)$ be the value of the primary key. An incorrect row is one such that there exists $s' \in S'$ with $s'(u) = s(u)$ and $s \ne s'$.
	\item \textbf{Missing: } Missing row errors are the set of rows (identified by the primary key) that exist in the up-to-date view but not in the stale view. For $s' \in S'$, let $s'(u)$ be the value of the primary key. A missing row is one such that there does not exist a $s \in S$ with $s(u) = s'(u)$.
	\item \textbf{Superfluous: } Superfluous row errors are the set of rows (identified by the primary key) that exist in the stale view but not in the up-to-date view. For $s \in S$, let $s(u)$ be the value of the primary key. A superfluous row is one such that there does not exist a $s' \in S'$ with $s(u) = s'(u)$.
\end{itemize}

\vspace{1.5em}

\textbf{The presentation needs to be improved further. The motivation is not clear now. Materialized views will be incrementally maintained from time to time and brought up to date. So, the use of a sampling-based method is between such updates. The question is how important it is to return an accurate result. While it is always better to return accurate results --- and that argument is where the strong point of this work lies --- the paper does not show the real need for this work. That is, if the errors are small, then a sampling based update method will improve the estimate by a small amount. And, if the errors are large, then the method does not do that well again. So the motivation needs to be made clearer.}

\noindent (1) For a fixed sampling ratio, for any update size, the expected relative error in estimation is bounded (Figure 6b) because we can take the best of SVC+CORR and SVC+AQP. 

\noindent  (2) Even for an update size that is 40\% of the base data, the average query result error with SVC is less than 3\%.

\noindent  (3) There are numerous applications in which bounded error is more desirable even for a little bit of extra computation. Consider real time analytic dashboards that monitor constantly changing data streams. There might be numerous views that need to be refreshed for up-to-date information making it infeasible to apply eager maintenance. We argue that the confidence intervals provide analysts with the guidance they need to determine the statistical significance of a trend.

\vspace{1.5em}

\textbf{Reviewer 3 suggested: The evaluation must be more realistic. The improvement of performance from 56 seconds to 7.5 seconds is not that exciting. Does it really matter? It could for larger datasets and larger workloads of queries. But, what actually happens when such datasets and queries are used is not demonstrated well. It would be interesting to see the difference on a real workload.}

We clarified the details of the Convia experiment which we believe demonstrates applicability in a real setting. The Conviva dataset is a 1TB user activity log. We derived materialized views based on actual analyst queries. Many queries were nested queries so we factored out inner queries as materialized views. For example, V5 was based on a nested query that groups users from similar regions/service providers together then aggregates over error types. For this view, eager maintenance required 858 seconds while sample maintenance required 96 seconds. 

\vspace{1.5em}

\textbf{Reviewer 3 suggested: The so-called Conviva dataset is a bit contrived too. First, are the updates real updates? Or artificial ones generated by the authors? I would assume that the view would be updated every day. So, all we are talking about is customer information since the view as updated, i.e., for the last day. Is it that crucial? At least the paper should paint the picture why.}

The Conviva dataset is a 1TB dataset of customer activity data from the company. The updates that we use are real updates corresponding to new log entries streaming in. For example, we create a view based on the first 800GB of data and use the remaining 200GB as the set of updates. The purpose of this experiment was to illustrate SVC’s applicability to real data distributions and scales. One of our views (V5), calculates aggregate error statistics over all users. Fresh results for this view would better allow the company's developers to understand the impact of new code changes or browser updates. 

\vspace{1.5em}

\textbf{Reviewer 3 suggested:  If you maintain a view, then it takes a bit more time, but, that effort would be used by all future queries to give better quality answers. Whereas the work done by SVC+CORR for drawing the sample is all gone and largely cannot be used for future queries. Or at least, the work does not show how the samples can be saved and reused later on and what impact that has. So, the real evaluation should not be on individual queries, but, on a workload of real queries and then it will possibly lessen the savings of the sampling-based method versus the view maintenance-based method. Would this sampling-based method then be useful enough? The paper should address this issue --- that has not been done very convincingly.}

We apologize if it was unclear, but SVC only materializes an up-to-date sample view once and then all subsequent queries are processed using this sample. SVC+AQP only queries the up-to-date sample, and therefore, is guaranteed to be faster than querying the full up-to-date view. SVC+CORR adds a small amount of overhead (query the old view and query the sample), this overhead is small compared to the overall maintenance time (Figure 6a).

\vspace{1.5em}

\textbf{Reviewer 3 suggested: The technical contribution of the paper could also have been improved. To me, the main contribution is to see if sampling can be pushed down. In the case of a join and the hashing on the primary key, when exactly can the operation be pushed down and when not. An example of when the things cannot be pushed down would have been useful. }

The sampling pushdown proposed in this paper is analogous to predicate push-down operations used query optimizers. The case when the sampling cannot be pushed down
is when the hash operator depends on values in two different relations. Consider the all-pairs join $A \times B$, a single hash of the keys $(k_a, k_b)$ cannot be pushed down to either A or B. However, we do believe this can be done with multiple pair-wise independent hashes and hope to explore this in future work.

\vspace{1.5em}

\textbf{Reviewer 3 suggested:I would also like things to be worked out carefully. Theorems and propositions about what type of sampling can be done for what type of operations would be useful. Correctness and error bounds should be proved too.}

We have clarified that our approach uses hashing which is a specific form of uniform sampling called Bernoulli sampling (refer Nirkhiwale et al. VLDB 2013). In terms of correctness, we cite (Cui and Widom 2003) for ``Proposition 1". In the interest of space, we have clarified Definition 1 such that it clearly follows from the technique and does not require a proof.
\begin{itemize}[noitemsep]
\item Uniformity: $\widehat{S'}$ and $\widehat{S}$ are uniform random samples of $S'$ and $S$ respectively with a sampling ratio of $m$
\item Removal of Superfluous Rows: $D = \{\forall s \in \widehat{S} !\exists s' \in S': s(u) = s'(u)\}$, $D \cap \widehat{S'} = \emptyset$ 
\item Sampling of Missing Rows: $I = \{\forall s' \in \widehat{S'} !\exists s \in S: s(u) = s'(u)\}$, $\mathbb{E}(\mid I \cap \widehat{S'} \mid) = m\mid I \mid $ 
\item Key Preservation for Updated Rows: For all $s\in \widehat{S}$ and not in $D$ or $I$, $s' \in \widehat{S}': s'(u) = s(u)$.
\end{itemize}
We prove Lemma 1, and our error bounds follow directly from the Central Limit Theorem and Bootstrap Estimation which are well known results in statistics. We have cited (Agarwal et al. 2013,Agarwal et al. 2014 ) which discuss the conditions under which such estimates are accurate.

\vspace{1.5em}

\textbf{Reviewer 3 suggested: While it is true that large datasets can be approximated using Gaussian models irrespective of the distribution to an extent, a lot of columns would get data from a mixture of Gaussians and approximating it with one Gaussian distribution would lead to serious errors. What happens in this case? Can the sampling still be done?}

We apologize for the confusion and have clarified that the CLT models the distribution of query estimates as Gaussian and not the data. This property has been well known in the database community (Olken et al., Agarwal et al., Hellerstein and Haas). Even in highly skewed datasets, the Gaussian approximation tends to be conservative; for a fixed mean and variance, the Gaussian distribution is the highest entropy distribution. However, as the sample sizes become smaller this approximation no longer holds and looser bounds can be used (e.g. Hoeffding or Markov Inequality). 






























































































 


