\section{Outlier Indexing}\label{outlier}
\reminder{I am a little concerned about the outlier indexing showing up in the title because it sounds like outlier indexing is one of our biggest contributions...}
Sampling update patterns may be sensitive to power-laws and other long-tailed distributions which are common in large datasets\cite{clauset2009power}.
Sampling may also hide any outliers, records with abnormally large or small attribute values.
Since oultiers may occur very rarely, they are unlikely to be represented in a small sample. 
We address this problem using a technique called outlier indexing which has been applied in SAQP \cite{chaudhuri2001overcoming}.
The basic idea is that we create an index of outlier records and ensure that these records are included in the sample.

\subsection{Building The Outlier Index}
The first step is that the user selects an attribute of the base table to index and specifies a threshold $t$ and a size limit $k$.
In a single pass of updates, the index is built storing references to the records with attributes greater than $t$.
If the size limit is reached, the incoming record is compared to the smallest indexed record and if it is greater then we evict the smallest record.
The same approach can be extended to attributes that have tails in both directions by making the threshold $t$ a range, which takes the highest and the lowest values.
However, in this section, we present the technique as a threshold for clarity.

To select the threshold, there are many heuristics that we can use.
For example, we can use our knowledge about the dataset to set a threshold.
Or we can use prior information from the base table, a calculation which can be done in the background during the periodic maintenance cycles.
If our size limit is $k$, we can find the records with the top $k$ attributes in the base table as to set a threshold to maximally fill up our index. 
Then, the attribute value of the lowest record becomes the threshold $t$.

\subsection{Adding Outliers to the Sample}
Since we index the base table, all of the materialized views in the system share a common set of these outlier indices.
We discuss how to ensure that these records are added to the sample and what overhead this introduces.
For \spview and \fjview, we sample the records that are inserted into the view.
In the same pass as the sample, we can test each record against the outlier indices. 
If the record exists in any of the indices it is added to the sample with a flag indicating that it is an outlier.

For \aggview, we sample the update pattern by taking a hash of the group keys.
If a record is in the outlier index, we must ensure that all records with its group key are also added to the sample.
To achieve this, we have to select all of the distinct group keys in the outlier index and add those aggregates to the sample.
As before, these records are marked with a flag denoting they are outliers.

Outlier indexing adds additional overhead since for \spview and \fjview it adds the overhead of a hash table lookup for each record, and for Aggregation views it further requires a single initial scan of the entire index. 
However, we envision that in most datasets the outlier index will be very small making this overhead negligible.
In our experiments, we show that even a very small outlier index (less than .01\% of records) is sufficient to greatly improve the accuracy of
correction estimates.

\reminder{Have you talked about how to deal with the case that a record is both sampled and indexed as an outlier?}

\subsection{Query Processing with the Outlier Index} 
The outlier index has two uses: (1) we can query all the rows that correspond to outlier rows, 
and (2) we can improve the accuracy of our \emph{aggregation} queries.
To query the outlier rows, we can select all of the rows in the materialized view that are flagged as outliers, and these rows are guaranteed to be up-to-date.

We can also incorporate the outliers into our correction estimates. 
We can use our outlier flag to ensure that we do not double count them in our results, thus avoiding bias. \reminder{``thus avoiding bias" is a little confusing. Maybe you can remove it.}
By guaranteeing that certain rows are in the index, we
have to merge a deterministic result (set of outlier rows) with the
estimate\reminder{This sentence is a little hard to follow although I know what you mean.}. 
This process truncates the distribution and it can be modeled as computing a query
result on regular records and on the outliers separately; and then combining them together with
a weighted average. 
If our dataset size is $N$, and we have $l$ outliers. \reminder{It sounds like $N$ and $l$ are irrelevant to the query. Is it true?}
Let $v_{reg}$ be the query result for the regular records, and $v_{out}$ is the query result for outliers, then:
\[
 v = \frac{N-l}{N}v_{reg} + \frac{l}{N}v_{out}
\]
We can use this method improve the accuracy of our correction estimates, by calculating $\dans$ 
on the outliers and the regular records seperately then averaging them together. 
%See \cite{chaudhuri2001overcoming} for additional query processing details.
\reminder{I removed the citation since it looks like this is our paper...}