\section{Outlier Indexing}
Sampling has been shown to be sensitive to outliers [?].
Since oultiers may occur very rarely, they are unlikely to be represented in a sample.
Thus, even though these elements can greatly affect a query result, they may be absent from our approximation.
Power-law and long-tailed distributions are a common feature in large datasets[?].
Such distributions often have variances that are orders of magnitude larger
than the mean.
As a result, answering queries such as SUM, COUNT, AVG may require a large 
sample size for sufficient accuracy.

Furthermore, the outliers themselves are interesting, and often these are the
most frequently queried records.
An application of particular interest for up-to-date query results
is outlier detection. When we have growing datasets, for example activity
logs, we may want to know which records correspond to abnormal activity.

The key question is how can we incorporate guarantees about outliers
and efficiency guarantees on long tails in our estimation framework.
The approach is to build an outlier index on the base relation similar to that proposed in prior work [?].
We extend this work to the context of materialized views where the outlier index guarantees
that rows in the view derived from the outlier base record are in the sample.
However, the materialized view setting poses a few new challenges: (1) we can take only
a single pass of the updates, (2) outliers must be indexed from the base relations, and
(3) the approach should scale with the update rate.

Our solution is a set of user specified thresholds on attributes in the base table, and possibly a storage size constraint.
Any record whose attribute value exceeds the threshold is added to the index until such time the index hits the size constraint.
Then only the largest values are retained.

Recall, our example base table:
\begin{lstlisting}
Log(sessionID, videoID, responseTime, userAgent)
\end{lstlisting}
and the selection view:
\begin{lstlisting}
View1 := SELECT * FROM Log 
WHERE userAgent 
LIKE '%Mozilla%'
\end{lstlisting}
If the user creates an outlier index on ``responseTime" with a threshold $t=100ms$, then
the records containing with values of greater than 100ms of response time are guaranteed to be 
in the sample of the delta view regardless of the sample size.

\subsection{Building The Outlier Index}
A key aspect of our outlier index is that it is derived on the base table, yet 
can still give benefits to queries on the view.
This has a few advantages: (1) the outlier index does not require materialization of the full delta table,
(2) the cost of building the index amortizes over all the materialized views, and (3) it can be built with a 
single pass of the updates.
The index also can be used by all views derived on the base table.

The first step is that the user selects an attribute to index and specifies a threshold $t$ and a storage constraint $k$.
In a single pass of only the inserted records, the index is built storing references to the records with attributes greater than $t$.
If the storage size limit is reached, the incoming record is compared to the smallest indexed record and if it is greater then we evict the smallest record.
For Select-Project and Foreign-Key Join views, this is sufficient as we can additionally iterate through the index when sampling and mark 
which records satisfy the view definition and perform any neccessary joins. 
This adds an additional scanning cost to the creation of each view, but we envision that in most datasets the outlier index will be
very small.
In our experiments, we show that even a very small outlier index (less than .01\% of records) is sufficent to greatly improve the accuracy of
correction estimates.

For aggregation views, at sample creation time, we must ensure that all rows in the materialized view that are derived from a record in the outlier index are added to the view. 
This means that we have to run a COUNT(DISTINCT) for the group by key of the view on the outlier index and then ensure that all of those keys are added to the view (that is ensure they are sampled).

\subsection{Query Processing with the Outlier Index}
The outlier index has two uses: (1) we can answer SELECTION queries on the outliers, 
and (2) we can improve the accuracy of our AGGREGATION queries.
For selection queries, we must simply check to see if the record satisfying the query is in the outlier index.
While the outlier index is small, outliers (and their derived materialized view rows) are often the records that
we want to query.
To see a more general handling of Selection Queries, see Section [?].

We can also incorporate the outliers into our estimates of the correction
$\epsilon$ for aggregation queries. By guaranteeing that certain rows are in the index, we
have to merge a deterministic result (set of outlier rows) with the
estimate. One way to think of this is that we have $\epsilon$ is
calculated from the set of records that are not outliers. Let $|V_{T}^{'}|$
be the size of the updated view, $l$ be the number of rows in the
outlier index, and let $D_{o}$ be the set of differences (as defined
in Section ?) for the rows in the outlier index. We can update $\epsilon$
with the outlier information by:
\[
\frac{|V_{T}^{'}|}{|V_{T}^{'}|-l}\epsilon+c\cdot f(D_{o})
\]


\subsection{Increased Accuracy For Long-Tailed Distributions}
This outlier indexing procedure can greatly increase the accuracy
of estimates where the set of difference is heavy tailed. This approach
has been well studied in AQP {[}?{]} and is called truncation in Statistics
{[}?{]}. The intuition is that by removing the tail, you are reducing
the variance of the distribution, and thus, making it easier to estimate
an aggregate from a sample.