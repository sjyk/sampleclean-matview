\section{Outlier Indexing}
One feature of many large datasets is long-tailed distributions[?].
Such distributions pose a challenge to sample-based approaches as the
distributions often have variances that are orders of magnitude larger
than the mean.
As a result, answering queries such as SUM, COUNT, AVG may require a large 
sample size for sufficient accuracy.

Furthermore, the outliers themselves are interesting, and often these are the
most frequently queried records.
An application of particular interest for up-to-date query results
is outlier detection. When we have growing datasets, for example activity
logs, we may want to know which records correspond to abnormal activity.

The key question is how can we incorporate guarantees about outliers
and efficiency guarantees on long tails in our estimation framework.
The approach is to build an outlier index similar to that proposed in prior work [?].
The user specifies an attribute in the base table and a percentage $o\%$, and we 
identify the records whose attribute value is in the top $o$ percentile.
We then construct an index to ensure that rows in the materialized view
that are derived from the top-o percentile records are guaranteed to be in the sample.

Recall, our example base table:
\begin{lstlisting}
Log(sessionID, videoID, responseTime, userAgent)
\end{lstlisting}
and the selection view:
\begin{lstlisting}
View1 := SELECT * FROM Log 
WHERE userAgent 
LIKE '%Mozilla%'
\end{lstlisting}
If the user creates an outlier index on ``responseTime" with $o=1\%$, then
the records containing the top 1\% of response times are guaranteed to be 
in the sample of the delta view regardless of the sample size.

\subsection{A Framework For Outlier Indexing}
A key aspect of our outlier index is that it is derived on the base table, yet 
can still give benefits to queries on the view.
This has a few advantages: (1) the outlier index does not require materialization of the full delta table,
(2) the cost of building the index amortizes over all the materialized views, and (3) it can be built with a 
single pass of the updates.
In principle, other outlier detection schemes [?] can be applied to build the index, but in this work, we 
implement and analyze an index of top o\%. 
In attributes with both positive and negative values, this can also be the records with top o\% in absolute value terms.

\subsection{Building The Outlier Index}
The first step is that the user selects an attribute to index and specifies the parameter $o\%$.
In a single pass of only the inserted records, the index is built storing references to the records in the top o\%.
For Select-Project and Foreign-Key Join views, this is sufficient as we can additionally iterate through the index when sampling and mark 
which records satisfy the view definition and perform any neccessary joins. 
This adds an additional scanning cost of $o\%$ to the creation of each view, but we envision that in most datasets the outlier index will be
very small.
In our experiments, we show that even a very small outlier index (less than .01\% of records) is sufficent to greatly improve the accuracy of
correction estimates.

For aggregation views, at sample creation time, we must ensure that all rows in the materialized view that are derived from a record in the outlier index are added to the view. 
This means that we have to run a COUNT(DISTINCT) for the group by key of the view on the outlier index and then ensure that all of those keys are added to the view (that is ensure they are sampled).

\subsection{Query Processing with the Outlier Index}
The outlier index has two uses: (1) we can answer SELECTION queries on the outliers, 
and (2) we can improve the accuracy of our AGGREGATION queries.
For selection queries, we must simply check to see if the record satisfying the query is in the outlier index.
While the outlier index is small, outliers (and their derived materialized view rows) are often the records that
we want to query.
To see a more general handling of Selection Queries, see Section [?].

We can also incorporate the outliers into our estimates of the correction
$\epsilon$ for aggregation queries. By guaranteeing that certain rows are in the index, we
have to merge a deterministic result (set of outlier rows) with the
estimate. One way to think of this is that we have $\epsilon$ is
calculated from the set of records that are not outliers. Let $|V_{T}^{'}|$
be the size of the updated view, $l$ be the number of rows in the
outlier index, and let $D_{o}$ be the set of differences (as defined
in Section ?) for the rows in the outlier index. We can update $\epsilon$
with the outlier information by:
\[
\frac{|V_{T}^{'}|}{|V_{T}^{'}|-l}\epsilon+c\cdot f(D_{o})
\]


\subsection{Increased Accuracy For Long-Tailed Distributions}

This outlier indexing procedure can greatly increase the accuracy
of estimates where the set of difference is heavy tailed. This approach
has been well studied in AQP {[}?{]} and is called truncation in Statistics
{[}?{]}. The intuition is that by removing the tail, you are reducing
the variance of the distribution, and thus, making it easier to estimate
an aggregate from a sample.