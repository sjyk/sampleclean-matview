\section{Outlier Indexing}
One feature of many large datasets is long-tailed distributions[?].
Such distributions pose a challenge to sample-based approaches as the
distributions often have variances that are orders of magnitude larger
than the mean.
As a result, answering queries such as SUM, COUNT, AVG may require a large 
sample size for sufficient accuracy.

Furthermore, the outliers themselves are interesting, and often these are the
most frequently queried records.
An application of particular interest for up-to-date query results
is outlier detection. When we have growing datasets, for example activity
logs, we may want to know which records correspond to abnormal activity.

The key question is how can we incorporate guarantees about outliers
and efficiency guarantees on long tails in our estimation framework.
The approach is to build an outlier index similar to that proposed in prior work [?].
The user specifies an attribute in the base table and a percentage $o\%$, and we 
identify the records whose attribute value is in the top $o$ percentile.
We then construct an index to ensure that rows in the materialized view
that are derived from the top-o percentile records are guaranteed to be in the sample.

Recall, our example base table:
\begin{lstlisting}
Log(sessionID, videoID, responseTime, userAgent)
\end{lstlisting}
and the selection view:
\begin{lstlisting}
View1 := SELECT * FROM Log 
WHERE userAgent 
LIKE '%Mozilla%'
\end{lstlisting}
If the user creates an outlier index on ``responseTime" with $o=1\%$, then
the records containing the top 1\% of response times are guaranteed to be 
in the sample of the delta view regardless of the sample size.

\subsection{A Model For Outlier Indexing}

We propose the following model for detecting and indexing outliers.
When creating a view, the user specifies an attribute in the base
relation to ``outlier index''. What this means is the that the records
with the $l$ largest attribute values are guaranteed to be included
in the sample view. In this outlier model, we detect outliers in updates
with a single pass and without having to build the entire delta table.
For aggregation views, for the records that are indexed as outliers,
we simply add those group by keys to the sample.


\subsection{Query Processing with the Outlier Index}

We can incorporate the outliers into our estimates of the correction
$\epsilon$. By guaranteeing that certain rows are in the index, we
have to merge a deterministic result (set of outlier rows) with the
estimate. One way to think of this is that we have $\epsilon$ is
calculated from the set of records that are not outliers. Let $|V_{T}^{'}|$
be the size of the updated view, $l$ be the number of rows in the
outlier index, and let $D_{o}$ be the set of differences (as defined
in Section ?) for the rows in the outlier index. We can update $\epsilon$
with the outlier information by:
\[
\frac{|V_{T}^{'}|}{|V_{T}^{'}|-l}\epsilon+c\cdot f(D_{o})
\]


\subsection{Increased Accuracy For Heavy-Tailed Distributions}

This outlier indexing procedure can greatly increase the accuracy
of estimates where the set of difference is heavy tailed. This approach
has been well studied in AQP {[}?{]} and is called truncation in Statistics
{[}?{]}. The intuition is that by removing the tail, you are reducing
the variance of the distribution, and thus, making it easier to estimate
an aggregate from a sample.