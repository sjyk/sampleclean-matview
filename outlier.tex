\section{Outlier Indexing}\label{outlier}
Sampling is known to be sensitive to outliers \cite{clauset2009power, chaudhuri2001overcoming}.
SVC may be sensitive to power-laws and other long-tailed distributions which are common in large datasets\cite{clauset2009power}.
Since outliers may occur very rarely, they are unlikely to be represented in a small sample. 
We address this problem using a technique called outlier indexing which has been applied in SAQP \cite{chaudhuri2001overcoming}.
The basic idea is that we create an index of outlier records and ensure that these records are included in the sample.
However, as this has not been explored in the materialized view setting there are new challenges in using this index for improved result accuracy.

\subsection{Indices on the Base Relations}
In SVC, we cannot index the sample in the same way as \cite{chaudhuri2001overcoming}. 
This is because we only materialize the sample view and we have no idea about outliers outside of this sample.
Instead, we propose building indices on the base relations.

The first step is that the user selects an attribute of any base relation to index and specifies a threshold $t$ and a size limit $k$.
In a single pass of updates (without maintaining the view), the index is built storing references to the records with attributes greater than $t$.
If the size limit is reached, the incoming record is compared to the smallest indexed record and if it is greater then we evict the smallest record.
The same approach can be extended to attributes that have tails in both directions by making the threshold $t$ a range, which takes the highest and the lowest values.
However, in this section, we present the technique as a threshold for clarity.

To select the threshold, there are many heuristics that we can use.
For example, we can use our knowledge about the dataset to set a threshold.
Or we can use prior information from the base table, a calculation which can be done in the background during the periodic maintenance cycles.
If our size limit is $k$, we can find the records with the top $k$ attributes in the base table as to set a threshold to maximally fill up our index. 
Then, the attribute value of the lowest record becomes the threshold $t$.

\subsection{Adding Outliers to the Sample}
Given this index, the next question is how we can use this information in our materialized views.
We ensure that any row in a materialized view that derives from an indexed record is guaranteed to be in the sample.
This problem is sort of an inverse to the efficient sampling problem.
We need to propagate the indices upwards through the query tree.

\reminder{TODO}

\subsection{Query Processing with the Outlier Index} 
The outlier index has two uses: (1) we can query all the rows that correspond to outlier rows, 
and (2) we can improve the accuracy of our \emph{aggregation} queries.
To query the outlier rows, we can select all of the rows in the materialized view that are flagged as outliers, and these rows are guaranteed to be up-to-date.

We can also incorporate the outliers into our correction estimates.  
By guaranteeing that certain rows are in the index, we
have to merge two results: one over the outliers and one over the regular records.
For a given aggregation query, let $N$ be the count of records that satisfy the query's condition and $l$ be the number of outliers that satisfy the condition.
Let $v_{reg}$ be the query result for the regular records, and $v_{out}$ is the query result for outliers, then:

\[
 v = \frac{N-l}{N}v_{reg} + \frac{l}{N}v_{out}
\]

We can use this method to improve the accuracy of our correction estimates (Table \ref{tbl:query-correct}), by calculating $\dans$ 
on the outliers and the regular records separately then averaging them together. 
%See \cite{chaudhuri2001overcoming} for additional query processing details.