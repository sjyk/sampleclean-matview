\section{Outlier Indexing}\label{outlier}
In the previous two sections, we described how we can sample updates to a materialized 
view and use information from that sample to correct stale queries.
We argued that this approach models incremental view maintenance as a data cleaning problem,
where we ``clean" a query result using a sample of up-to-date data.

One problem is that the presented approach gives bounds only for aggregation queries on the view.
This is a known limitation of all sampling-based approaches, including SAQP.
However, an application of particular interest for up-to-date query results is outlier detection. 
When we have growing datasets, for example activity logs, we may want to know which records correspond to abnormal activity.
Often times, selection queries on materialized views are ones that correspond to outliers.
The lack of support for these queries means that outliers or anomolies may be easily missed.
%We define outliers to mean records whose attribute values deviate from the normal range.

In addition, to answering selection queries, sampled estimates have been shown to be sensitive to outliers even for aggregation queries [?].
Since oultiers may occur very rarely, they are unlikely to be represented in a sample.
Thus, even though these elements can greatly affect a query result, they may be absent from our approximation.
Power-law and long-tailed distributions are a common feature in large datasets[?].
Such distributions often have variances that are orders of magnitude larger
than the mean.
As a result, answering queries such as SUM, COUNT, AVG may require a large 
sample size for sufficient accuracy.

Our solution is to build an outlier index on the base table similar to that proposed in prior work [?].
We extend this work to the context of materialized views where the outlier index guarantees
that rows in the view derived from the outlier base record are included in the sample.
However, the materialized view setting poses a few new challenges: (1) we can take only
a single pass of the updates, (2) outliers must be indexed from the base relations, and
(3) the approach should scale with the update rate.
We address these challenges by allowing for user specified thresholds on attributes in the base table, and possibly a storage size constraint.
Any record whose attribute value exceeds the threshold is added to the index until such time the index hits the size constraint.
Then only the largest values are retained in the index.
The same approach can be extended to attributes that have tails in both directions by making the threshold a range, which takes the highest and the lowest
values.
In our experiments, we work with datasets with Zipfian distributed attributes and thus a single threshold indexing the largest records are enough.
Therefore, we present the indexing rule as a threshold, but it can easily be extended to a range.

To make the process more concrete, recall, our example base table:
\begin{lstlisting}
Log(sessionID, videoID, responseTime, userAgent)
\end{lstlisting}
and the selection view:
\begin{lstlisting}
View1 := SELECT * FROM Log 
WHERE userAgent 
LIKE '%Mozilla%'
\end{lstlisting}
If the user creates an outlier index on ``responseTime" with a threshold $t=100ms$, then
the records containing with values of greater than 100ms of response time are guaranteed to be 
in the sample of the delta view regardless of the sample size.

Outlier indexing clearly presents a tradeoff between increased accuracy and support for selection queries and degraded performance gains from sampling. 
As the index grows in size, we lose the benefits of sampling.
However, in our experiments, we empirically found that very small indices were sufficent.

\subsection{Building The Outlier Index}
Our outlier index is that it is derived on the base table, yet 
can still give benefits to queries on the view.
This has a few advantages: (1) the outlier index does not require materialization of the full delta table,
(2) the cost of building the index amortizes over all the materialized views, and (3) it can be built with a 
single pass of the updates.
The index also can be used by all views derived on the base table.

The first step is that the user selects an attribute to index and specifies a threshold $t$ and a storage constraint $k$.
In a single pass of only the inserted records, the index is built storing references to the records with attributes greater than $t$.
If the storage size limit is reached, the incoming record is compared to the smallest indexed record and if it is greater then we evict the smallest record.
For Select-Project and Foreign-Key Join views, this is sufficient as we can additionally iterate through the index when sampling and mark 
which records satisfy the view definition and perform any neccessary joins. 
This adds an additional scanning cost to the creation of each view, but we envision that in most datasets the outlier index will be
very small.
In our experiments, we show that even a very small outlier index (less than .01\% of records) is sufficent to greatly improve the accuracy of
correction estimates.

For aggregation views, at sample creation time, we must ensure that all rows in the materialized view that are derived from a record in the outlier index are added to the view. 
This means that we have to additionally select all distinct group by keys in outlier index and ensure that they are sampled.

\subsection{Selecting the Threshold}
To select the threshold, we can use information from the base table.
If our storage constraint is $k$, we can find the records with the top $k$ attributes in the base table. 
Then, the attribute value of the lowest record becomes the threshold $t$.

This approach is, of course, a heuristic which assumes that the updates are distributionally similar to the 
base relation. 
However, the storage constraint $k$ prevents a loss in performance of our approach due to unexpectedly large updates.
Furthermore, calculating this heuristic requires a scan of the base table.
This can be done in the background during the periodic maintenance cycles.

\subsection{Query Processing with the Outlier Index} \reminder{Make this consistent with JW's notation}
The outlier index has two uses: (1) we can answer \emph{selection} queries on the outliers, 
and (2) we can improve the accuracy of our \emph{aggregation} queries.
For selection queries, we must simply check to see if the record satisfying the query is in the outlier index.
While the outlier index is small, outliers (and their derived materialized view rows) are often the records that
we want to query.

We can also incorporate the outliers into our estimates of the correction
$\epsilon$ for aggregation queries. By guaranteeing that certain rows are in the index, we
have to merge a deterministic result (set of outlier rows) with the
estimate. One way to think of this is that we have $\epsilon$ is
calculated from the set of records that are not outliers. Let $|V_{T}^{'}|$
be the size of the updated view, $l$ be the number of rows in the
outlier index, and let $D_{o}$ be the set of differences (as defined
in Section ?) for the rows in the outlier index. We can update $\epsilon$
with the outlier information by:
\[
\frac{|V_{T}^{'}|}{|V_{T}^{'}|-l}\epsilon+c\cdot f(D_{o})
\]


\subsection{Increased Accuracy For Long-Tailed Distributions}
This outlier indexing procedure can greatly increase the accuracy
of estimates where the set of difference is heavy tailed. This approach
has been well studied in AQP {[}?{]} and is called truncation in Statistics
{[}?{]}. The intuition is that by removing the tail, you are reducing
the variance of the distribution, and thus, making it easier to estimate
an aggregate from a sample.