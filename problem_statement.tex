\section{Background}\label{sec-background}
In this section, we briefly overview the current challenges with view maintenance and
our prior work in scalable data cleaning.

\subsection{Materialized View Maintenance}\label{subsec-inc}
Views define logical tables which can be queried instead of physical base relations.
Materialized Views are a class of views that are pre-computed and stored (i.e materialized).
Any form of pre-computed, derived data encounters challenges with staleness when the physical base data changes.

One approach to this problem is to recompute the materialized view every time there are updates to the base tables.
However, this approach is very inefficient if updates to the data generally have small or sparse effect on the materialized view. 
A contrasting approach is incremental view maintenance (IVM), where rows in the materialized views are incrementally updated based on the updates to the base table.
Incremental maintenance of materialized views has been well studied; see \cite{chirkova2011materialized} for a survey of the approaches. 
At a high-level, incremental maintenance algorithms typically consist of the following steps: (1) maintain a cache of insertions and deletions for each physical base table, then using the view definition derive a \emph{change propagation formula} in terms of the set of insertions and deletions, and finally apply the formula to the materialized view.
For a variety of view types, these rules are described in detail in \cite{DBLP:journals/vldb/KochAKNNLS14, DBLP:conf/pods/Koch10}.

Incremental maintenance may not be efficient in all cases.
Consider the relation R(employeeid,country,salary) and view that calculates the \maxfunc salary of employees grouped by country. 
However, if we allow deletions then it is unclear how to update a view if an employee with a maximal salary is deleted since we do not know the salary of the next highest employee. 
In general, to ensure correctness the view has to store the entire set of salaries for each country.
Along the lines of this example, especially when considering views with non-distributive aggregates and non-monotone operations, there are 
cases when recomputation may require less storage of state or even less computation.

So materialized views are maintained either with incremental maintenance, recompution, or a mix.
We formalize this process as a \emph{maintenance strategy} $\mathcal{M}$.
\begin{definition} MAINTENANCE STRATEGY.
Let, $\mathcal{D}$ be a database which is a collection of relations $\{R_i\}$.
Let, $S$ be a materialized view.
We denote the insertions to a relation $R_i$ as $\Delta R_i$ and the deletions as $\nabla R_i$.
A maintenance strategy $\mathcal{M}$ is a function of the database $\mathcal{D}$, $S$, and all the delta relations $\{\Delta R_i\} \cup \{\nabla R_i\}$ that returns a relational expression the execution of which updates the view.
\end{definition}

SVC provides a framework that can analyze a maintenance strategy, and only apply updates to a random sample of the stale view.

\subsubsection{Practical Considerations}
The algebraic analysis of incremental maintenance \cite{DBLP:journals/vldb/KochAKNNLS14, DBLP:conf/pods/Koch10} informs us which views can be incrementally maintained.
However, there are many practical considerations of excuting these operations in real database systems and it may not always be feasible to immediately apply updates; even if theoretically possible.
For example, in distributed systems record-at-a-time maintenance may be excessively affected by overheads (such as coordination and serialization) and batching updates together can allows for amortization.
In other systems, such as Apache Spark, the immutability of data structures means that updates must be processed synchronously and old views must be garbage collected further exacerbating this problem.
Finally, immediate scheduling of maintenance can place a bottleneck on updates to the base table which may result in degraded performance system-wide.

The main problem is that while immediate incremental maintenance has many advantages, the particulars of the database system and available resources often dictate how updates are propagated.
To address these challenges, deferred maintenance is an alternative solution and often preferred solution.
The main insight of deferral is to avoid maintaining the view immediately and to schedule an update at a more convenient time either in a pre-set way or adaptively.
In deferred maintenance approaches, the user often accepts some degree of staleness for additional flexibility in scheduling.
For example, views can be updated at night when the system can use more resources to process the maintenance without affecting a critical application.
However, this also means that during the day the materialized view becomes increasingly stale as it was computed the night before.

These costs can also be deferred to query execution time.
In particular, we highlight a technique called lazy maintenance which applies updates to the view only when a user's query requires a row \cite{zhou2007lazy}.
While always fresh, both lazy maintenance and immediate maintenance hit a bottleneck when there are rapid updates, and this results increasingly degraded performance if a user wants to query a view.
The alternative is a periodic strategy, but this means that there is unbounded error on queries between maintenance periods.

The data cleaning perspective that SVC offers on this problem is that there is a tradeoff between accuracy and computation.
By using sampling, we give the user access to a new tradeoff space between immediate (or close to immediate i.e. mini-batch) maintenance and long-periodic maintenance.

\subsection{SampleClean: Fast and Accurate Query Processing on Dirty Data}
In our prior work on the SampleClean project, we proposed a framework for scalable data cleaning.
Similar to the accuracy-performance contrast between incremental maintenance and periodic maintenance in the materialized view setting, data cleaning also faces a similar challenge.
Traditionally, data cleaning has explored expensive, up-front cleaning of entire datasets for increased query accuracy, and those who were unwilling to pay the full cleaning cost avoided data cleaning altogether.
We proposed SampleClean to add an additional tradeoff to this design space by using sampling.

SampleClean (Figure \ref{sc}) has three parts: (1) sampling, (2) data cleaning, and (3) query result estimation.
First, SampleClean creates a sample of dirty data (which are erroneous, missing, or otherwise corrupted records).
Then, the framework applies a data cleaning operation to the sample.
Finally, when users query the dataset, the framework uses the clean sample to extrapolate clean query results.
The key challenge is that data cleaning can potentially change the statistics of a sample and the queries need to compensate for those effects.
In our initial work, SampleClean focused on three common aggregates: \sumfunc, \avgfunc, and \countfunc queries.

In SampleClean, we noticed that there were two contrasting query processing approaches, which we termed: \textbf{RawSC} and \textbf{NormalizedSC}.
In \textbf{RawSC}, we could apply queries, with some re-weighting, directly to the clean sample of data.
This approach is very similar to that taken in Sample-based Approximate Query Processing \cite{OlkenR86,AgarwalMPMMS13, joshi2008materialized}.
\textbf{NormalizedSC}, on the other hand, processed the changes made by the data cleaning and issued ``corrections" to the query results.
We found that these two algorithms tradeoff statistical efficiency and robustness to error; as \textbf{RawSC} only depends on clean data it is robust to the magnitude of data error and as \textbf{NormalizedSC} depends on the delta of the data cleaning it performs well when the data cleaning is sparse. 
In this work, we found that estimating a correction (\textbf{NormalizedSC}) led to lower variance results on real datasets (see Section \ref{exp}). 

\begin{figure}[t] \vspace{-2em}
\centering
 \includegraphics[scale=0.30]{figs/sys-arch2.pdf} \vspace{-.25em}
 \caption{A basic overview of SampleClean. SampleClean uses a random sample of dirty data to learn how a data cleaning algorithm affects queries on the sample. We can then derive a correction to compensate for the dirtiness. \label{sc}}\vspace{-1.75em}
\end{figure}

\subsection{Contributions}
Inspired by SampleClean, SVC samples a stale view, cleans this view by restricting the maintenance strategy to just the rows in the sample, and then applies \textbf{NormalizedSC} to correct the results of queries on the view.
Applying this data cleaning framework to the materialized view setting leads to some interesting theoretical challenges with new insights for both materialized view maintenance and data cleaning.
In materialized views, unlike the models of error studied in SampleClean, staleness can lead to rows that are missing from the ``dirty" view or conversely need to be deleted.
This problem poses an important statistical challenge since to calculate a correction with \textbf{NormalizedSC} we need to measure the difference in rows attribute values after cleaning, and this requires us to define this algorithm when rows are missing from either side of the difference.

In SampleClean, we assumed samples directly from base relations which is straight-forward to implement with a reservoir-style sampling technique.
However, sampling from materialized views is much more complex.
This combined with the challenges of missing and superflous stale rows, led us to design a rule-based sampling optimizer that can ``push down" a sampling operator through a maintenance strategy.

We further use this new setting to explore and formalize the class of queries that our frameworks can support.
It turns out that the framework is more general than the \sumfunc, \avgfunc, and \countfunc queries studied before.
We show that \textbf{NormalizedSC} actually can give meaningful results for \selectfunc queries.
Furthermore, a key result is that any query that can be estimated in an unbiased way from a sample can also be corrected using \textbf{NormalizedSC}.
It turns out that \sumfunc, \avgfunc, and \countfunc are just a special case where optimality with respect to estimate variance holds.
A further insight is that queries without closed form confidence intervals can also be supported with a statistical bootstrap, and we
derive an algorithm to achieve empirical confidence intervals for aggregates such as \texttt{median} and \texttt{corr}.

Finally, in this work, we give an explicit treatment of non-erroneous outliers records.
Sampling is particularly sensitive to variance in the dataset, and large outliers can significantly reduce query accuracy.
We address this problem with an index of outlier records on base relations.
However, an important challenge is to propagate this information upwards to the views and to incorporate this information into \textbf{NormalizedSC}.





