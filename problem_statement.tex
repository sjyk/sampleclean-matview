\section{Background}
Materialized views are stored query results that are used 
to optimize query processing [?].
Due to the decreasing cost of memory, in-memory materialization 
has had much interest in recent research [?] and materialization 
research has expanded beyond the SQL setting [?].

However, pre-computed query results face the obvious challenge of \emph{staleness} when applied
in a setting where the underlying tables are updating.
One commonly applied solution is to recompute the materialized views when the table has been updated.
This can be very expensive in the presence of small updates that hardly change the derived views.
Consequently, incremental maintenance of materialized views is well studied see [?]
for a survey of the approaches. 
A simple model of incremental maintenance consists of two steps: calculating a ``delta" view,
and ``refreshing" the materialized view with the delta.
More formally, given a base relation $T$, a set of updates $U$,
and a view $\textbf{V}_{T}$:

\textbf{Calculate the Delta View- }
In this step, we apply the view definition to the updates and we call
the intermediate result a ``delta'' view.
\[
\Delta\textbf{V}=\textbf{V}_{U}
\]

This is also called a \emph{change propagation formula} in some literature,
especially on algebraic representations of incremental view maintenance.

\textbf{Refresh View- }
Given the ``delta'' view, we merge the results with the existing
view:
\[
\textbf{V}_{T}^{'}=refresh(\textbf{V}_{T},\Delta\textbf{V})
\] 
The details of the refresh operation depend on the view definition.
Refer to [?] for details.

In this work, we address three types of materialized views: Select-Project, Aggregation, and
Foreign-Key Join views; and four common aggregation queries on these
views: SUM, COUNT, AVG, and VAR. We further analyze 
only insertions into the database and defer analysis of updates
and deletions for future work.

\subsection{Scheduling Maintenance}
While often less expensive than recomputing a materialized view,
incremental maintenance can still be computationally expensive.
Materialized views are growing larger and are more frequently 
implemented in distributed systems.
Due to this cost, which we will refer to as the ``maintenance" cost, 
scheduling the refresh operation has been an important topic of research.

There are two principle types of scheduling strategies: immediate and deferred. 
In immediate maintenance, as soon as a record is updated, 
the change is propagated to any derived materialized view.
Immediate maintenance has an advantage that materialized view is always up-to-date, 
however it can be very expensive.
This scheduling strategy places a bottleneck when records are written reducing 
the available write-throughput for the database.
Furthermore, especially in a distributed setting, record-by-record 
maintenance cannot take advantage of the benefits of consolidating overheads by batching.
To address these challenges, deferred maintenance is alternative solution.
In deferred maintenance, the user often accepts some degree of staleness in 
the materialized view for additional flexibility for scheduling refresh operations.
For example, a user can update the materialized view 
nightly during times of low-activity in the system.
More sophisticated deferred scheduling schemes are also possible, refer to [?] for a full survey.

\subsection{A Data Cleaning Approach}
Without consistency guarantees, in many deferred incremental maintenance approaches, 
queries on the materialized view are stale until the refresh.
In fact, in general, the relative error between the stale result and the up-to-date result is unbounded.
But the tradeoff is that deferred maintainence allows flexibility in scheduling to meet the performance constraints
of the system and workload.

To similar effect, sampling has been applied to scale expensive query processing operations in streaming [?], 
aggregate query processing [?], and data cleaning [?].
In particular, the problem of staleness closely resembles a data cleaning problem.
There are rows in materialized view that are incorrect, and there is a procedure to ``clean" these rows.
In the SampleClean project [?], the authors suggested that to process aggregate queries on dirty datasets, 
one need not clean the entire dataset but rather a random sample to get an approximate answer.
Rather than an uncleaned result that can be arbitrarily dirty, you acheive an approximate result with bounded sampling error. 

Similarly, in context of materialized views, we address the key question of whether we have to maintain the entire 
view to answer aggregate queries with bounded error.
In this work, we use sampling to infer up-to-date results for aggregation queries on stale materialized views. 
We can apply incremental maintenance to just a sample of the view, thus greatly reducing the cost needed to maintain the view (Figure ?). 
With the sampling ratio, the user has a knob to tradeoff performance (in terms of throughput and refresh latency) and accuracy.

We envision that this work is complementary to incremental maintenance.
In the periods between full materialized view refreshes, the user maintains a sample of the view. 
The user specifies a sampling ratio that meets the systems performance constraints.
With this, the user gets guarantees that for the aggregate queries the results have bounded error and 
the queries return with error bars.

\subsection{Extending SampleClean For Correcting Staleness}
Given the three categories of views and the SUM, COUNT, AVG, and VAR
queries on these views, we can formalize the concept of staleness.
Let $\textbf{V}_{T}$ be the old view, and $\textbf{V}_{T}^{'}$ be
the up-to-date view. If $f$ is an aggregation function, then we call
the staleness error of the query $\epsilon$ if :
\[
f(\textbf{V}_{T}^{'})=f(\textbf{V}_{T})+\epsilon
\]
Since we already have the out-of-date view, we can easily compute
$f(\textbf{V}_{T})$. 
In this work, we look at correcting staleness by estimating $\epsilon$ 
from the sample.
In other words, we use sampling to learn how updates affect the aggregation query on the view
and then correct the query results.

In {[}?{]}, the authors proposed an algorithm called ``NormalizedSC'' 
which estimated the error term $\epsilon$ from a sample of the \emph{difference set}.
We address a few new challenges in this work. First, NormalizedSC
was designed in the context of static tables and data dirtiness that
can be modeled as record transformations. 
In this work, we notice that correcting staleness can imply inserting new rows 
into the view as well as transforming old ones.
A further addition, is we consider the effects of outliers
on the estimate of NormalizedSC and characterize the optimality of
the NormalizedSC algorithm.



