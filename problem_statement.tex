\section{Background}\label{sec-background}
%In this section, we briefly overview the problem setting.

\subsection{Motivation and Example}\label{subsec-inc}
Materialized view maintenance can be very expensive resulting in staleness. 
Many important use-cases require creating a large number of views including: visualization, personalization, privacy, and real-time monitoring.  
The problem with eager maintenance is that every view created by an analyst places a bottleneck on incoming transactions.
There has been significant research on fast MV maintenance algorithms, most recently DBToaster \cite{DBLP:journals/vldb/KochAKNNLS14} which uses SQL query compilation and higher-order maintenance.
However, even with these optimizations, some materialized views are computationally difficult to incrementally maintain.
For example, incremental maintenance of views with correlated subqueries can grow with the size of the data.
Additionally, large views may require distribution and this further increases maintenance costs due to coordination.
In real deployments, it is common to use the same infrastructure to maintain multiple MVs (along with other analytics tasks) adding further contention to computational resources and reducing overall available throughput. 
When faced with such challenges, it is common to batch updates to amortize maintenance overheads and add flexibility to scheduling.

\noindent \textbf{Log Analysis Example: } 
Suppose we are a video streaming company analyzing user engagement.
Our database consists of two tables \tbl{Log} and \tbl{Video}, with the following schema:
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
Log(sessionId$\textrm{,}$ videoId)
Video(videoId$\textrm{,}$ ownerId$\textrm{,}$ duration)
\end{lstlisting}
The \tbl{Log} table stores each visit to a specific video with primary key (\texttt{sessionId}) and a foreign-key to the \tbl{Video} table (\texttt{videoId}).
%The \tbl{Video} stores each video with the primary key (\texttt{videoId}), a number identifying the owner of the view (\texttt{ownerId}), and the video duration (\texttt{duration}).
For our analysis, we are interested in finding aggregate statistics on visits, such as the average visits per video and the total number of visits predicated on different subsets of owners. 
We could define the following MV that counts the visits for each \texttt{videoId} associated with owners and the duration: 
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
CREATE VIEW visitView
AS SELECT videoId, ownerId, duration,
          count(1) as visitCount
FROM Log, Video
WHERE Log.videoId = Video.videoId
GROUP BY videoId
\end{lstlisting}
As \tbl{Log} table grows, this MV becomes stale, and we denote the insertions to the table as:
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
LogIns(sessionId$\textrm{,}$ videoId)
\end{lstlisting}

Staleness does not affect every query uniformly.
Even when the number of new entries in \texttt{LogIns} is small relative to \texttt{Log}, some queries might be very inaccurate.
For example, views to newly added videos may account for most of \texttt{LogIns}, so queries that count visits to the most recent videos will be more inaccurate.
The amount of inaccuracy is unknown to the user, who can only estimate an expected error based on prior experience.
This assumption may not hold in rapidly evolving data.
We see an opportunity for approximation through sampling which can give bounded query results for a reduced maintenance cost.
In other words, a small amount of up-to-date data allows the user to estimate the magnitude of query result error due to staleness.

\subsection{SampleClean~\cite{wang1999sample}}
To estimate up-to-date query results from stale materialized views, we leverage theory developed for query processing on dirty data.
SampleClean is a framework for scalable aggregate query processing on dirty data.
Traditionally, data cleaning has explored expensive, up-front cleaning of entire datasets for increased query accuracy.
Those who were unwilling to pay the full cleaning cost avoided data cleaning altogether.
We proposed SampleClean to add an additional trade-off to this design space by using sampling, i.e., bounded results for aggregate queries when only a sample of data is cleaned.
The problem of high computational costs for accurate results mirrors the challenge faced in the MV setting with the tradeoff between immediate maintenance (expensive and up-to-date) and deferred maintenance (inexpensive and stale). 
Thus, we explore how samples of ``clean" (up-to-date) data can be used for improved query processing on MVs without incurring the full cost of maintenance.

However, the metaphor of stale MVs as a Sample-and-Clean problem only goes so far and there are significant new challenges that we have to address in this paper.
In prior work, we modeled data cleaning as a row-by-row user-specified transformation.
This model does not work for missing and superfluous rows in stale MVs.
In particular, our sampling method has to account for this issue and we propose a hashing based technique to efficiently materialize a uniform sample even in the presence of missing/superfluous rows.
Next, we greatly expand the query processing scope of SampleClean beyond \sumfunc, \countfunc, and \avgfunc queries.
Bounding estimates that are not \sumfunc, \countfunc, and \avgfunc queries, is significantly more complicated.
This requires new analytic tools such as a statistical bootstrap estimation to calculate confidence intervals.
Finally, we add an outlier indexing technique to improve estimates on skewed data.


