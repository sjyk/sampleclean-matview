\section{Background}\label{sec-background}

\subsection{}
Incremental maintenance of materialized views is well studied, see \cite{chirkova2011materialized} for a survey of the approaches. 
Most incremental maintenance algorithms consists of two steps: calculating a ``delta" view,
and ``refreshing" the materialized view with the delta.
More formally, given a base relation $T$, a set of updates $U$,
and a view $\textbf{V}_{T}$:

\textbf{Calculate the Delta View- }
In this step, we apply the view definition to the updates and we call
the intermediate result a ``delta'' view:
\[
\Delta\textbf{V}
\]

This is also called a \emph{change propagation formula} in some literature,
especially on algebraic representations of incremental view maintenance.

\textbf{Refresh View- }
Given the ``delta'' view, we merge the results with the existing
view:
\[
\textbf{V}_{T}^{'}=refresh(\textbf{V}_{T},\Delta\textbf{V})
\] 
The details of the refresh operation depend on the view definition.
Refer to [?] for details.

%In this work, we address three types of materialized views: Select-Project, Aggregation, and
%Foreign-Key Join views; and four common aggregation queries on these
%views: SUM, COUNT, AVG, and VAR. We further analyze 
%only insertions into the database and defer analysis of updates
%and deletions for future work.

\subsection{Scheduling Maintenance}
While often less expensive than recomputing a materialized view,
incremental maintenance can still be costly.
Materialized views are growing larger and are more frequently 
implemented in distributed systems.
%Furthermore, database systems face an increased velocity of incoming data which
%can make infeasible to keep the views up-to-date.
Due to this cost, which we will refer to as the ``maintenance" cost, 
scheduling the refresh operation has been an important topic of research.





existing literature proposes the following approaches for this problem: 
\vspace{1em}

\noindent\textbf{Re-calculation: }
The most basic solution to address out-of-date materialized views is to periodically 
recalculate the entire view when the base tables have been updated (ie. re-execute the query on the entire dataset).
This approach can be very inefficient especially when the updates to the base table have only a small effect on the pre-computed result.
On the other hand, this approach requires no additional processing when records in the base table are updated.
For example, we may have user activity logs or sensors constantly feeding data to be inserted into our base table and incremental maintenance 
may place a bottleneck on each insertion as derived views need to be updated.
Furthermore, this approach is very easy to implement especially in systems such as Apache Spark or Hive which do not support
selective updates. 

\vspace{1em}

\noindent\textbf{Immediate Incremental Maintenance: }
Another possible solution is maintaining the view \emph{immediately}, that is, for every incoming update
immediately propagate the changes to the materialized view.
This solution guarantees that the view will always be up-to-date, but this can be very costly.
Updating the views becomes a bottleneck for each update and limits the write-throughput to the base table.

\vspace{1em}

\noindent\textbf{Periodic or Deferred Maintenance: }
It may be infeasible to keep the materialized view up-to-date for every incoming update.
One solution is to defer the maitenance to a time when more resources are avaible, eg. nightly.
Depending on the deferral peiod, these approaches can lead to long periods of stale query results.
Like periodic re-calculation, this removes the bottleneck for updates to the base table.

\vspace{1em}

\noindent\textbf{SAQP: }
Estimating the results of aggregate queries from samples has been
well studied in a field called Sample-based Approximate Query Processing
(SAQP). On the other hand, our approach differs from SAQP as we look to
approximately correct a query rather than directly estimating the query result.
In other words, we use a sample of up-to-date data to understand how to compensate for the
staleness. The SAQP approach to this problem, would be to
estimate the result directly from the maintained sample; a sort of
SAQP scheme on the sample of the view \cite{joshi2008materialized}. We found that estimating
a correction and leveraging an existing deterministic result lead
to lower variance results on real datasets (see Section \ref{exp}). We analyze
the tradeoffs of these techniques in the following sections.

\vspace{1em}

In the space of existing approahces, our system lies in between freshness guarantees of immediate maintenance and the performance of deferred maintenance.
The user sets the sampling ratio to meet either their desired performance and throughput level or accuracy of their query results.
Our proposed approach will be complementary to a periodic maintenance or re-calculation approach.
We envision the scenario where materialized views are being refreshed periodically eg. nightly.
While maintaining the entire view throughout the day may be infeasible, sampling allows the database to scale maintenance with the performance and resource constraints during the day.
Then, between maintenance periods, we can provide approximately up-to-date query results for aggregation queries.







There are two principle types of scheduling strategies: immediate and deferred. 
In immediate maintenance, as soon as a record is updated, 
the change is propagated to any derived materialized view.
Immediate maintenance has an advantage that materialized view is always up-to-date, 
however it can be very expensive.
This scheduling strategy places a bottleneck when records are written reducing 
the available write-throughput for the database.
Such an approach lacks flexibility since system resources need to be provisioned 
for the worst-case influx of updates.
Furthermore, especially in a distributed setting, record-by-record 
maintenance cannot take advantage of the benefits of consolidating communication overheads by batching.

To address these challenges, deferred maintenance is alternative solution.
The main idea of deferral is to avoid maintaining the view immediately and schedule an update at a more convenient time either pre-set or adaptively.
In deferred maintenance, the user often accepts some degree of staleness in 
the materialized view for additional flexibility for scheduling refresh operations.
For example, a user can update the materialized view nightly during times of low-activity in the system.
During these times, the system can take use more resources to process the updates without worrying about the affect on the throughput.
However, this also means that during the day the materialized view becomes increasingly stale as it was computed the night before.

More sophisticated deferred scheduling schemes are also possible.
In particular, we highlight a technique called lazy maintenance which defers maintenance until such time
a query on the view requires a row to be up-to-date.
This technique is not exclusive and lazy maintenance can work in conjunction with immediate maintenance on a subset of rows \cite{zhou2007lazy}.
While it ensures that query results are never stale, lazy maintenance potentially shifts much of the computational cost to query execution time.
In some queries this can defeat the purpose of caching a pre-computed result to speed up query execution time.

Immediate maintenance introduces a bottlneck on updates and lazy maintenance introduces a bottleneck during query exection,
and rapid collection of data can make these approches impractical.
As a consequence, periodic maintenance or recalculation is often the most feasible solution.
Periodic maintainence allows for coarse flexibility in scheduling to meet the resource constraints of the system and workload.
The problem is that between maintenance periods, which can be very long, queries on the view are stale.
While the user can set the maintenance period to control the expected staleness of the data, in general, the relative error between a stale query result and the up-to-date result is unbounded.

\subsection{Data Cleaning}
Much of data cleaning research focuses on improving query accuracy on dirty datasets.
For example, designing rules or algorithms to remove or correct erroneous records \cite{rahm2000data}.
However, recent work has considered the costs of data cleaning and how to budget this effort.
The Data Wrangler project argues that rules to correct the entire dataset can be learned from a small set of training examples \cite{kandel2011wrangler}.
Similarly, the SampleClean project presents a query processing framework that cleans on a sample of data, and then bounds the results of aggregate queries on dirty datasets \cite{wang1999sample}.

One key aspect of SampleClean is the tradeoff between data error and sampling error.
Cleaning a sample mitigates some of the error in the dataset, but sampling introduces uncertainty into the result.
There is a break-even point where the sample is sufficiently large that the mitigation of data error is more than the introduction
of sampling error.
This process allows the user to control the accuracy of the query with the sampling ratio as opposed to an unknown level of data error.
One of the algorithms in SampleClean, NormalizedSC, takes a dirty dataset and query and using a sample of clean data learns how
to correct a query so the result is ``clean".
This use of sampling is fundementally different from sampling as used in Sample-based Approximate Query Processing (SAQP).
In SAQP, the sample becomes the dataset and queries are applied directly to the sample.
However, in NormalizedSC, queries are still applied to the original unsampled and dirty base data.
The algorithm cleans a sample of data and then uses this information to correct the query result on the dirty data.

Since the ``correction" is derived from a sample, the correction is approximate but it turns out it bounded in error for SUM, COUNT, and AVG queries.
In comparision to SAQP, this approach gives more accurate results in datasets where errors are sparse since the correction will be small.
The idea of query correction is suited for the materialized view setting where if the views are very large updates may affect only a small fraction of rows in the view.
In this work, we apply a similar idea to use sampling to learn how the updates affect a given query, then use that information to correct the query so it is up-to-date.


