\section{Correction Query Processing}
\label{correction}
%In the previous section, we discussed how to efficiently maintain a sample of a materialized view by applying the hashing operator to the maintenance strategy.
%The result of the previous section is a cleaned sampled materialized view $\hat{S'}$. 
In this section, we discuss how to correct stale query results using the two corresponding samples $\hat{S}$ and $\hat{S'}$. 
Our data cleaning perspective allows us to look at each dirty row in an MV and what cleaning was applied to clean it (insert, delete, or update).
We can use this analysis to calculate corrections that compensate for the effect of staleness of aggregate queries.
The intuition is to take a point-wise difference of $\hat{S'}$ and $\hat{S}$ which is challenging in the presence of missing data.
%This gives us a uniform sample of the differences between the stale view and a hypothetical up-to-date view, and allows us to test how query results can be affected by the updates.
%\reminder{The following text is not consistent with the structure of the section.}
We first present our extensions to the existing SampleClean queries: \sumfunc, \countfunc, and \avgfunc.
Then, we discuss how to extend this framework to other aggregate functions.
%We also show how SampleClean can work with biased estimates as well and what statistical tools we can use to bound these estimates.
%Next, we discuss selection queries and how SampleClean can give limited support to these.
%We summarize these results in Table \ref{table:nonlin}, where we list common aggregation queries and describe 
%how \svc estimates their results. 
%Finally, we analyze the cost of query correction.


\iffalse
\begin{table*}[ht!]\scriptsize\vspace{-2em}
\caption{Query Result Semantics}  % title of Table
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Queries & Unbiased & Bounded Bias & Type of Bound \\ [0.5ex] % inserts table 
%heading
\hline % inserts single horizontal line
\sumfunc, \countfunc, \avgfunc & Yes & - & Optimal Analytical Via CLT \\ % inserting body of the table
\histfunc, \corrfunc, \varfunc, \covfunc & Yes & - & Empirical Via Bootstrap \\
\medfunc, \percfunc & No & Yes & Empirical Via Bootstrap \\
\maxfunc, \minfunc & No & No & Loose Probability Bound via Cantelli's Inequality \\
%\texttt{f(DISTINCT)} & No & No & None in general \\ [1ex] % [1ex] adds vertical space
\hline %inserts single line
\hline
\texttt{SELECT *} & Yes & Yes & Optimal bound on result size 
\end{tabular}\vspace{-1em}
\label{table:nonlin} % is used to refer this table in the text
\end{table*}
\fi

\subsection{SUM, COUNT, and AVG}
%In our prior work, in data cleaning, \nsc considered \sumfunc, \countfunc, and \avgfunc queries.
%\begin{lstlisting} [mathescape]
%SELECT $f(a)$ FROM View 
%WHERE $\cond(A)$
%\end{lstlisting}
%Group-by clauses can be handled as part of the predicate.
\nsc corrects a dirty query result (for \sumfunc, \countfunc, and \avgfunc queries), by taking a sample of dirty data, applying data cleaning, and estimating a compensation for the dirtiness from the sample.
It then calculates the row-by-row difference for each row in the sample between dirty and clean (i.e., how much did cleaning change the data).
We showed that the result of applying the query to the set of differences can be interpreted as a ``correction'' for dirty query results. 
Corrections differ from the traditional AQP approach which would be to apply a query directly to the up-to-date sample view.
%Then, we apply the query to the sample set of differences scaling appropriately (eg. for \sumfunc).
However, in prior work, our corrections only considered data error that updated rows, not those that required new rows to be inserted or deleted.
In the MV setting, it is possible that there are rows in the new view and the old view that do not exist in the other.

In Definition \ref{correspondence}, we formalized a notion of correspondence between two samples.
We use this property to handle this problem of missing rows from either side of the difference.
Our sampling procedure in the previous section, gives us two sample views that correspond with each other.
We define a new operator $\dot{-}$, called correspondence subtract, which will allow us to apply \nsc to such samples.
\begin{definition}[Correspondence Subtract] Given an aggregate query, and two corresponding relations $R_1$ and $R_2$ with the schema $(a_1, a_2, ...)$ where $a_1$ is the primary key for $R_1$ and $R_2$, and $a_2$ is the aggregation attribute for the query. 
$\dot{-}$ is defined as a projection of the full outer join on equality of $R_1.a_1 = R_2.a_1$: \[ \Pi_{R_1.a_2 - R_2.a_2} ( R_1 \fullouterjoin R_2 ) \]
Null values $\emptyset$ are represented as~zero.
\end{definition}
To apply this operation, we rewrite the \sumfunc and \countfunc queries using a selection with a \textbf{case} statement, and use the primary key (pk) that we defined in the previous section.
A case statement is defined as follows, we define $\cond(*)$ to be 1 when the predicate is true and 0 when false.
%Let $\cond(\hat{S'})=1$ ($\cond(\hat{S})=1$) be \textbf{case} statements to denote that its old (new) record satisfies the query condition; otherwise, $\cond(\hat{S'})=0$ ($\cond(\hat{S})=0$). 

\vspace{.5em}

\noindent For \sumfunc:
\begin{lstlisting} [mathescape]
$\;\;\;\;$q(View) := SELECT pk, $a \cdot \cond(*)$ FROM View
\end{lstlisting}

\noindent and for \countfunc:
\begin{lstlisting} [mathescape]
$\;\;\;\;$q(View) := SELECT pk, $\cond(*)$ FROM View
\end{lstlisting}

We can use our correspondence subtract operator to get the point-wise difference:
$d = q(\hat{S'}) \dot{-} q(\hat{S})$.  
The definition of the correspondence subtract allows us to be agnostic to both insertions and deletions.
For sampling ratio $m$, we can estimate the query correction for \sumfunc and \countfunc:
%\begin{lstlisting}[mathescape,basicstyle={\scriptsize}] 
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\dans_{count,sum} = $ SELECT $ \sumfunc\big(*)/m$ FROM d;
\end{lstlisting}
To apply the correction, we take the stale query result and add the correction:
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\ans'_{sum,count} \approx \ans_{sum,count} + \dans_{sum,count}$;
\end{lstlisting}

For the \avgfunc query, we can divide corrected results for \sumfunc and \countfunc:
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\ans'_{avg} = \frac{\ans'_{sum}}{\ans'_{count}}$;
\end{lstlisting}

\iffalse
\vspace{-1em}
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\dans_{count} = $ SELECT $ \sumfunc\big(*)/m$ FROM d;
\end{lstlisting}
\vspace{-1em}

We use the estimated correction to correct the stale query result as follows:
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\sumfunc(S') \approx \sumfunc(S) + \dans_{sum}$;
\end{lstlisting}
\vspace{-1em}
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\countfunc(S') \approx \countfunc(S) + \dans_{count}$;
\end{lstlisting}
\vspace{-1em}
\fi

In each of these formulas, all of the $\dans$ terms correspond to estimates, and these estimates can be bounded.
In \cite{wang1999sample}, we showed that the corrections for three queries can be rewritten as a sample mean.
The basic idea is that by the Central Limit Theorem, the mean value of numbers drawn by uniform random sampling $\bar{X}$ approaches a normal distribution with:
\[
\bar{X} \sim N(\mu,\frac{\sigma^2}{k}),
\]
where $\mu$ is the true mean, $\sigma^2$ is the variance, and $k$ is the sample size.
Refer to our prior work \cite{wang1999sample} for further details on how to bound these estimates with the Central Limit Theorem (CLT).

\iffalse
By the Central Limit Theorem, the mean value of numbers drawn by uniform random sampling $\bar{X}$ approaches a normal distribution with:
\[
\bar{X} \sim N(\mu,\frac{\sigma^2}{k}),
\]
where $\mu$ is the true mean, $\sigma^2$ is the variance, and $k$ is the sample size.\footnote{\scriptsize For sampling without replacement, there is an additional term of $\sqrt{\frac{N-k}{N-1}}$. We consider estimates where N is large in comparison to k making this term insignificant.}
We can use this to bound the term with its 95\% confidence interval (or any other user specified probability), e.g., $\bar{X} \pm 1.96 \frac{\sigma}{\sqrt{k}}$; refer to our prior work \cite{wang1999sample} for further details.

In the statistical literature, the class of aggregate functions for which we can apply this technique is formalized as the class of U-statistics, see \cite{hoeffding1948class}.
\fi

\subsubsection{Optimality}
%\reminder{I remebered there was one reviewer who had some concerns about this section. Make sure you addressed his concern.}
We can prove that for the \sumfunc, \countfunc, and \avgfunc queries this estimate is optimal with respect to the variance.
\begin{proposition}
An estimator is called a minimum variance unbiased estimator (MVUE) of a parameter if it is unbiased and the variance of the parameter estimate is less than or equal to that of any other unbiased estimator of the parameter.
\end{proposition}
The concept of a Minimum Variance Unbiased Estimator (MVUE) comes from statistical decision theory \cite{cox1979theoretical}.
\iffalse
Unbiased estimators are ones that, in expectation, are correct.
However, on its own, the concept of an unbiased estimate is not useful as we can construct bad unbiased estimates.
For example, if we simply pick a random element from a set, it is still an unbiased estimate of the mean of the set.
The variance of the estimate determines the size of our confidence intervals thus is important to consider.
\fi
%The \sumfunc, \countfunc, and \avgfunc queries are linear functions of their input rows.
%We explored whether our estimate was optimal for linear estimates, for example, should we weight our functions when applied to the sample.
It turns out that the proposed corrections are the optimal strategy when nothing is known about the data distribution a priori.

\begin{theorem}
Suppose we have a set of real numbers $X$ of size $N$. 
$X$ defines an empirical (non-parametric) distribution as follows for a random draw from $X$ takes on the value each $x \in X$ with probability $\frac{1}{N}$.
For the family of non-parametric distributions, the sample mean is an MVUE of the expected value of $X$ (the population mean).
Thus, for \sumfunc, \countfunc, and \avgfunc queries, our estimate of the correction is optimal when no other information is known about the distribution. 
%In other words, there exists no other linear function of the set input rows $\{ X_i \}$ that gives a lower variance correction.
\end{theorem}

\begin{proof}[Sketch]
It is known that the sample mean is an MVUE for the population mean for an arbitrary distribution \cite{shuster1982nonparametric}; however we calculate a correction and we explore the optimality of this correction.
%In the first step of \nsc, we rewrite \sumfunc, \countfunc, and \avgfunc as Select queries.
%This gives us a set of real numbers which defines our empirical distribution.
Then, we apply our correspondence subtract operator and since point-wise subtraction is commutative with these three queries,
we can re-write $\dans$ as a sample mean of the set of differences (where nulls are 0).
Thus, $\dans$ is an MVUE for the correction and since the stale result is deterministic it does not affect the estimate.
See \cite{technicalReport} for the full version of the proof.
\end{proof}
The implication of this theorem is that there does not exist any other estimation algorithm for $\dans$ that has lower variance when nothing else is known about the data a priori.
However, as we will see in Section \ref{outlier}, when we know additional information (e.g., outliers), we may be able to get better results.
%For example, we could imagine training a machine learning algorithm to predict how each row changes and apply this algorithm to answer aggregate queries; this theorem shows that for \sumfunc, \countfunc, and \avgfunc this would be suboptimal.

\subsubsection{Correction vs. Direct Estimate}
Correcting query results approximately is often more accurate than an AQP-style direct estimate. 
\sloppy
We can provide some mathematical intuition for this.
For \sumfunc, \countfunc, and \avgfunc, our correction algorithm gives a confidence interval (via CLT) that is
proportional to the variance of the \emph{change} and inversely proportional to the sample size 
$\frac{\sigma_{c}^2}{k}$.
On the other hand, an AQP approach would give us an estimate that is proportional to the variance of the up-to-date data 
$\frac{\sigma_{S'}^2}{k}$.
Since the change is the difference between the stale and up-to-date data, this can be rewritten as
\[\frac{\sigma_{S}^2 + \sigma_{S'}^2 - 2cov(S,S')}{k}\]
Therefore, a correction will have less variance when:
\[\sigma_{S}^2 \le 2cov(S,S')\]
%If the difference is small, i.e. $S$ is nearly identical to $S'$,then $cov(S,S')~\approx~\sigma_{S}^2$ making . 
This result shows that there is a point when updates to the stale MV are significant enough that direct estimates are more accurate.
When we cross the break-even point we can switch from using corrections applying an AQP estimate.
%An intersting implication of this break-even point is it gives us a bound on the approximation error no matter how significant the staleness is.
The AQP approach does not depend on $cov(S,S')$ which is a measure of how much the data has changed.
Thus, we guarantee an approximation error of at most $\frac{\sigma_{S'}^2}{k}$.
In our experiments (Figure \ref{exp-1-total}(b)), we evaluate this break even point empirically. 

\fussy

\subsection{General Aggregate Queries}
In SampleClean, we proposed the \nsc algorithm to give unbiased corrections to \sumfunc, \countfunc, and \avgfunc.
We found that these queries are a special case of a broader class of aggregate queries; namely if a query has an unbiased sample estimate there also exists an unbiased correction.
However, in general, these aggregate functions do not satisfy the optimality conditions of the previous section.
The main condition that fails is the commutativity of the correspondence subtraction operation (i.e., $var(x-y) \ne var(x) - var(y)$).

These queries include: \histfunc, \corrfunc, \varfunc, \covfunc, and the estimation approach is different.
The general estimation procedure is the following:
\begin{enumerate}[noitemsep]
\item Apply q to the stale sample, 
\item Apply q to the up-to-date sample, 
\item Apply q to the full stale view
\item Take the difference between (2) and (1) and add it to (3).
\end{enumerate}
The implications of this are that any query that can be answered in prior work with SAQP (e.g., in BlinkDB \cite{AgarwalMPMMS13}) in an unbiased way can also be answered with our approach.

Suppose, we have an aggregate query $q$ and we apply the query to the stale view $S$.
%Without loss of generality, we assume this query is without a group by expression as we can always model this expression as part of the predicate.
The query result is stale by $c$ if:
$ c = q(S') - q(S)$.
\begin{lemma}\label{lemma:unbiased}
If there exists an unbiased sample estimator for q(S') then there exists an unbiased sample estimator for c.
\end{lemma}
\begin{proof}[Sketch] 
Deterministic constants do not change the expected value of a random variable.
The query result on the entire stale view is deterministic.
Refer to our extended paper for more details \cite{technicalReport}.
\end{proof}

\iffalse
Suppose, we have an unbiased sample estimator $\bar{q}$ of $q$. 
Then, it follows that \[\mathbb{E}\big[\bar{q}(\hat{S'})\big] = q(S')\]
If we substitute in this expression:
\[ c = \mathbb{E}\big[\bar{q}(\hat{S'})\big] -q(s) \] 
Applying the linearity of expectation:
\[ c = \mathbb{E}\big[\bar{q}(\hat{S'}) - q(s)\big] \]
\fi

Some queries do not have unbiased sample estimators, but the bias of their sample estimators can be bounded. Example queries include: \medfunc, \percfunc.
A corollary to the previous lemma, is that if we can bound the bias for our estimator then we can achieve a bounded bias for $c$ as well.
\begin{corollary}
If there exists a bounded bias sample estimator for $q$ then there exists a bounded bias sample estimator for $c$.
\end{corollary}
Functionally, we can apply the same estimation procedure as the unbiased case.

%What this means for a user is that the error bars are asymmetric with increased error in the direction of the bias (often in the direction of skew in the dataset).

%This procedure will also work for \sumfunc, \countfunc, and \avgfunc and in fact will give the same result estimate.
%However, unlike before, it does not give us an analytic way to calculate a confidence interval.
%This is because variance does not commute with subtraction for correlated random variables.
%We can use a technique called a statistical bootstrap \cite{AgarwalMPMMS13} to empirically bound our correction.


For both cases above, we may not get analytic confidence intervals on our results, nor is it guaranteed that our estimates are optimal.
We can use a technique called a statistical bootstrap \cite{AgarwalMPMMS13} to empirically bound our correction.
In this approach, we repeatedly subsample with replacement from our sample and apply the sample estimator.
We use these repeated executions to build a distribution of values that the correction can take allowing us to bound the result.
There has been recent research in using techniques such as Poissonized resampling \cite{agarwalknowing}, analytical bootstrap\cite{DBLP:conf/sigmod/ZengGMZ14}, and bagging \cite{DBLP:conf/kdd/KleinerTASJ13} to make this algorithm better suited for latency-sensitive query processing application.
Refer to the extended technical report for the details on the bootstrap procedure \cite{technicalReport}.

\subsubsection{MIN and MAX}
\minfunc and \maxfunc fall into their own category since there does not exist any unbiased sample estimator nor can that bias be bounded.
We devise an estimation procedure that corrects these queries.
However, we can only achieve bound that has a slightly different interpretation than the confidence intervals seen before.
We can calculate the probability that a larger (or smaller) element exists in the unsampled view.
Refer to the extended technical report for the details on \minfunc and \maxfunc \cite{technicalReport}.

\iffalse
We devise the following correction estimate for \maxfunc: (1) For all rows in both $S$ and $S'$, calculate the row-by-row difference, (2) let $c$ be the max difference, and (3) add $c$ to the max of the stale view.

We can give weak bounds on the results using Cantelli's Inequality.
If $X$ is a random variable with mean $\mu_x$ and variance $var(X)$, then the probability that $X$ is larger than a constant $\epsilon$ 
\[
\mathbb{P}(X \ge \epsilon + \mu_x ) \le \frac{var(X)}{var(X) + \epsilon^2}
\]
Therefore, if we set $\epsilon$ to be the difference between max value estimate and the average value, we can calculate the probability that we will see a higher value. 

The same estimator can be modified for \minfunc, with a corresponding bound:
\[
\mathbb{P}(X \le \mu_x - a )) \le \frac{var(x)}{var(x) + a^2}
\]
This bound has a slightly different interpretation than the confidence intervals seen before.
This gives the probability that a larger (or smaller) element exists in the unsampled view.
\fi

\iffalse
\begin{proposition} (BOOTSTRAP OVER DIFFERENCES) Let q be an aggregate query that has an unbiased sample estimate, and let $\hat{S}$ and $\hat{S'}$ be sample views as defined before. 
One sample of the bootstrap estimator $s$ is defined as the difference of q applied to random subsample of size $b_1$ (with replacement) of $\hat{S}$ and $\hat{S'}$.
We denote subsamples of the samples as $\hat{S'}_{sub}$ and $\hat{S}_{sub}$ respectively.
\[q(\hat{S'}_{sub}) - q(\hat{S}_{sub})\]
To build the confidence interval, we repeatedly apply this procedure $b_2$ times.
\end{proposition}
\fi

\vspace{-.25em}
\subsection{Select Queries}
In \svc, we also explore how to extend this correction procedure to Select queries.
Suppose, we have a Select query with a predicate:
\begin{lstlisting} [mathescape]
SELECT $*$ FROM View WHERE Condition(A);
\end{lstlisting}

We first run the Select query on the stale view, and this returns a set of rows.
This result has three types of data error: rows that are missing, rows that are falsely included, and rows whose values are incorrect.

As in the \sumfunc, \countfunc, and \avgfunc query case, we can apply the query to the sample of the up-to-date view.
From this sample, using our lineage defined earlier, we can quickly identify which rows were added, updated, and deleted.
For the updated rows in the sample, we overwrite the out-of-date rows in the stale query result.
For the new rows, we take a union of the sampled selection and the updated stale selection.
For the missing rows, we remove them from the stale selection.
To quantify the approximation error, we can rewrite the Select query as \countfunc to get an estimate of number of rows that were updated, added, or deleted (thus three ``confidence'' intervals). 


\iffalse
\subsection{Bounded Bias Aggregates}
Some queries do not have unbiased sample estimators, but the bias of their sample estimators can be bounded. Example queries include: \medfunc, \percfunc.

A corollary to the previous lemma, is that if we can bound the bias for our estimator then we can achieve a bounded bias for $c$ as well.
\begin{corollary}
If there exists a bounded bias sample estimator for $q$ then there exists a bounded bias sample estimator for $c$.
\end{corollary}

What this means for a user is that the error bars are asymmetric with increased error in the direction of the bias (often in the direction of skew in the dataset).
Functionally, we can apply the same technique discussed in the previous section for these queries and apply the bootstrap to bound the results. 
\fi

\iffalse 

\subsection{Query Execution Cost}
It is true that \svc adds to the query execution time by issuing a correction. 
However, this correction is only calculated on a sample and thus is small compared to the query execution time over the entire old view.
In our experiments, we show that the overheads are small in comparison to the execution time over the entire old view and the savings from only maintaining a sample (Section \ref{exp-datacube}).
\fi