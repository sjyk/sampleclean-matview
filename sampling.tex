\section{Correction Query Processing}
\label{correction}
In the previous section, we discussed how to efficiently find $\hat{S'}$ by applying the hashing operator to the maintenance strategy.
The result of this execution is an up-to-date sampled materialized view $\hat{S'}$. 
In this section, we discuss how to correct stale query results using the two corresponding samples $\hat{S}$ and $\hat{S'}$. 
The main idea to take a point-wise difference of $\hat{S'}$ and $\hat{S}$, and apply the query to that difference.
%This gives us a uniform sample of the differences between the stale view and a hypothetical up-to-date view, and allows us to test how query results can be affected by the updates.
%\reminder{The following text is not consistent with the structure of the section.}
We first present our extensions to the existing SampleClean queries: \sumfunc, \countfunc, and \avgfunc.
Then, we discuss how to extend this framework to other aggregate functions that have unbiased estimators.
We also show how SampleClean can work with biased estimates as well and what statistical tools we can use to bound these estimates.
Next, we discuss selection queries and how SampleClean can give limited support to these.
Finally, we analyze the cost of query correction.

\begin{table*}[ht!]
\caption{Query Result Semantics}  % title of Table
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Queries & Unbiased & Bounded Bias & Type of Bound \\ [0.5ex] % inserts table 
%heading
\hline % inserts single horizontal line
\sumfunc, \countfunc, \avgfunc & Yes & - & Optimal Analytical Via CLT \\ % inserting body of the table
\histfunc, \corrfunc, \varfunc, \covfunc & Yes & - & Empirical Via Bootstrap \\
\medfunc, \percfunc & No & Yes & Empirical Via Bootstrap \\
\maxfunc, \minfunc & No & No & Loose Probability Bound via Cantelli's Inequality \\
%\texttt{f(DISTINCT)} & No & No & None in general \\ [1ex] % [1ex] adds vertical space
\hline %inserts single line
\hline
\texttt{SELECT *} & Yes & Yes & Optimal bound on result size 
\end{tabular}
\label{table:nonlin} % is used to refer this table in the text
\end{table*}

\subsection{\sumfunc, \countfunc, and \avgfunc}
In our prior work, NormalizedSC considered \sumfunc, \countfunc, and \avgfunc of the form:
\begin{lstlisting} [mathescape]
SELECT $f(a)$ FROM View 
WHERE cond(A)
\end{lstlisting}
Group-by clauses can be handled as part of the predicate.

We showed that these three queries can be rewritten as a sample mean, for example, \sumfunc is a mean multiplied by the dataset size and \countfunc is the mean of the indicator function multipled by the dataset size.
When we re-wrote these queries, we were able to derive a correction for the query results by taking the mean difference.
A sample mean is an unbiased estimator of a population mean for all data distributions and sample sizes.
Furthermore, the Central Limit Theorem states that asymptotically this mean approaches a normal distribution giving very tight bounds on the error.
In the statistical literature, this class of aggregate function is formalized as the class of U-statistics, see \cite{hoeffding1948class}.

However, in the materialized view setting, it is possible that there are tuples in the new view and the old view that do not exist in the other.
This makes the a pointwise subtraction between two samples ill-defned.
We define a new operator $\check{-}$, which we call correspondence subtract, which handles this problem.
\begin{definition} (CORRESPONDENCE SUBTRACT). For two corresponding relations $R_1$ and $R_2$ with the schema $(a_1, a_2, ...)$ where 
$a_1$ is the primary key for $R_1$ and $R_2$, and $a_2$ is the aggregation attribute. 
$\check{-}$ is defined as a projection of the full outer join on equality of $R_1.a_1 = R_2.a_1$: \[ \Pi_{R_1.a_2 - R_2.a_2} ( R_1 \fullouterjoin R_2 ) \]
If $R_1.a_1$ or $R_2.a_1$ is $\emptyset$ then they are respectively represented as a $0$.
\end{definition}

Then, we rewrite the \sumfunc, \countfunc, and \avgfunc query using as a selection with a \textbf{case} statement, and use the primary key for the view we defined in the previous section.
Let $\cond(\hat{S'})=1$ ($\cond(\hat{S})=1$) be \textbf{case} statments to denote that its old (new) record satisfies the query condition; otherwise, $\cond(\hat{S'})=0$ ($\cond(\hat{S})=0$). 

For \sumfunc, and \avgfunc:
\begin{lstlisting} [mathescape]
q := SELECT pk, $a \cdot cond(A)$ FROM View
\end{lstlisting}

and for \countfunc:
\begin{lstlisting} [mathescape]
q := SELECT pk, $cond(A)$ FROM View
\end{lstlisting}

We can use our correspondence subtract operator to get the pointwise difference:
\[d = q(\hat{S'}) \check{-} q(\hat{S})\]  
The definition of the correspondence subtract allows us to be agnostic to both insertions and deletions.
For sampling ratio $m$:
%\begin{lstlisting}[mathescape,basicstyle={\scriptsize}] 
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{sum} = $ SELECT $ \sumfunc\big(*)$ 
$~~~~~~~~~~~~~~~~~$ FROM d;
$\sumfunc(S') \approx \sumfunc(S) + \dans_{sum}/m$
\end{lstlisting}

\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{cnt} = $ SELECT $ \sumfunc\big(*)$ 
$~~~~~~~~~~~~~~~~~$FROM d;
$\countfunc(S') \approx \countfunc(S) + \dans_{cnt}/m$
\end{lstlisting}


\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{avg} = \frac{\dans_{sum}}{\dans_{cnt}}$;
$\avgfunc(S') \approx \avgfunc(S) + \dans_{avg}$
\end{lstlisting}

In each of these formulas, all of the $\dans$ terms correspond to estimates, and these estimates need to be bounded.
To find these bounds, we can rewrite the terms $\dans$ as a mean value of random variables.
By the Central Limit Theorem, the mean value of numbers drawn by uniform random sampling $\bar{X}$ approaches a normal distribution with:
\[
\bar{X} \sim N(\mu,\frac{\sigma^2}{k}),
\]
where $\mu$ is the true mean, $\sigma^2$ is the variance, and $k$ is the sample size.\footnote{\scriptsize For sampling without replacement, there is an additional term of $\sqrt{\frac{N-k}{N-1}}$. We consider estimates where N is large in comparison to k making this term insignificant.}
We can use this to bound the term with its 95\% confidence interval (or any other user specified probability), e.g., $\bar{X} \pm 1.96 \frac{\sigma}{\sqrt{k}}$; refer to our prior work \cite{wang1999sample} for futher details.

\subsubsection{Optimality}
%\reminder{I remebered there was one reviewer who had some concerns about this section. Make sure you addressed his concern.}
We can prove that for the \sumfunc, \countfunc, and \avgfunc queries this estimate is optimal with respect to the variance.
\begin{proposition}
An estimator is called a minimum variance unbiased estimator (MVUE) of a parameter if it is unbiased and the variance of the parameter estimate is less than or equal to that of any other unbiased estimator of the parameter.
\end{proposition}
The concept of a Minimum Variance Unbiased Estimator (MVUE) comes from statistical decision theory \cite{cox1979theoretical}.
Unbiased estimators are ones that, in expectation, are correct.
However, on its own, the concept of an unbiased estimate is not useful as we can construct bad unbiased estimates.
For example, if we simply pick a random element from a set, it is still an unbiased estimate of the mean of the set.
The variance of the estimate determines the size of our confidence intervals thus is important to consider.

The \sumfunc, \countfunc, and \avgfunc queries are linear functions of their input rows.
We explored whether our estimate was optimal for linear estimates, for example, should we weight our functions when applied to the sample.
It turns out that the proposed corrections are the optimal strategy when nothing is known about the data distribution a priori.

\begin{theorem}
For \sumfunc, \countfunc, and \avgfunc queries, our estimate of the correction is optimal over the class of linear estimators when no other information is known about the distribution. 
In other words, there exists no other linear function of the set input rows $\{ X_i \}$ that gives a lower variance correction.
\end{theorem}
\begin{proof}[Sketch]
As discussed before, we reformulate \sumfunc, \countfunc, and \avgfunc as means over the entire table. We prove the theorem by induction. 
Suppose, we have two independent unbiased estimates of the mean $\bar{X}_1$ and $\bar{X}_2$ and we want to merge them into one estimate $\bar{X} = c_1\bar{X}_1+c_2\bar{X}_2$.
Since both estimates are unbiased, $c_1 + c_2 = 1$ to ensure the merged estimate is also unbiased, and now we consider the variance of the merged estimate.
The variance of the estimate is $var(\bar{X}) = c_1^2var(\bar{X}_1) + c_2^2var(\bar{X}_2)$.
Since, we don't know anything about the distribution of the data, by symmetry $var(\bar{X}_1) = var(\bar{X}_2)$.
Consequently, the optimal choice is $c_1=c_2=0.5$. 
We can recursively induct, and we find that if we do not know the variance of data, as is impossible with new updates, then equally weighting all input rows is optimal. 
\end{proof}
The implication of this proof is that there does not exist any other weighted average that gives better estimate (for any distribution or sample size).

\subsection{General Aggregate Queries}
In SampleClean, we proposed the \textbf{NormalizedSC} algorithm to give unbiased corrections to \sumfunc, \countfunc, and \avgfunc.
In this work, we found that these queries are a special case of a broader class of aggregate queries; namely if a query has an unbiased sample estimate there also exists an unbiased correction.
The implications of this are that any query that can be answered in prior work with SAQP in an unbiased way can also be answered with our approach.

Suppose, we have an aggregate query $q$ and we apply the query to the stale view $S$.
Without loss of generality, we assume this query is without a group by expression as we can always model this expression as part of the predicate.
The query result is stale by a factor of $c$ if:
\[ c = q(S') - q(S)\] 

\begin{lemma}
If there exists an unbiased sample estimator for q then there exists an unbiased sample estimator for c.
\end{lemma}

\emph{Proof.} Suppose, we have an unbiased sample estimator $\bar{q}$ of $q$. 
Then, it follows that \[\mathbb{E}(\bar{q}(\hat{S'})) = q(S')\]
If we substitute in this expression:
\[ c = \mathbb{E}(\bar{q}(\hat{S'})) -q(s) \] 
Applying the linearity of expectation:
\[ c = \mathbb{E}(\bar{q}(\hat{S'}) - q(s)) \blacksquare \]

However, in general, these aggregate functions do not satisfy the optimality conditions of the previous section.
These queries include: \histfunc, \corrfunc, \varfunc, \covfunc, and the estimation approach is different.
We apply the query to stale view, and then we apply the sample estimate of the query to up-to-date sample.
The general procedure is the following: (1) apply q to the stale sample, (2) apply q to the clean sample, (3) apply q to the full stale view, and (4) finally take the difference between (2) and (1) and add it to (3). 

This procudure will also work for \sumfunc, \countfunc, and \avgfunc and in fact will give the same result estimate.
However, unlike before, it does not give us an analytic way to calculate a confidence interval.
This is because variance does not commute with subtraction for correlated random variables.
We can use a technique called a statistical bootstrap \cite{AgarwalMPMMS13} to empirically bound our correction.
In this approach, we repeatedly subsample with replacement from our sample and apply the sample estimator.
We use these repeat executions to build a distribution of values that the correction can take allowing us to bound the result.
The conditions under which this technique is valid are nuanced, and detecting when it is valid is a subject of \cite{agarwalknowing}.

\begin{proposition} (BOOTSTRAP OVER DIFFERENCES) Let q be an unbiased aggregate query, and let $\hat{S}$ and $\hat{S'}$.
One sample of the bootstrap estimator $s$ is defined as the difference of q applied to random subsample of size $b_1$ (with replacment) of $q(\hat{S'}_{sub}) - q(\hat{S}_{sub})$. \reminder{what is $\hat{S'}_{sub}$?}
To build the confidence interval, we repeatedly apply this procedure $b_2$ times.
\end{proposition}

It is true that bootstrap is more computationally expensive than the analytical derivations for the \sumfunc, \countfunc, and \avgfunc queries, and it is also sensitive to the choice of parameters $b_1$ and $b_2$.
There has been recent research in using techniques such as Poissonized resampling \cite{agarwalknowing}, analytical bootstrap\cite{DBLP:conf/sigmod/ZengGMZ14}, and bagging \cite{DBLP:conf/kdd/KleinerTASJ13} to make this algorithm better suited for latency-senstive query processing application.

\subsection{Bounded Bias Aggregates}
A corollary to the previous lemma, is that if we can bound the bias for our estimator then we can achieve a bounded bias for $c$ as well.
\begin{corollary}
If there exists a bounded bias sample estimator for $q$ then there exists a bounded bias sample estimator for $c$.
\end{corollary}
Example queries include: \medfunc, \percfunc.

What this means for a user is that the error bars are asymmetric with increased error in the direction of the bias (often in the direction of skew in the dataset).
Functionally, we can apply the same technique discussed in the previous section for these queries and apply the bootstrap to bound the results. 

\subsection{MIN and MAX}
\minfunc and \maxfunc fall into their own category since there does not exist any unbiased sample estimator nor can that bias be bounded.
We can, however, give weak bounds on the results using Cantelli's Inequality:
\reminder{I think you need to explain the ineuality more. What are $X$, $\mu_x$, and $a$?}
\[
\mathbb{P}(X \ge a + \mu_x ) \le \frac{var(x)}{var(x) + a^2}
\]
Therefore, if we set $a$ to be the max value, we can calculate the probability that we will see a higher value.\reminder{It seems that the bound has a different meaning than what we defined for other queries?} The same logic can be reversed for \minfunc.
\[
\mathbb{P}(X \le \mu_x - a )) \le \frac{var(x)}{var(x) + a^2}
\]

\subsection{Select Queries}
We can correct stale Select queries with SVC.
Suppose, we have a Select query with a simple predicate:
\begin{lstlisting} [mathescape]
SELECT $*$ FROM View 
WHERE Condition(A);
\end{lstlisting}

We can first run the Select on the stale view, and the result of this is a multiset.
First, we must define what it means to ``correct" this stale result.
This result has three types of data error: rows that are missing, rows that are falsely included, and rows whose values are incorrect.

As in the aggregate query case, we can apply the query to the sample of the up-to-date view.
From this sample, using our lineage defined earlier, we can quickly identify which rows were added, updated, and deleted.
For the updated rows in the sample, we overwrite the out-of-date rows in the stale query result.
For the new rows, we take a union of the sampled selection and the updated stale selection.
For the missing rows, we remove them from the stale selection.
To quantify the approximation error, we can rewrite the Select query as \countfunc to get an estimate of number of rows that were not updates, added, or deleted (thus three ``confidence" intervals). 

\subsection{Query Execution Cost}
It is true that SVC adds to the query execution time by issuing a correction. 
However, this correction is only calcuated on a sample and thus is small compared to the query execution time over the entire old view.
In our experiments, we show that the overheads are small in comparison to the execution time over the entire old view and the savings from only maintaining a sample (Section \ref{exp-datacube}).
