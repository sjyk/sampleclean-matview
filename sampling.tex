\section{Correction Query Processing}
\label{correction}
In the previous section, we discussed how to efficiently find $\hat{S'}$ by applying the hashing operator to the maintenance strategy.
The result of this execution is an up-to-date sampled materialized view $\hat{S'}$. 
In this section, we discuss how to correct stale query results using the two corresponding samples $\hat{S}$ and $\hat{S'}$. 
The main idea to take a point-wise difference of $\hat{S'}$ and $\hat{S}$, and apply the query to that difference.
%This gives us a uniform sample of the differences between the stale view and a hypothetical up-to-date view, and allows us to test how query results can be affected by the updates.
%\reminder{The following text is not consistent with the structure of the section.}
We first present our extensions to the existing SampleClean queries: \sumfunc, \countfunc, and \avgfunc.
Then, we discuss how to extend this framework to other aggregate functions that have unbiased estimators.
We also show how SampleClean can work with biased estimates as well and what statistical tools we can use to bound these estimates.
Next, we discuss selection queries and how SampleClean can give limited support to these.
We summarize these results in Table \ref{table:nonlin}, where we list each of the aggregate queries supported in Apache HiveQL and describe 
how SVC estimates its results. 
Finally, we analyze the cost of query correction.


\begin{table*}[ht!]
\caption{Query Result Semantics}  % title of Table
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Queries & Unbiased & Bounded Bias & Type of Bound \\ [0.5ex] % inserts table 
%heading
\hline % inserts single horizontal line
\sumfunc, \countfunc, \avgfunc & Yes & - & Optimal Analytical Via CLT \\ % inserting body of the table
\histfunc, \corrfunc, \varfunc, \covfunc & Yes & - & Empirical Via Bootstrap \\
\medfunc, \percfunc & No & Yes & Empirical Via Bootstrap \\
\maxfunc, \minfunc & No & No & Loose Probability Bound via Cantelli's Inequality \\
%\texttt{f(DISTINCT)} & No & No & None in general \\ [1ex] % [1ex] adds vertical space
\hline %inserts single line
\hline
\texttt{SELECT *} & Yes & Yes & Optimal bound on result size 
\end{tabular}
\label{table:nonlin} % is used to refer this table in the text
\end{table*}

\subsection{SUM, COUNT, and AVG}
In our prior work, in the context of data cleaning, \nsc considered \sumfunc, \countfunc, and \avgfunc of the form:
\begin{lstlisting} [mathescape]
SELECT $f(a)$ FROM View 
WHERE $\cond(A)$
\end{lstlisting}
Group-by clauses can be handled as part of the predicate.
As input \nsc is given a query (\sumfunc, \countfunc, or \avgfunc), the result of running this query on the dirty data, and a dirty sample and a cleaned version of the sample.
We then calculate the row-by-row difference for each row in the sample between dirty and clean.
Then, we apply the query to the sample set of differences scaling appropriately (eg. for \sumfunc).
We showed that the result of applying the query to the set of differences can be interpreted as a ``correction" for dirty query results.

However, \nsc only considered data errors that transformed rows, not those that required new rows to be inserted or deleted.
In the materialized view setting, it is possible that there are rows in the new view and the old view that do not exist in the other.
This makes the a row-by-row subtraction between two samples of data ill-defned. 

In Definition \ref{correspondence}, we formalized a notion of correspondence between two samples.
We use this property to handle this problem of missing rows from either side of the subtraction.
Our sampling procedure in the previous section, gives us two sample views that correspond with each other.
We define a new operator $\dot{-}$, called correspondence subtract, which will allow us to apply \nsc to such samples.
\begin{definition}[Correspondence Subtract] Given an aggregate query, and two corresponding relations $R_1$ and $R_2$ with the schema $(a_1, a_2, ...)$ where $a_1$ is the primary key for $R_1$ and $R_2$, and $a_2$ is the aggregation attribute for the query. 

$\dot{-}$ is defined as a projection of the full outer join on equality of $R_1.a_1 = R_2.a_1$: \[ \Pi_{R_1.a_2 - R_2.a_2} ( R_1 \fullouterjoin R_2 ) \]
If $R_1.a_1$ or $R_2.a_1$ is $\emptyset$ then they are respectively represented as a $0$.
\end{definition}

Then, we rewrite the \sumfunc, \countfunc, and \avgfunc queries using a selection with a \textbf{case} statement, and use the primary key (pk) that we defined in the previous section.
Let $\cond(\hat{S'})=1$ ($\cond(\hat{S})=1$) be \textbf{case} statements to denote that its old (new) record satisfies the query condition; otherwise, $\cond(\hat{S'})=0$ ($\cond(\hat{S})=0$). 

\vspace{.5em}

\noindent For \sumfunc:
\begin{lstlisting} [mathescape]
$\;\;\;\;$q(View) := SELECT pk, $a \cdot \cond(A)$ FROM View
\end{lstlisting}

\noindent and for \countfunc:
\begin{lstlisting} [mathescape]
$\;\;\;\;$q(View) := SELECT pk, $\cond(A)$ FROM View
\end{lstlisting}

We can use our correspondence subtract operator to get the pointwise difference:
\[d = q(\hat{S'}) \dot{-} q(\hat{S})\]  
The definition of the correspondence subtract allows us to be agnostic to both insertions and deletions.
For sampling ratio $m$, we can estimate the query correction for \sumfunc, \countfunc, and \avgfunc, respectively:
%\begin{lstlisting}[mathescape,basicstyle={\scriptsize}] 
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\dans_{sum} = $ SELECT $ \sumfunc\big(*)/m$ FROM d;
\end{lstlisting}
\vspace{-1em}
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\dans_{count} = $ SELECT $ \sumfunc\big(*)/m$ FROM d;
\end{lstlisting}
\vspace{-1em}
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\dans_{avg} = \frac{\dans_{sum}}{\dans_{cnt}}$;
\end{lstlisting}
We use the estimated correction to correct the stale query result as follows:
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\sumfunc(S') \approx \sumfunc(S) + \dans_{sum}$;
\end{lstlisting}
\vspace{-1em}
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\countfunc(S') \approx \countfunc(S) + \dans_{count}$;
\end{lstlisting}
\vspace{-1em}
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\;\;\;\;$$\avgfunc(S') \approx \avgfunc(S) + \dans_{avg}$;
\end{lstlisting}

In each of these formulas, all of the $\dans$ terms correspond to estimates, and these estimates can be bounded.
In \cite{wang1999sample}, we showed that the corrections for three queries can be rewritten as a sample mean.
A sample mean is an unbiased estimator of a population mean for all data distributions and sample sizes.
Furthermore, the Central Limit Theorem states that asymptotically this mean approaches a normal distribution giving very tight bounds on the error.

By the Central Limit Theorem, the mean value of numbers drawn by uniform random sampling $\bar{X}$ approaches a normal distribution with:
\[
\bar{X} \sim N(\mu,\frac{\sigma^2}{k}),
\]
where $\mu$ is the true mean, $\sigma^2$ is the variance, and $k$ is the sample size.\footnote{\scriptsize For sampling without replacement, there is an additional term of $\sqrt{\frac{N-k}{N-1}}$. We consider estimates where N is large in comparison to k making this term insignificant.}
We can use this to bound the term with its 95\% confidence interval (or any other user specified probability), e.g., $\bar{X} \pm 1.96 \frac{\sigma}{\sqrt{k}}$; refer to our prior work \cite{wang1999sample} for further details.

In the statistical literature, the class of aggregate functions for which we can apply this technique is formalized as the class of U-statistics, see \cite{hoeffding1948class}.

\subsubsection{Optimality}
%\reminder{I remebered there was one reviewer who had some concerns about this section. Make sure you addressed his concern.}
We can prove that for the \sumfunc, \countfunc, and \avgfunc queries this estimate is optimal with respect to the variance.
\begin{proposition}
An estimator is called a minimum variance unbiased estimator (MVUE) of a parameter if it is unbiased and the variance of the parameter estimate is less than or equal to that of any other unbiased estimator of the parameter.
\end{proposition}
The concept of a Minimum Variance Unbiased Estimator (MVUE) comes from statistical decision theory \cite{cox1979theoretical}.
Unbiased estimators are ones that, in expectation, are correct.
However, on its own, the concept of an unbiased estimate is not useful as we can construct bad unbiased estimates.
For example, if we simply pick a random element from a set, it is still an unbiased estimate of the mean of the set.
The variance of the estimate determines the size of our confidence intervals thus is important to consider.

The \sumfunc, \countfunc, and \avgfunc queries are linear functions of their input rows.
We explored whether our estimate was optimal for linear estimates, for example, should we weight our functions when applied to the sample.
It turns out that the proposed corrections are the optimal strategy when nothing is known about the data distribution a priori.

\begin{theorem}
For \sumfunc, \countfunc, and \avgfunc queries, our estimate of the correction is optimal over the class of linear estimators when no other information is known about the distribution. 
In other words, there exists no other linear function of the set input rows $\{ X_i \}$ that gives a lower variance correction.
\end{theorem}
\begin{proof}[Sketch]
As discussed before, we reformulate \sumfunc, \countfunc, and \avgfunc as means over the entire table. We prove the theorem by induction. 
Suppose, we have two independent unbiased estimates of the mean $\bar{X}_1$ and $\bar{X}_2$ and we want to merge them into one estimate $\bar{X} = c_1\bar{X}_1+c_2\bar{X}_2$.
Since both estimates are unbiased, $c_1 + c_2 = 1$ to ensure the merged estimate is also unbiased, and now we consider the variance of the merged estimate.
The variance of the estimate is $var(\bar{X}) = c_1^2var(\bar{X}_1) + c_2^2var(\bar{X}_2)$.
Since, we don't know anything about the distribution of the data, by symmetry $var(\bar{X}_1) = var(\bar{X}_2)$.
Consequently, the optimal choice is $c_1=c_2=0.5$. 
We can recursively induct, and we find that if we do not know the variance of data, as is impossible with new updates, then equally weighting all input rows is optimal. 
\end{proof}
The implication of this proof is that there does not exist any other weighted average that gives better estimate (for any distribution or sample size).

\subsection{General Aggregate Queries}
In SampleClean, we proposed the \nsc algorithm to give unbiased corrections to \sumfunc, \countfunc, and \avgfunc.
In this work, we found that these queries are a special case of a broader class of aggregate queries; namely if a query has an unbiased sample estimate there also exists an unbiased correction.
The implications of this are that any query that can be answered in prior work with SAQP (eg. in BlinkDB \cite{AgarwalMPMMS13}) in an unbiased way can also be answered with our approach.

Suppose, we have an aggregate query $q$ and we apply the query to the stale view $S$.
Without loss of generality, we assume this query is without a group by expression as we can always model this expression as part of the predicate.
The query result is stale by a factor of $c$ if:
\[ c = q(S') - q(S)\] 

\begin{lemma}\label{lemma:unbiased}
If there exists an unbiased sample estimator for q \reminder{$q$ or $q(S')$}then there exists an unbiased sample estimator for c.
\end{lemma}

\begin{proof}[Sketch] Suppose, we have an unbiased sample estimator $\bar{q}$ of $q$. 
Then, it follows that \[\mathbb{E}\big[\bar{q}(\hat{S'})\big] = q(S')\]
If we substitute in this expression:
\[ c = \mathbb{E}\big[\bar{q}(\hat{S'})\big] -q(s) \] 
Applying the linearity of expectation:
\[ c = \mathbb{E}\big[\bar{q}(\hat{S'}) - q(s)\big] \]
\end{proof}

However, in general, these aggregate functions do not satisfy the optimality conditions of the previous section.
These queries include: \histfunc, \corrfunc, \varfunc, \covfunc, and the estimation approach is different.
The general estimation procedure is the following: (1) apply q to the stale sample, (2) apply q to the clean sample, (3) apply q to the full stale view, and (4) finally take the difference between (2) and (1) and add it to (3). 

This procudure will also work for \sumfunc, \countfunc, and \avgfunc and in fact will give the same result estimate.
However, unlike before, it does not give us an analytic way to calculate a confidence interval.
This is because variance does not commute with subtraction for correlated random variables.
We can use a technique called a statistical bootstrap \cite{AgarwalMPMMS13} to empirically bound our correction.
In this approach, we repeatedly subsample with replacement from our sample and apply the sample estimator.
We use these repeat executions to build a distribution of values that the correction can take allowing us to bound the result.
The conditions under which this technique is valid are nuanced, and detecting when it is valid is a subject of \cite{agarwalknowing}.

\begin{proposition} (BOOTSTRAP OVER DIFFERENCES) Let q be an aggregate query that has an unbiased sample estimate, and let $\hat{S}$ and $\hat{S'}$ be sample views as defined before. \reminder{Revise the following two sentences. It seems that you repeatedly introduce $\hat{S'}_{sub}$ and $\hat{S}_{sub}$ twice. }
We denote subsamples of the samples as $\hat{S'}_{sub}$ and $\hat{S}_{sub}$ respectively.
One sample of the bootstrap estimator $s$ is defined as the difference of q applied to random subsample of size $b_1$ (with replacement) of $\hat{S}$ and $\hat{S'}$.
\[q(\hat{S'}_{sub}) - q(\hat{S}_{sub})\]
To build the confidence interval, we repeatedly apply this procedure $b_2$ times.
\end{proposition}

It is true that bootstrap is more computationally expensive than the analytical derivations for the \sumfunc, \countfunc, and \avgfunc queries, and it is also sensitive to the choice of parameters $b_1$ and $b_2$.
There has been recent research in using techniques such as Poissonized resampling \cite{agarwalknowing}, analytical bootstrap\cite{DBLP:conf/sigmod/ZengGMZ14}, and bagging \cite{DBLP:conf/kdd/KleinerTASJ13} to make this algorithm better suited for latency-sensitive query processing application.

\subsection{Bounded Bias Aggregates}
Some queries do not have unbiased sample estimators, but the bias of their sample estimators can be bounded. Example queries include: \medfunc, \percfunc.

A corollary to the previous lemma, is that if we can bound the bias for our estimator then we can achieve a bounded bias for $c$ as well.
\begin{corollary}
If there exists a bounded bias sample estimator for $q$ then there exists a bounded bias sample estimator for $c$.
\end{corollary}

What this means for a user is that the error bars are asymmetric with increased error in the direction of the bias (often in the direction of skew in the dataset).
Functionally, we can apply the same technique discussed in the previous section for these queries and apply the bootstrap to bound the results. 

\subsection{MIN and MAX}
\minfunc and \maxfunc fall into their own category since there does not exist any unbiased sample estimator nor can that bias be bounded.
We devise the following correction estimate for \maxfunc: (1) For all rows in both $S$ and $S'$, calculate the row-by-row difference, (2) let $c$ be the max difference, and (3) add $c$ to the max of the stale view.

We can give weak bounds on the results using Cantelli's Inequality.
If $X$ is a random variable with mean $\mu_x$ and variance $var(X)$, then the probability that $X$ is larger than a constant $\epsilon$ 
\[
\mathbb{P}(X \ge \epsilon + \mu_x ) \le \frac{var(X)}{var(X) + \epsilon^2}
\]
Therefore, if we set $\epsilon$ to be the difference between max value estimate and the average value, we can calculate the probability that we will see a higher value. 

The same estimator can be modified for \minfunc, with a corresponding bound:
\[
\mathbb{P}(X \le \mu_x - a )) \le \frac{var(x)}{var(x) + a^2}
\]
This bound has a slightly different interpretation than the confidence intervals seen before.
This gives the probability that a larger (or smaller) element exists in the unsampled view.

\subsection{Select Queries}
We can correct stale Select queries with SVC.
Suppose, we have a Select query with a simple predicate:
\begin{lstlisting} [mathescape]
SELECT $*$ FROM View 
WHERE $\cond(A)$
\end{lstlisting}

We can first run the Select query on the stale view, and the result of this is a multiset.
First, we must define what it means to ``correct" this stale result.
This result has three types of data error: rows that are missing, rows that are falsely included, and rows whose values are incorrect.

As in the aggregate query case, we can apply the query to the sample of the up-to-date view.
From this sample, using our lineage defined earlier, we can quickly identify which rows were added, updated, and deleted.
For the updated rows in the sample, we overwrite the out-of-date rows in the stale query result.
For the new rows, we take a union of the sampled selection and the updated stale selection.
For the missing rows, we remove them from the stale selection.
To quantify the approximation error, we can rewrite the Select query as \countfunc to get an estimate of number of rows that were not updated, added, or deleted (thus three ``confidence" intervals). 

\subsection{Query Execution Cost}
It is true that SVC adds to the query execution time by issuing a correction. 
However, this correction is only calculated on a sample and thus is small compared to the query execution time over the entire old view.
In our experiments, we show that the overheads are small in comparison to the execution time over the entire old view and the savings from only maintaining a sample (Section \ref{exp-datacube}).
