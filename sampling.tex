\section{Correction Query Processing}
\label{correction}
In the previous section, we discussed how to efficiently find $\hat{S'}$ by applying the hashing operator to the maintenance plan $\eta(\mathcal{M})$. 
The result of this execution is an up-to-date sampled materialized view $\hat{S'} \subset S'$.
In this section, we discuss how to correct stale query results using the two corresponding samples $\hat{S}$ and $\hat{S'}$. 

The main idea to compare $\hat{S'}$ to $\hat{S}$.
This gives us a uniform sample of the differences between the stale view and a hypothetical up-to-date view, and allows us to test how query results can be affected by the updates.
The key challenge is that even though the hashing operator is consistent and lineage is maintained through the updates; rows may be missing from either side due to the effects of updates.
We first present our correction query processing in Section~\ref{subsec:correct-principle} and analyze the costs in Section~\ref{subsec-corr-cost}. Then, we discuss the error bars and optimality of query corrections in Section~\ref{subsec:correct-practical}.

\begin{table*}[ht!]
\caption{Query Result Semantics} % title of Table
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Queries & Unbiased & Bounded Bias & Type of Bound \\ [0.5ex] % inserts table 
%heading
\hline % inserts single horizontal line
\sumfunc, \countfunc, \avgfunc & Yes & - & Optimal Analytical Via CLT \\ % inserting body of the table
\histfunc, \corrfunc, \varfunc, \covfunc & Yes & - & Empirical Via Bootstrap \\
\medfunc, \percfunc & No & Yes & Empirical Via Bootstrap \\
\maxfunc, \minfunc & No & No & Loose Probability Bound via Cantelli's Inequality \\
\texttt{f(DISTINCT)} & No & No & None in general \\ [1ex] % [1ex] adds vertical space
\hline %inserts single line
\hline
\texttt{SELECT *} & Yes & Yes & Optimal bound on result size 
\end{tabular}
\label{table:nonlin} % is used to refer this table in the text
\end{table*}

\subsection{ Aggregate Queries}
First, let us consider single-attribute, non-nested aggregate queries with simple predicates on a single view:
\begin{lstlisting} [mathescape]
SELECT $f(a)$ FROM View 
WHERE Condition(A);
\end{lstlisting}

In SampleClean, we proposed the \textbf{NormalizedSC} algorithm to give unbiased corrections to \sumfunc, \countfunc, and \avgfunc.
In this work, we found that these queries are a special case of a broader class of aggregate queries; namely if a query has an unbiased sample estimate there also exists an unbiased correction.
The implications of this are that any query that can be answered in prior work with SAQP in an unbiased way can also be answered with our approach.

Suppose, we have an aggregate query $q$ and we apply the query to the stale view $S$.
Without loss of generality, we assume this query is without a group by expression as we can always model this expression as part of the predicate.
The query result is stale by a factor of $c$ if:
\[ c = q(S') - q(S)\] 
\begin{lemma}
If there exists an unbiased sample estimator for q then there exists an unbiased sample estimator for c.
\end{lemma}

\emph{Proof.} Suppose, we have an unbiased sample estimator $\bar{q}$ of $q$. 
Then, it follows that \[\mathbb{E}(\bar{q}(\hat{S'})) = q(S')\]
If we substitute in this expression:
\[ c = \mathbb{E}(\bar{q}(\hat{S'})) -q(s) \] 
Applying the linearity of expectation:
\[ c = \mathbb{E}(\bar{q}(\hat{S'}) - q(s)) \blacksquare \]

\subsection{Mean-like Aggregates}
NormalizedSC considered \sumfunc, \countfunc, and \avgfunc.
We call this first class of queries,  \sumfunc, \countfunc, and \avgfunc, mean-like queries.
That is, these queries behave like a sample mean, and we can see that \sumfunc is a mean multiplied by the dataset size and \countfunc is the mean of the indicator function multipled by the dataset size.
A sample mean is an unbiased estimator of a population mean for all data distributions and sample sizes.
Furthermore, the Central Limit Theorem states that asymptotically this mean approaches a normal distribution giving very tight bounds on the error.

However, it is important to note, we are not trying to estimate \sumfunc, \countfunc, and \avgfunc but rather this correction factor $c$, the important question is how do we reformulate the problem to estimate $c$ as a sample mean.
This problem would be solved if we could somehow bring the subtraction inside the query:
\[ c = q(\hat{S'} - S)\] 
In NormalizedSC, we simply used the fact that these queries were commutative and associative and moved the subtraction into the summation.
However, it is possible that there are tuples in the new view and the old view that do not exist in the other.

We define a new operator $\check{-}$, which we call correspondence subtract, which handles this problem.
\begin{definition} (CORRESPONDENCE SUBTRACT). For two relations $R_1$ and $R_2$ with the schema $(a_1, a_2, ...)$ where 
$a_2$ is a numerical attribute. $\check{-}$ is defined as a projection of the full outer join on equality of $R_1.a_1 = R_2.a_2$: \[ \Pi_{R_1.a_2 - R_1.a_2} ( R_1 \fullouterjoin R_2 ) \]
 If $R_1.a_2$ or $R_2.a_2$ is $\emptyset$ then they are respectively represented as a $0$.
\end{definition}
Using this definition, we can write:
\[ c = q(\hat{S'} \check{-} \hat{S})\] 
The definition of this new operator allows us to be agnostic to both insertions $\Delta$ and $\nabla$ which can result missing records.

To make this clear in SQL, let use $\cond(\hat{S'})=1$ ($\cond(\hat{S})=1$) be \textbf{case} statments to denote that its old (new) record satisfies the query condition; otherwise, $\cond(\hat{S'})=0$ ($\cond(\hat{S})=0$). 
For a sampling ratio $m$, to obtain the correction result, we apply the query to the sample update patterns as follows:
%\begin{lstlisting}[mathescape,basicstyle={\scriptsize}] 
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{sum} = $ SELECT $ \sumfunc\big(S'*\textrm{Cond}(S') \check{-} S*\textrm{Cond}(S)\big)$ 
$~~~~~~~~~~~~~~~~~$ FROM $\hat{S'}, \hat{S}$ ;
$\sumfunc(S') \approx \sumfunc(S) + \dans_{sum}/m$
\end{lstlisting}

\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{cnt} = $ SELECT $ \sumfunc\big(\textrm{Cond}(S') \check{-} \textrm{Cond}(S)\big)$ 
$~~~~~~~~~~~~~~~~~$FROM $\hat{S'}, \hat{S}$;
$\countfunc(S') \approx \countfunc(S) + \dans_{cnt}/m$
\end{lstlisting}


\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{avg} = \frac{\dans_{sum}}{\dans_{cnt}}$;
$\avgfunc(S') \approx \avgfunc(S) + \dans_{avg}$
\end{lstlisting}

In each of these formulas, all of the $\dans$ terms correspond to estimates, and these estimates need to be bounded.
To find these bounds, we can rewrite the terms $\dans$ as a mean value of random variables.
By the Central Limit Theorem, the mean value of numbers drawn by uniform random sampling $\bar{X}$ approaches a normal distribution with:
\[
\bar{X} \sim N(\mu,\frac{\sigma^2}{k}),
\]
where $\mu$ is the true mean, $\sigma^2$ is the variance, and $k$ is the sample size.\footnote{\scriptsize For sampling without replacement, there is an additional term of $\sqrt{\frac{N-k}{N-1}}$. We consider estimates where N is large in comparison to k making this term insignificant.}
We can use this to bound the term with its 95\% confidence interval (or any other user specified probability), e.g., $\bar{X} \pm 1.96 \frac{\sigma}{\sqrt{k}}$; refer to our prior work \cite{wang1999sample} for futher details.

\subsection{Optimality Mean-like Aggregates}
We can prove that for the \sumfunc, \countfunc, and \avgfunc queries this estimate is optimal with respect to the variance.
\begin{proposition}
An estimator is called a minimum variance unbiased estimator (MVUE) of a parameter if it is unbiased and the variance of the parameter estimate is less than or equal to that of any other unbiased estimator of the parameter.
\end{proposition}
The concept of a Minimum Variance Unbiased Estimator (MVUE) comes from statistical decision theory \cite{cox1979theoretical}.
Unbiased estimators are ones that, in expectation, are correct.
However, on its own, the concept of an unbiased estimate is not useful as we can construct bad unbiased estimates.
For example, if we simply pick a random element from a set, it is still an unbiased estimate of the mean of the set.
The variance of the estimate determines the size of our confidence intervals thus is important to consider.

The \sumfunc, \countfunc, and \avgfunc queries are linear functions of their input rows.
We explored whether our estimate was optimal for linear estimates, for example, should we weight our functions when applied to the sample.
It turns out that the proposed corrections are the optimal strategy when nothing is known about the data distribution a priori.

\begin{theorem}
For \sumfunc, \countfunc, and \avgfunc queries, our estimate of the correction is optimal over the class of linear estimators when no other information is known about the distribution. 
In other words, there exists no other linear function of the set input rows $\{ X_i \}$ that gives a lower variance correction.
\end{theorem}
\begin{proof}[Sketch]
As discussed before, we reformulate \sumfunc, \countfunc, and \avgfunc as means over the entire table. We prove the theorem by induction. 
Suppose, we have two independent unbiased estimates of the mean $\bar{X}_1$ and $\bar{X}_2$ and we want to merge them into one estimate $\bar{X} = c_1\bar{X}_1+c_2\bar{X}_2$.
Since both estimates are unbiased, $c_1 + c_2 = 1$ to ensure the merged estimate is also unbiased, and now we consider the variance of the merged estimate.
The variance of the estimate is $var(\bar{X}) = c_1^2var(\bar{X}_1) + c_2^2var(\bar{X}_2)$.
Since, we don't know anything about the distribution of the data, by symmetry $var(\bar{X}_1) = var(\bar{X}_2)$.
Consequently, the optimal choice is $c_1=c_2=0.5$. 
We can recursively induct, and we find that if we do not know the variance of data, as is impossible with new updates, then equally weighting all input rows is optimal. 
\end{proof}
The implication of this proof is that there does not exist any other weighted average that gives better estimate (for any distribution or sample size).

\subsection{Other Unbiased Aggregates}
There is further a class of aggregate queries that is also unbiased but does not satisfy the optimality conditions of the previous section.
The reason is that the subtraction-trick applied in the previous section will not work in general for possibly correlated variables.
These queries include: \histfunc, \corrfunc, \varfunc, \covfunc, and the estimation approach is different.
We apply the query to stale view, and then we apply the sample estimate of the query to up-to-date sample.

The question is how can we bound this estimate.
We can use a technique called a statistical bootstrap \cite{AgarwalMPMMS13}.
In this approach, we repeatedly subsample with replacement from our sample and apply the sample estimator.
We use these repeat executions to build a distribution of values that the correction can take allowing us to bound the result.
The conditions under which this technique is valid are nuanced, and detecting when it is valid is a subject of \cite{agarwalknowing}.

\begin{proposition} (BOOTSTRAP OVER DIFFERENCES) Let q be an unbiased aggregate query, and let $\hat{S}$ and $\hat{S'}$.
One sample of the bootstrap estimator $s$ is defined as the difference of q applied to random subsample of size $b_1$ (with replacment) of $q(\hat{S'}_sub) - q(\hat{S}_sub)$.
To build the confidence interval, we repeatedly apply this procedure $b_2$ times.
\end{proposition}

It is true that bootstrap is more computationally expensive than the analytical derivations for the ``Mean-like" queries, and it is also sensitive to the choice of parameters $b_1$ and $b_2$.
There has been recent research in using techniques such as Poissonized resampling \cite{agarwalknowing}, and bagging \cite{DBLP:conf/kdd/KleinerTASJ13} to make this algorithm better suited for latency-senstive query processing application.
As the bootstrap estimator is empirical and asymptotically consistent, its bound when it applies. 
In fact, in practice, the bootstrap estimator often gives tighter confidence intervals than the CLT for smaller data sizes (a further note in the Discussion Section ??).

\subsection{Bounded Bias Aggregates}
A corollary to Proposition ??, is that if we can bound the bias for our estimator then we can acheive a bounded bias for $c$ as well.
\begin{corollary}
If there exists a bounded bias sample estimator for $q$ then there exists a bounded bias sample estimator for $c$.
\end{corollary}
Example queries include: \medfunc, \percfunc.

What this means for a user is that the error bars are asymmetric with increased error in the direction of the bias (often in the direction of skew in the dataset).
Functionally, we can apply the same technique discussed in Section ??. 

\subsection{MIN and MAX}
\minfunc and \maxfunc fall into their own category since there does not exist any unbiased sample estimator nor can that bias be bounded.
We can, however, give weak bounds on the results using Cantelli's Inequality:
\[
\mathbb{P}(X \ge a + \mu_x )) \le \frac{var(x)}{var(x) + a^2}
\]
Therefore, if we set $a$ to be the max value, we can calculate the probability that we will see a higher value. The same logic can be reversed for \minfunc.
\[
\mathbb{P}(X \le \mu_x - a )) \le \frac{var(x)}{var(x) + a^2}
\]

\subsection{Select Queries}
We can correct stale Select queries with SVC.
Suppose, we have a Select query with a simple predicate:
\begin{lstlisting} [mathescape]
SELECT $*$ FROM View 
WHERE Condition(A);
\end{lstlisting}

We can first run the Select on the stale view, and the result of this is a multiset.
First, we must define what it means to ``correct" this stale result.
This result has three types of data error: rows that are missing, rows that are falsely included, and rows whose values are incorrect.

As in the aggregate query case, we can apply the query to the sample of the up-to-date view.
From this sample, using our lineage defined earlier, we can quickly identify which rows were added, updated, and deleted.
For the updated rows in the sample, we overwrite the out-of-date rows in the stale query result.
For the new rows, we take a union of the sampled selection and the updated stale selection.
For the missing rows, we remove them from the stale selection.
To quantify the approximation error, we can rewrite the Select query as \countfunc to get an estimate of number of rows that were not updates, added, or deleted (thus three ``confidence" intervals).

\subsection{Query Execution Cost}
It is true that SVC adds to the query execution time by issuing a correction, and we can quantify this overhead. Let $m$ be the sampling ratio, $\mid S \mid$ be the cardinality stale view, and $\mid S' \mid$ be the cardinality  of the up-to-date view. Also, let $\textbf{join}$ be the cost of the one-to-one join of the sampled up-to-date view and the sampled stale view. Since the expensive step is a one-to-one join of two samples, we assume that this can be executed fairly quickly (eg. via hash table lookup).

\subsubsection{Mean-like queries and Select}
In the worst case, when executing the query requires a full scan, the correction plan requires:
 \[O(\mid S \mid + m \cdot \mid S' \mid + \textbf{join})\]

 \subsubsection{Bootstrap Queries}
When executing queries that require bootstrap for error estimation, there are additional costs $b$ to the bootstrap error estimate which are on the order of the sample size:
 \[O(\mid S \mid + b\cdot m \cdot \mid S' \mid + \textbf{join})\]

