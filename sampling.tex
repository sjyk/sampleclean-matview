\section{Sampling for Approximate Corrections}
In the previous section, we described how to do the correction using the full delta view.
However, this is just as expensive as incremental view maintenance.
In this section, we describe how we can couple sampling with the correction calculation to
save on I/O and communication costs. 

\subsection{Sampling for Select-Project and Foreign-Key Join Views}
We can extend our deterministic corrections to approximate corrections from a 
simple random sample $S_{\Delta V}$ of the delta view $S_{\Delta V}\subseteq\Delta\textbf{V}$. 
Recall that a simple random sample is uniform sample where every row $r\in\Delta V$
is in $S_{\Delta V}$ with equal probability $p$. For Select-Project
and Foreign-Key Join Views, this means we have to take a sample of
the updates and then apply the view definition to the sample of the
updates. Formally, for every record u inserted into the table, with
probability $p$, we include it in the sample $S$. Then, we take
the sample updates $S$ and apply the view definition forming $S_{\Delta V}$.
Therefore,
\[
c\cdot f(S_{\Delta V})\approx\epsilon
\]
The scaling constant $c$ for the SUM and COUNT queries
depends on the sample size and is $c = \frac{K}{N}$.
This estimate is guaranteed to be unbiased, that is, in expectation the answer is $\epsilon$.

\subsection{Cost Analysis for Select-Project and Foreign-Key Join Views}
Let $n$ be the number of inserted records, $v$ be the cardinality of the old view, $v'$ be the cardinality of the new view, $\delta_v$ be the cardinality of the delta view, and $p$ be the sampling ratio.  

\textbf{Scan of Updates: }
Both incremental maintenance and our proposed solution require at least one scan of the inserted records, and in both solutions we can
load the updates into memory once and amortize that I/O cost over all views. 

\textbf{Delta View: } For Select-project and foreign-key join views, incremental maintenance has processing cost of $n$ records where the predicate or the join has to be evaluated for each inserted record. Our approach has a cost of $np$ as we have to evaluate this only on our sample. 

\textbf{Refresh: } For Select-project and foreign-key join views, incremental maintenance has to insert $\delta_v$ rows while we have to insert only $p\delta_v$ records. If there is an outlier index, this cost increases to $p\delta_v + l$. 

\textbf{Query: } As incremental maintenance completely refreshes the view, the cost of processing a query on the view is at most $v'$. For Select-project and foreign-key join views, the processing cost is $v + p\delta_v$ and this is guaranteed to be less than $v'$. 

\subsection{Accuracy Analysis For Select-Project and Foreign-Key Join Views}
By the Central Limit Theorem, means of independent random variables converge 
to a Gaussian distribution.
We can apply this theorem to bound the approximation error in the correction $\epsilon$.
For $\epsilon = c\cdot f(S_{\Delta V})$, and a sampling ratio of $p$.
Recall, from the previous section that $f$ can be expressed as a sum of the function $\phi$ applied to each row.
Let $\phi(S_{\Delta V})$ be set of $\phi$ applied to each of rows in the sample delta view.
Then variance of the estimate is:
\[
c^2\frac{var(\phi(S_{\Delta V}))}{p|\Delta V|}
\] 
Since there is asymptotic convergence to a normal distribution, we can use this variance to bound the distance the true $\epsilon$:   
\[
\epsilon \pm \gamma \cdot c\frac{std(\phi(S_{\Delta V}))}{p|\Delta V|}
\]
We can set $\gamma$ to our desired normal confidence level (eg. 1.96 for 95\% confidence).
In other words, the accuracy of the estimate depends on the variance of the inserted records.

\subsection{Sampling for Aggregation Views}
In the previous section, we discussed how the delta view did not contain enough
information to calculate a correction.
Similarly, sampling to estimate the correction for queries on Aggregation Views
is more challenging.
We notice that in the refreshed view each GROUP BY key is unique, and
thus, to sample the refreshed view we have to sample by GROUP BY keys
in the inserted records. For each inserted record we apply a hash
to the cols in the GROUP BY clause, and then we take the result of
the hash modulo a sampling ratio to sample the table. The result is
that we ensure that every record with the same group by key is either
fully in the sample or not, thus none of the rows in the delta view
are approximate. 
Then, we refresh this sample delta view instead of the full view.
We can then join this delta view with the old view to approximate $\epsilon$.

Unlike before this is not a sample of a delta view, it is a sample of the entire view joined with the stale resuts.
Thus, we denote this sample as $S_{V}$.
Also, recall that unlike the other views, Aggregation Views did not have 
an additional proportionality constant for a full correction.
\[
f(S_{V})\approx\epsilon
\]
However, when we introduce sampling a scaling constant is now neccessary.
The scaling constant $c$ for the SUM and COUNT queries depends on the sample size and is $c = \frac{K}{N}$, 
but $c = 1$ for the AVG query.
For SUM, COUNT, AVG, and VAR, this estimate of $\epsilon$ is unbiased as before.

\subsection{Cost Analysis for Aggregation Views}
Let $n$ be the number of inserted records, $v$ be the cardinality of the old view, $v'$ be the cardinality of the new view, $\delta_v$ be the cardinality of the delta view, and $p$ be the sampling ratio.  

\textbf{Scan of Updates: }
As in the Select-Project and Foreign-Key Join views, both incremental maintenance and our proposed solution require at least one scan of the inserted records, and in both solutions we can load the updates into memory once and amortize that I/O cost over all views. 

\textbf{Delta View: }  For aggregation views, incremental maintenance has the processing cost of $n$ and in addition an aggregation cost of $\delta_v$ where aggregates for each of the groups have to be maintained. In contrast, for aggregation views, our approach has a cost of $p\delta_v$ as we sample the group by keys and an expected processing cost of $np$. 
For aggregation views, in a distributed environment, there are potentially additional communication costs as the updates may not be partitioned by the group by key.

\textbf{Refresh: } For aggregation views, the cost is a little bit more complicated as we have a combination of insertions into the view and updates to the view. 
Incremental maintenance has to refresh $\delta_v$ rows while we have to refresh $p\delta_v$ rows. 
If there is a join index, in constant time, we can determine which rows are new insertions and which correspond to rows already in the view.

The costs become higher in a distributed environment as we need to consider communication and query processing engines that rely on partitioned joins rather than indices.
For aggregation views, we want to partition the data by the group by key.
This allows a paritioned join which only requires communication (a shuffle operation) of the delta table.
Therefore, in incremental maintenance we have to communicate $\delta_v$ rows while our solution requires $p\delta_v$ rows.

\textbf{Query: } For aggregation views, the cost is $v + pv'$ where we calculate a correction by processing $pv'$ rows and correct an existing aggregation of $v$ records.

\subsection{Accuracy Analysis For Aggregation Views}
As in the Select-Project and Foreign-Key views the Central Limit Theorem can
be used to bound the approximation.
However, there are two cases with aggregation views: (1) updates to existing rows and (2) insertions into the view.
Let $S_{V}^{(i)}$ be the set differences for those rows in the sample materialized view that are updated; that is, represents how much each attribute 
changes, and let $S_{V}^{(i)}$ be the sample materialized view that correspond to inserted records.
Let $w_u$ by the fraction of the delta view which are updates to existing rows and $w_i$ be the fraction insertions into the view.

Recall, from the previous section that an aggregation function on the view $f$ can be expressed as mean of the function $\phi$ applied to each row.
Let $\phi(S_{V}^{(i)})$ be set of $\phi$ applied to each of rows that inserted, $\phi(S_{V}^{(i)})$ be the set of $\phi$ applied to the update differences, and $\phi(S_{V})$ be the union of the two sets.
As before, we can use the Central Limit Theorem to calculate the variance of the estimate and bound the approximation error.
The variance of a mixture of two distributions is:
\[
\sigma^2 = w_1((\mu_1-\mu)^2 + \sigma^2_1) + w_2((\mu_2-\mu)^2 + \sigma^2_2)
\]
Applying this in our setting, let us define the following values:
\[
\mu^{(i)}_{diff} = mean(\phi(S_{V}^{(i)})) - mean(\phi(S_{V}))
\]
\[
\mu^{(u)}_{diff} = mean(\phi(S_{V}^{(i)})) - mean(\phi(S_{V}))
\]
\[
\sigma^2_{(i)} = var(\phi(S_{V}^{(i)}))
\]
\[
\sigma^2_{(u)} = var(\phi(S_{V}^{(u)}))
\]
\[
\sigma^2_{diff} = w_i((\mu^{(i)}_{diff})^2 + \sigma_{(i)}) + w_u((\mu^{(u)}_{diff})^2 + \sigma_{(u)})
\] 
Therefore, the estimate variance is:
\[
c^2\frac{\sigma^2_{diff}}{|S_{V}|}
\]
And as before there is asymptotic convergence to a normal distribution, we can use this variance to bound the distance the true $\epsilon$:   
\[
\epsilon \pm \gamma \cdot c\frac{\sigma^2_{diff}}{|S_{V}|}
\]
We can set $\gamma$ to our desired normal confidence level (eg. 1.96 for 95\% confidence).

To interpret $\sigma^2_{diff}$, it not only has dependence on the variance of the inserted records, but also on the variance of the updates and the relative difference between the updates and inserted records. 
A worst case, would be if there is a 50-50 split between updates and insertions and changes to existing rows are in distribution very different than the new inserted rows in the materialized view. 

