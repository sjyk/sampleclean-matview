
\section{Correcting Stale Query Results}
\label{correction}
In this section, we discuss how to correct stale query results using sample update patterns. We first present our correcting query processing in Section~\ref{subsec:correct-principle} and then discuss some practical considerations in Section~\ref{subsec:correct-practical}.


\subsection{\Cqp}\label{subsec:correct-principle}
Given a stale view and a query on the view, let \ans denote the stale query result. The goal of \cqp is to use the sample update patterns w.r.t the view to correct \ans. To achieve this goal, our basic idea is to first obtain a \emph{delta query result} (denoted by $\dans$), by rewriting the query against the sample update patterns, and then combine \ans and $\dans$ into a corrected query result. 

In the following, we will discuss how to apply this idea to different types of views and analyze the cost of our correcting query processing. For ease of presentation, we assume the query does not have a group-by clause, i.e.,
\begin{lstlisting} [mathescape]
SELECT $\sumfunc(a)/\countfunc(a)/\avgfunc(a)$ FROM View 
WHERE Condition(A);
\end{lstlisting}
But note that our methods can be easily extended to a group-by query by treating each group key as an addition condition of the query. 

%where \texttt{f} is an aggregation function (i.e., $\avgfunc$, $\sumfunc$, $\countfunc$), \texttt{a} is an attribute, and \texttt{A} is a set of attributes. 





%formulation

%basic idea

\subsubsection{Select-Project and Foreign-Key Join Views} \label{subsubsec:correct-spfj}
Since both of the update patterns of \spview and \fjview only consist of inserted records, their correcting query processing phases are the same. Given the above query, to get its delta query result, we rewrite the query and run it on the sample update patterns. The following show the rewritten queries for different aggregation functions.  

\vspace{-.25em}

\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{sum} = $ SELECT $\sumfunc(a)$ FROM $\textrm{sample\_update\_patterns}$ 
$~~~~~~~~~~~~~~~$ WHERE Condition(A);
\end{lstlisting}

\vspace{-.25em}

\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{cnt} = $ SELECT $\countfunc(a)$ FROM $\textrm{sample\_update\_patterns}$ $~~~~~~~~~$WHERE Condition(A);
\end{lstlisting}

\vspace{-.25em}

\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{avg}, \dans_{cnt} = $ SELECT $\avgfunc(a)$, $\countfunc(a)$ 
$~~~~~~~~~~~~~~~~~~~~~~~~~~$ FROM $\textrm{sample\_update\_patterns}$
$~~~~~~~~~~~~~~~~~~~~~~~~~~$ WHERE Condition(A);
\end{lstlisting}

\vspace{-.25em}

For the \sumfunc and \countfunc queries, we only need to compute $\dans_{sum}$ and $\dans_{count}$, respectively. But for the \avgfunc query, in addition to $\dans_{avg}$, we also need to compute $\dans_{cnt}$ for query correction.


\begin{table}[tup]\renewcommand{\arraystretch}{1.5}
\caption{Correcting a stale query result}\label{tbl:query-correct} \scriptsize \centering
%\begin{tabular}[t]{|c@{\::\:}l@{\:,\:}l@{\:,\:}r|}
%\begin{tabular}[t]{|@{\:\:}l@{\:\:}||@{\:\:}r@{\:\:}|@{\:\:}r@{\:\:}|@{\:\:}r@{\:\:}|}
\hspace*{-1em}\begin{tabular}[t]{|l||c|c|c|}
  %\multicolumn{4}{c}{\normalsize {(a) $\ars$}} \\ \hline
  %\multicolumn{4}{|c|}{A set $\Lambda$ of $\ar$s} \\ \hline\hline
   \hline
                & \bf{\specialcell{\spview \& \fjview}} & \bf{\aggview} \\ \hline \hline
   \sumfunc     &  $\ans_{sum}+\dans_{sum}/\ratio$ &   $\ans_{sum}+\dans_{sum}/\ratio$        \\ \hline 
   \countfunc   &  $\ans_{cnt}+\dans_{cnt}/\ratio$ &   $\ans_{cnt}+\dans_{cnt}/\ratio$       \\ \hline 
   \avgfunc     &  $\frac{\ans_{cnt}\cdot\ans_{avg}+(\dans_{cnt}/\ratio)\cdot\dans_{avg}}{\ans_{cnt}+(\dans_{cnt}/\ratio)}$                                &     $\ans_{avg}+\dans_{avg}$     \\ \hline 
\end{tabular}
\end{table}

After obtaining delta query results, we next discuss how to use them to correct stale query results. Table~\ref{tbl:query-correct} shows the corrected results of  the \sumfunc, \countfunc, and \avgfunc queries. 

To correct a \sumfunc query result, we cannot simply add it by the delta query result (i.e., $\ans_{sum}+\dans_{sum}$) since the delta query result is computed on the sample update patterns. Thus, we have to scale the delta query result according to the sampling ratio and correct the stale query result by adding the rescaled value, i.e., $\ans_{sum}+\dans_{sum}/\ratio$. 

To correct a \countfunc query result, we can use the same idea as above since a \countfunc query can be thought as the special case of a \sumfunc query when aggregate attribute values are all equal to one. Thus, the corrected \countfunc query result is $\ans_{count}+\dans_{count}/\ratio$.

To correct an \avgfunc query result, we can imagine it as computing a weighted average value between a stale view and update patterns. In a stale view, $\ans_{avg}$ is the average value over $\ans_{cnt}$ records\footnote{\small $\ans_{cnt}$ denotes the number of records that satisfy the \avgfunc query's condition, which can be easily obtained from computing $\ans_{avg}$}, thus its weight is $\ans_{cnt}$; In the sample update patterns, $\ans_{avg}$ is the average value over $\dans_{cnt}$ records. Since the value is computed on a sample, its weight is $\dans_{cnt}/\ratio$. Therefore, the corrected \avgfunc query result is computed as  $\frac{\ans_{cnt}\cdot\ans_{avg}+(\dans_{cnt}/\ratio)\cdot\dans_{avg}}{\ans_{cnt}+(\dans_{cnt}/\ratio)}$.




\subsubsection{Aggregation View}\label{subsubsec:correct-agg}
The update patterns of \aggview contain the information that how every old record should be changed to a new one. Given the query as shown in Section~\ref{subsec:correct-principle}, let $a_{old}$ ($a_{new}$) denote the old (new) attribute in the update patterns corresponding to $a$; let $A_{old}$ ($A_{new}$) denote the old (new) attribute set in the update patterns corresponding to $A$.
For a single update pattern, we use $\cond(A_{old}) = 1$ ($\cond({A_{new}}) = 1$) to denote that the old (new) record satisfies the query condition; otherwise, $\cond({A_{old}}) = 0$ ($\cond({A_{new}}) = 0$). The following show the rewritten queries of getting the delta query result for each aggregation function. 


%\begin{lstlisting}[mathescape,basicstyle={\scriptsize}] 
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{sum} = $ SELECT $ \sumfunc\big(a_{new}*\textrm{Cond}(A_{new})-a_{old}*\textrm{Cond}(A_{old})\big)$ 
$~~~~~~~~~~~~~~~~~$FROM $\textrm{sample\_update\_patterns}$;
\end{lstlisting}

\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{cnt} = $ SELECT $ \sumfunc\big(\textrm{Cond}(A_{new})-\textrm{Cond}(A_{old})\big)$ 
$~~~~~~~~~~~~~~$ FROM $\textrm{sample\_update\_patterns}$;
\end{lstlisting}


\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
$\dans_{avg} = $ SELECT $
\frac{\sumfunc\big(a_{new}*\textrm{Cond}(A_{new})\big)}{\sumfunc\big(\textrm{Cond}(A_{new})\big)}- \frac{\sumfunc\big(a_{old}*\textrm{Cond}(A_{old})\big)}{\sumfunc\big(\textrm{Cond}(A_{old})\big)}$ 
$~~~~~~~~~~~~~~~~$FROM $\textrm{sample\_update\_patterns}$;
\end{lstlisting}

Intuitively, a delta query result tells us how data updates will affect the query result on an old (i.e., a stale) sample \aggview. Thus, it is computed as the difference between the query results on a stale sample \aggview and an updated sample \aggview. To use it to correct a stale query result (Table~\ref{tbl:query-correct}), for the \sumfunc query, since the \sumfunc difference is computed on a sample, we need to scale it by the sampling ratio and add the stale query result by the scaled value, i.e., $\ans_{sum}+\dans_{sum}/\ratio$; for the \countfunc query, since a \countfunc query can be taken a special case of a \sumfunc query, similar to the \sumfunc query, we have $\ans_{count}+\dans_{count}/\ratio$; for the \avgfunc query, since the \avgfunc difference on a sample is an unbiased estimation of the \avgfunc difference on the full data, we add the stale query result by the delta query result directly, i.e., $\ans_{avg}+\dans_{avg}$.


\input{practical-subsec.tex}


\iffalse
\section{Sampling for Approximate Corrections}
In the previous section, we described how to do the correction using the full delta view.
However, this is just as expensive as incremental view maintenance.
In this section, we describe how we can couple sampling with the correction calculation to
save on I/O and communication costs. 

\subsection{Sampling for Select-Project and Foreign-Key Join Views}
We can extend our deterministic corrections to approximate corrections from a 
simple random sample $S_{\Delta V}$ of the delta view $S_{\Delta V}\subseteq\Delta\textbf{V}$. 
Recall that a simple random sample is uniform sample where every row $r\in\Delta V$
is in $S_{\Delta V}$ with equal probability $p$. For Select-Project
and Foreign-Key Join Views, this means we have to take a sample of
the updates and then apply the view definition to the sample of the
updates. Formally, for every record u inserted into the table, with
probability $p$, we include it in the sample $S$. Then, we take
the sample updates $S$ and apply the view definition forming $S_{\Delta V}$.
Therefore,
\[
c\cdot f(S_{\Delta V})\approx\epsilon
\]
The scaling constant $c$ for the SUM and COUNT queries
depends on the sample size and is $c = \frac{K}{N}$.
This estimate is guaranteed to be unbiased, that is, in expectation the answer is $\epsilon$.

\subsection{Cost Analysis for Select-Project and Foreign-Key Join Views}
Let $n$ be the number of inserted records, $v$ be the cardinality of the old view, $v'$ be the cardinality of the new view, $\delta_v$ be the cardinality of the delta view, and $p$ be the sampling ratio.  

\textbf{Scan of Updates: }
Both incremental maintenance and our proposed solution require at least one scan of the inserted records, and in both solutions we can
load the updates into memory once and amortize that I/O cost over all views. 

\textbf{Delta View: } For Select-project and foreign-key join views, incremental maintenance has processing cost of $n$ records where the predicate or the join has to be evaluated for each inserted record. Our approach has a cost of $np$ as we have to evaluate this only on our sample. 

\textbf{Refresh: } For Select-project and foreign-key join views, incremental maintenance has to insert $\delta_v$ rows while we have to insert only $p\delta_v$ records. If there is an outlier index, this cost increases to $p\delta_v + l$. 

\textbf{Query: } As incremental maintenance completely refreshes the view, the cost of processing a query on the view is at most $v'$. For Select-project and foreign-key join views, the processing cost is $v + p\delta_v$ and this is guaranteed to be less than $v'$. 

\subsection{Accuracy Analysis For Select-Project and Foreign-Key Join Views}
By the Central Limit Theorem, means of independent random variables converge 
to a Gaussian distribution.
We can apply this theorem to bound the approximation error in the correction $\epsilon$.
For $\epsilon = c\cdot f(S_{\Delta V})$, and a sampling ratio of $p$.
Recall, from the previous section that $f$ can be expressed as a sum of the function $\phi$ applied to each row.
Let $\phi(S_{\Delta V})$ be set of $\phi$ applied to each of rows in the sample delta view.
Then variance of the estimate is:
\[
c^2\frac{var(\phi(S_{\Delta V}))}{p|\Delta V|}
\] 
Since there is asymptotic convergence to a normal distribution, we can use this variance to bound the distance the true $\epsilon$:   
\[
\epsilon \pm \gamma \cdot c\frac{std(\phi(S_{\Delta V}))}{p|\Delta V|}
\]
We can set $\gamma$ to our desired normal confidence level (eg. 1.96 for 95\% confidence).
In other words, the accuracy of the estimate depends on the variance of the inserted records.

\subsection{Sampling for Aggregation Views}
In the previous section, we discussed how the delta view did not contain enough
information to calculate a correction.
Similarly, sampling to estimate the correction for queries on Aggregation Views
is more challenging.
We notice that in the refreshed view each GROUP BY key is unique, and
thus, to sample the refreshed view we have to sample by GROUP BY keys
in the inserted records. For each inserted record we apply a hash
to the cols in the GROUP BY clause, and then we take the result of
the hash modulo a sampling ratio to sample the table. The result is
that we ensure that every record with the same group by key is either
fully in the sample or not, thus none of the rows in the delta view
are approximate. 
Then, we refresh this sample delta view instead of the full view.
We can then join this delta view with the old view to approximate $\epsilon$.

Unlike before this is not a sample of a delta view, it is a sample of the entire view joined with the stale resuts.
Thus, we denote this sample as $S_{V}$.
Also, recall that unlike the other views, Aggregation Views did not have 
an additional proportionality constant for a full correction.
\[
f(S_{V})\approx\epsilon
\]
However, when we introduce sampling a scaling constant is now neccessary.
The scaling constant $c$ for the SUM and COUNT queries depends on the sample size and is $c = \frac{K}{N}$, 
but $c = 1$ for the AVG query.
For SUM, COUNT, AVG, and VAR, this estimate of $\epsilon$ is unbiased as before.

\subsection{Cost Analysis for Aggregation Views}
Let $n$ be the number of inserted records, $v$ be the cardinality of the old view, $v'$ be the cardinality of the new view, $\delta_v$ be the cardinality of the delta view, and $p$ be the sampling ratio.  

\textbf{Scan of Updates: }
As in the Select-Project and Foreign-Key Join views, both incremental maintenance and our proposed solution require at least one scan of the inserted records, and in both solutions we can load the updates into memory once and amortize that I/O cost over all views. 

\textbf{Delta View: }  For aggregation views, incremental maintenance has the processing cost of $n$ and in addition an aggregation cost of $\delta_v$ where aggregates for each of the groups have to be maintained. In contrast, for aggregation views, our approach has a cost of $p\delta_v$ as we sample the group by keys and an expected processing cost of $np$. 
For aggregation views, in a distributed environment, there are potentially additional communication costs as the updates may not be partitioned by the group by key.

\textbf{Refresh: } For aggregation views, the cost is a little bit more complicated as we have a combination of insertions into the view and updates to the view. 
Incremental maintenance has to refresh $\delta_v$ rows while we have to refresh $p\delta_v$ rows. 
If there is a join index, in constant time, we can determine which rows are new insertions and which correspond to rows already in the view.

The costs become higher in a distributed environment as we need to consider communication and query processing engines that rely on partitioned joins rather than indices.
For aggregation views, we want to partition the data by the group by key.
This allows a paritioned join which only requires communication (a shuffle operation) of the delta table.
Therefore, in incremental maintenance we have to communicate $\delta_v$ rows while our solution requires $p\delta_v$ rows.

\textbf{Query: } For aggregation views, the cost is $v + pv'$ where we calculate a correction by processing $pv'$ rows and correct an existing aggregation of $v$ records.

\subsection{Accuracy Analysis For Aggregation Views}
As in the Select-Project and Foreign-Key views the Central Limit Theorem can
be used to bound the approximation.
However, there are two cases with aggregation views: (1) updates to existing rows and (2) insertions into the view.
Let $S_{V}^{(i)}$ be the set differences for those rows in the sample materialized view that are updated; that is, represents how much each attribute 
changes, and let $S_{V}^{(i)}$ be the sample materialized view that correspond to inserted records.
Let $w_u$ by the fraction of the delta view which are updates to existing rows and $w_i$ be the fraction insertions into the view.

Recall, from the previous section that an aggregation function on the view $f$ can be expressed as mean of the function $\phi$ applied to each row.
Let $\phi(S_{V}^{(i)})$ be set of $\phi$ applied to each of rows that inserted, $\phi(S_{V}^{(i)})$ be the set of $\phi$ applied to the update differences, and $\phi(S_{V})$ be the union of the two sets.
As before, we can use the Central Limit Theorem to calculate the variance of the estimate and bound the approximation error.
The variance of a mixture of two distributions is:
\[
\sigma^2 = w_1((\mu_1-\mu)^2 + \sigma^2_1) + w_2((\mu_2-\mu)^2 + \sigma^2_2)
\]
Applying this in our setting, let us define the following values:
\[
\mu^{(i)}_{diff} = mean(\phi(S_{V}^{(i)})) - mean(\phi(S_{V}))
\]
\[
\mu^{(u)}_{diff} = mean(\phi(S_{V}^{(i)})) - mean(\phi(S_{V}))
\]
\[
\sigma^2_{(i)} = var(\phi(S_{V}^{(i)}))
\]
\[
\sigma^2_{(u)} = var(\phi(S_{V}^{(u)}))
\]
\[
\sigma^2_{diff} = w_i((\mu^{(i)}_{diff})^2 + \sigma_{(i)}) + w_u((\mu^{(u)}_{diff})^2 + \sigma_{(u)})
\] 
Therefore, the estimate variance is:
\[
c^2\frac{\sigma^2_{diff}}{|S_{V}|}
\]
And as before there is asymptotic convergence to a normal distribution, we can use this variance to bound the distance the true $\epsilon$:   
\[
\epsilon \pm \gamma \cdot c\frac{\sigma^2_{diff}}{|S_{V}|}
\]
We can set $\gamma$ to our desired normal confidence level (eg. 1.96 for 95\% confidence).

To interpret $\sigma^2_{diff}$, it not only has dependence on the variance of the inserted records, but also on the variance of the updates and the relative difference between the updates and inserted records. 
A worst case, would be if there is a 50-50 split between updates and insertions and changes to existing rows are in distribution very different than the new inserted rows in the materialized view. 
\fi
