\subsection{Error Bars and Optimality}\label{subsec:correct-practical}
\subsubsection{Confidence Intervals}
In Table \ref{tbl:query-correct}, we present formulas to correct stale queries.
In each of these formulas, all of the $\dans$ terms correspond to estimates and these estimates need to be bounded.
The intuition is that we can rewrite the terms $\dans$ as a mean value of uniformly random variables.
By the Central Limit Theorem, the mean value of numbers drawn by uniform random sampling $\bar{X}$ approaches a normal distribution with:
\[
\bar{X} \sim N(\mu,\frac{\sigma^2}{k}),
\]
where $\mu$ is the true mean, $\sigma^2$ is the variance, and $k$ is the sample size.\footnote{\scriptsize For sampling without replacement, there is an additional term of $\sqrt{\frac{N-k}{N-1}}$. We consider estimates where N is large in comparison to k making this term insignificant.}
We can use this to bound the term with its 95\% confidence interval (or any other user specified probability), e.g., $\bar{X} \pm 1.96 \frac{\sigma}{\sqrt{k}}$.

For all of the queries in \aggview, and for \sumfunc, \countfunc in \spview and \fjview, there is only one term to bound.
However, for \avgfunc in \spview and \fjview, as presented there are two terms to bound.
Using the associativity properties of these queries, we can re-write the correction for \avgfunc in terms of a single summation.
The numerator can be written as a single \sumfunc query, where we move the stale result $\ans_{cnt}\cdot\ans_{avg}$ inside the estimate.
Next, we notice that the reformulation is now a \sumfunc over a \countfunc of the same set of records (with an additional offset of $ans_{cnt}$.
Thus, this can be re-written as a single mean value. 
See \cite{wang1999sample} for more details on the confidence intervals.

\subsubsection{Optimality}
We can prove that for the \sumfunc, \countfunc, and \avgfunc queries this estimate is optimal with respect to the variance.
\begin{proposition}
An estimator is called a minimum variance unbiased estimator (MVUE) of a parameter if it is unbiased and the variance of the parameter estimate is less than or equal to that of any other unbiased estimator of the parameter.
\end{proposition}
The concept of a Minimum Variance Unbiased Estimator (MVUE) comes from statistical decision theory \cite{cox1979theoretical}.
Unbiased estimators are ones that, in expectation, are correct.
However, on its own, the concept of an unbiased estimate is not useful as we can construct bad unbiased estimates.
For example, if we simply pick a random element from a set, it is still an unbiased estimate of the mean of the set.
The variance of the estimate determines the size of our confidence intervals thus is important to consider.

The \sumfunc, \countfunc, and \avgfunc queries are linear functions of their input rows.
We explored whether our estimate was optimal for linear estimates, for example, should we weight our functions when applied to the sample.
It turns out that the proposed corrections are the optimal strategy when nothing is known about the data distribution a priori.

\begin{theorem}
For \sumfunc, \countfunc, and \avgfunc queries, our estimate of the correction is optimal over the class of linear estimators when no other information is known about the distribution. 
In other words, there exists no other linear function of the set input rows $\{ X_i \}$ that gives a lower variance correction.
\end{theorem}
\begin{proof}[Sketch]
We can reformulate \sumfunc, \countfunc, and \avgfunc as means over the entire table. \sumfunc is the mean multiplied by the dataset size, and \countfunc
is the sum where all the records have been set to one. Then, we prove the theorem by induction. 
Suppose, we have two independent estimates of the mean $\bar{X}_1$ and $\bar{X}_2$ with unknown variance and we want to merge them into one estimate $\bar{X} = c_1\bar{X}_1+c_2\bar{X}_2$.
Since, we do not know anything about the distribution by symmetry $c_1 = c_2 = 0.5$. 
Thus we can recursively induct, and we find that if we do not know the variance of data, as is impossible with new updates, then equally weighting all input rows is optimal. 
\end{proof}
